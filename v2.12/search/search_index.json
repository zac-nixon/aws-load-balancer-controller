{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>          A         Kubernetes          controller for         Elastic Load Balancers </p> <p> </p> <p> </p>"},{"location":"#aws-load-balancer-controller","title":"AWS Load Balancer Controller","text":"<p>AWS Load Balancer Controller is a controller to help manage Elastic Load Balancers for a Kubernetes cluster.</p> <ul> <li>It satisfies Kubernetes Ingress resources by provisioning Application Load Balancers.</li> <li>It satisfies Kubernetes Service resources by provisioning Network Load Balancers.</li> </ul> <p>This project was formerly known as \"AWS ALB Ingress Controller\", we rebranded it to be \"AWS Load Balancer Controller\".</p> <ul> <li> <p>AWS ALB Ingress Controller was originated by Ticketmaster and CoreOS as part of Ticketmaster's move to AWS and CoreOS Tectonic. Learn more about Ticketmaster's Kubernetes initiative from Justin Dean's video at Tectonic Summit.</p> </li> <li> <p>AWS ALB Ingress Controller was donated to Kubernetes SIG-AWS to allow AWS, CoreOS, Ticketmaster and other SIG-AWS contributors to officially maintain the project. SIG-AWS reached this consensus on June 1, 2018.</p> </li> </ul>"},{"location":"#support-policy","title":"Support Policy","text":"<p>Currently, AWS provides security updates and bug fixes to the latest available minor versions of AWS LBC. For other ad-hoc supports on older versions, please reach out through AWS support ticket.</p>"},{"location":"CONTRIBUTING/","title":"Contributing Guidelines","text":"<p>Welcome to Kubernetes. We are excited about the prospect of you joining our community! The Kubernetes community abides by the CNCF code of conduct. Here is an excerpt:</p> <p>As contributors and maintainers of this project, and in the interest of fostering an open and welcoming community, we pledge to respect all people who contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or patches, and other activities.</p>"},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":""},{"location":"CONTRIBUTING/#building-the-project","title":"Building the project","text":"<p>Controller development documentation has instructions on how to build the project and project specific expectations.</p>"},{"location":"CONTRIBUTING/#contributing-to-docs","title":"Contributing to docs","text":"<p>The documentation is generated using Material for MkDocs. In order to generate and preview docs locally, use the steps below -</p> <ul> <li>Install pipenv</li> <li>run <code>make docs-preview</code>. This will generate and serve docs locally at http://127.0.0.1:8000</li> </ul>"},{"location":"CONTRIBUTING/#contributing","title":"Contributing","text":"<p>We also have more documentation on how to get started contributing here:</p> <ul> <li>Contributor License Agreement Kubernetes projects require that you sign a Contributor License Agreement (CLA) before we can accept your pull requests</li> <li>Kubernetes Contributor Guide - Main contributor documentation, or you can just jump directly to the contributing section</li> <li>Contributor Cheat Sheet - Common resources for existing developers</li> </ul>"},{"location":"CONTRIBUTING/#mentorship","title":"Mentorship","text":"<ul> <li>Mentoring Initiatives - We have a diverse set of mentorship programs available that are always looking for volunteers!</li> </ul>"},{"location":"CONTRIBUTING/#contact-information","title":"Contact Information","text":"<ul> <li>Slack channel</li> <li>Mailing list</li> </ul>"},{"location":"code-of-conduct/","title":"Kubernetes Community Code of Conduct","text":"<p>Please refer to our Kubernetes Community Code of Conduct</p>"},{"location":"controller-devel/","title":"AWS Load Balancer Controller Development Guide","text":"<p>We'll walk you through the setup to start contributing to the AWS Load Balancer Controller project. No matter if you're contributing code or docs, follow the steps below to set up your development environment.</p> <p>Issue before PR</p> <p>Of course we're happy about code drops via PRs, however, in order to give us time to plan ahead and also to avoid disappointment, consider creating an issue first and submit a PR later. This also helps us to coordinate between different contributors and should in general help keeping everyone happy.</p>"},{"location":"controller-devel/#prerequisites","title":"Prerequisites","text":"<p>Please ensure that you have properly installed Go.</p> <p>Go version</p> <p>We recommend to use a Go version of <code>1.14</code> or above for development.</p>"},{"location":"controller-devel/#fork-upstream-repository","title":"Fork upstream repository","text":"<p>The first step in setting up your AWS Load Balancer controller development environment is to fork the upstream AWS Load Balancer controller repository to your personal Github account.</p>"},{"location":"controller-devel/#ensure-source-code-organization-directories-exist","title":"Ensure source code organization directories exist","text":"<p>Make sure in your <code>$GOPATH/src</code> that you have directories for the <code>sigs.k8s.io</code> organization:</p> <pre><code>mkdir -p $GOPATH/src/github.com/sigs.k8s.io\n</code></pre>"},{"location":"controller-devel/#git-clone-forked-repository-and-add-upstream-remote","title":"<code>git clone</code> forked repository and add upstream remote","text":"<p>For the forked repository, you will <code>git clone</code> the repository into the appropriate folder in your <code>$GOPATH</code>. Once <code>git clone</code>'d, you will want to set up a Git remote called \"upstream\" (remember that \"origin\" will be pointing at your forked repository location in your personal Github space).</p> <p>You can use this script to do this for you:</p> <pre><code>GITHUB_ID=\"your GH username\"\n\ncd $GOPATH/src/github.com/sigs.k8s.io\ngit clone git@github.com:$GITHUB_ID/aws-load-balancer-controller\ncd aws-load-balancer-controller/\ngit remote add upstream git@github.com:kubernetes-sigs/aws-load-balancer-controller\ngit fetch --all\n</code></pre>"},{"location":"controller-devel/#create-your-local-branch","title":"Create your local branch","text":"<p>Next, you create a local branch where you work on your feature or bug fix. Let's say you want to enhance the docs, so set <code>BRANCH_NAME=docs-improve</code> and then:</p> <pre><code>git fetch --all &amp;&amp; git checkout -b $BRANCH_NAME upstream/main\n</code></pre>"},{"location":"controller-devel/#commit-changes","title":"Commit changes","text":"<p>Make your changes locally, commit and push using:</p> <pre><code>git commit -a -m \"improves the docs a lot\"\n\ngit push origin $BRANCH_NAME\n</code></pre>"},{"location":"controller-devel/#create-a-pull-request","title":"Create a pull request","text":"<p>Finally, submit a pull request against the upstream source repository.</p> <p>We monitor the GitHub repo and try to follow up with comments within a working day.</p>"},{"location":"controller-devel/#building-the-controller","title":"Building the controller","text":"<p>To build the controller binary, run the following command.</p> <pre><code>make controller\n</code></pre> <p>To install CRDs into a Kubernetes cluster, run the following command.</p> <pre><code>make install\n</code></pre> <p>To uninstall CRD from a Kubernetes cluster, run the following command.</p> <pre><code>make uninstall\n</code></pre> <p>To build the container image for the controller and push to a container registry, run the following command.</p> <pre><code>make docker-push\n</code></pre> <p>To deploy the CRDs and the container image to a Kubernetes cluster, run the following command.</p> <pre><code>make deploy\n</code></pre>"},{"location":"how-it-works/","title":"How AWS Load Balancer controller works","text":""},{"location":"how-it-works/#design","title":"Design","text":"<p>The following diagram details the AWS components this controller creates. It also demonstrates the route ingress traffic takes from the ALB to the Kubernetes cluster.</p> <p></p> <p>Note</p> <p>The controller manages the configurations of the resources it creates, and we do not recommend out-of-band modifications to these resources because the controller may revert the manual changes during reconciliation. We recommend to use configuration options provided as best practice, such as ingress and service annotations, controller command line flags and IngressClassParams.</p>"},{"location":"how-it-works/#ingress-creation","title":"Ingress Creation","text":"<p>This section describes each step (circle) above. This example demonstrates satisfying 1 ingress resource.</p> <p>[1]: The controller watches for ingress events from the API server. When it finds ingress resources that satisfy its requirements, it begins the creation of AWS resources.</p> <p>[2]: An ALB (ELBv2) is created in AWS for the new ingress resource. This ALB can be internet-facing or internal. You can also specify the subnets it's created in using annotations.</p> <p>[3]: Target Groups are created in AWS for each unique Kubernetes service described in the ingress resource.</p> <p>[4]: Listeners are created for every port detailed in your ingress resource annotations. When no port is specified, sensible defaults (<code>80</code> or <code>443</code>) are used. Certificates may also be attached via annotations.</p> <p>[5]: Rules are created for each path specified in your ingress resource. This ensures traffic to a specific path is routed to the correct Kubernetes Service.</p> <p>Along with the above, the controller also...</p> <ul> <li>deletes AWS components when ingress resources are removed from k8s.</li> <li>modifies AWS components when ingress resources change in k8s.</li> <li>assembles a list of existing ingress-related AWS components on start-up, allowing you to   recover if the controller were to be restarted.</li> </ul>"},{"location":"how-it-works/#ingress-traffic","title":"Ingress Traffic","text":"<p>AWS Load Balancer controller supports two traffic modes:</p> <ul> <li>Instance mode</li> <li>IP mode</li> </ul> <p>By default, <code>Instance mode</code> is used, users can explicitly select the mode via <code>alb.ingress.kubernetes.io/target-type</code> annotation.</p>"},{"location":"how-it-works/#instance-mode","title":"Instance mode","text":"<p>Ingress traffic starts at the ALB and reaches the Kubernetes nodes through each service's NodePort. This means that services referenced from ingress resources must be exposed by <code>type:NodePort</code> in order to be reached by the ALB.</p>"},{"location":"how-it-works/#ip-mode","title":"IP mode","text":"<p>Ingress traffic starts at the ALB and reaches the Kubernetes pods directly. CNIs must support directly accessible POD ip via secondary IP addresses on ENI.</p>"},{"location":"release/","title":"AWS Load Balancer Controller Release Process","text":""},{"location":"release/#create-the-release-commit","title":"Create the Release Commit","text":"<p>Run <code>hack/set-version</code> to set the new version number and commit the resulting changes.  This is called the \"release commit\".</p>"},{"location":"release/#merge-the-release-commit","title":"Merge the Release Commit","text":"<p>Create a pull request with the release commit. Get it reviewed and merged to <code>main</code>.</p> <p>Upon merge to <code>main</code>, GitHub Actions will create a release tag for the new release.</p> <p>If the release is a \".0-beta.1\" release, GitHub Actions will also create a release branch for the minor version.</p> <p>(Remaining steps in process yet to be documented.)</p>"},{"location":"deploy/configurations/","title":"Controller configuration options","text":"<p>This document covers configuration of the AWS Load Balancer controller</p> <p>limitation</p> <p>The v2.0.0+ version of AWSLoadBalancerController currently only support one controller deployment(with one or multiple replicas) per cluster.</p> <p>The AWSLoadBalancerController assumes it's the solo owner of worker node security group rules with <code>elbv2.k8s.aws/targetGroupBinding=shared</code> description, running multiple controller deployment will cause these controllers compete with each other updating worker node security group rules.</p> <p>We will remove this limitation in future versions: tracking issue</p>"},{"location":"deploy/configurations/#aws-api-access","title":"AWS API Access","text":"<p>To perform operations, the controller must have required IAM role capabilities for accessing and provisioning ALB resources. There are many ways to achieve this, such as loading <code>AWS_ACCESS_KEY_ID/AWS_SECRET_ACCESS_KEY</code> as environment variables or using kube2iam.</p> <p>Refer to the installation guide for installing the controller in your kubernetes cluster and for the minimum required IAM permissions.</p>"},{"location":"deploy/configurations/#setting-ingress-resource-scope","title":"Setting Ingress Resource Scope","text":"<p>You can limit the ingresses ALB ingress controller controls by combining following two approaches:</p>"},{"location":"deploy/configurations/#limiting-ingress-class","title":"Limiting ingress class","text":"<p>Setting the <code>--ingress-class</code> argument constrains the controller's scope to ingresses with matching <code>ingressClassName</code> field.</p> <p>An example of the container spec portion of the controller, only listening for resources with the class \"alb\", would be as follows.</p> <pre><code>spec:\n  containers:\n  - args:\n    - --ingress-class=alb\n</code></pre> <p>Now, only ingress resources with the appropriate class are picked up, as seen below.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: echoserver\n  namespace: echoserver\nspec:\n  ingressClassName: alb\n  ...\n</code></pre> <p>If the ingress class is not specified, the controller will reconcile Ingress objects without the ingress class specified or ingress class <code>alb</code>.</p>"},{"location":"deploy/configurations/#limiting-namespaces","title":"Limiting Namespaces","text":"<p>Setting the <code>--watch-namespace</code> argument constrains the controller's scope to a single namespace. Ingress events outside of the namespace specified are not be seen by the controller.</p> <p>An example of the container spec, for a controller watching only the <code>default</code> namespace, is as follows.</p> <pre><code>spec:\n  containers:\n  - args:\n    - --watch-namespace=default\n</code></pre> <p>Currently, you can set only 1 namespace to watch in this flag. See this Kubernetes issue for more details.</p>"},{"location":"deploy/configurations/#controller-command-line-flags","title":"Controller command line flags","text":"<p>The --cluster-name flag is mandatory and the value must match the name of the kubernetes cluster. If you specify an incorrect name, the subnet auto-discovery will not work.</p> Flag Type Default Description aws-api-endpoints AWS API Endpoints Config AWS API endpoints mapping, format: serviceID1=URL1,serviceID2=URL2 aws-api-throttle AWS Throttle Config default value throttle settings for AWS APIs, format: serviceID1:operationRegex1=rate:burst,serviceID2:operationRegex2=rate:burst aws-max-retries int 10 Maximum retries for AWS APIs aws-region string instance metadata AWS Region for the kubernetes cluster aws-vpc-id string instance metadata AWS VPC ID for the Kubernetes cluster aws-vpc-tags stringMap Tags for the Kubernetes cluster VPC, When both flags <code>--aws-vpc-id</code> and <code>--aws-vpc-tags</code> are specified, the controller prioritizes <code>--aws-vpc-id</code> and ignores the other flag. aws-vpc-tag-key string Name Optional tag key used with aws-vpc-tags add only if VPC name tag key is not the default value \"Name\" allowed-certificate-authority-arns stringList [] Specify an optional list of CA ARNs to filter on in cert discovery (empty means all CAs are allowed) backend-security-group string Backend security group id to use for the ingress rules on the worker node SG cluster-name string Kubernetes cluster name default-ssl-policy string ELBSecurityPolicy-2016-08 Default SSL Policy that will be applied to all Ingresses or Services that do not have the SSL Policy annotation default-tags stringMap AWS Tags that will be applied to all AWS resources managed by this controller. Specified Tags takes highest priority default-target-type string instance Default target type for Ingresses and Services - ip, instance default-load-balancer-scheme string internal Default scheme for ELBs - internal,  internet-facing disable-ingress-class-annotation boolean false Disable new usage of the <code>kubernetes.io/ingress.class</code> annotation disable-ingress-group-name-annotation boolean false Disallow new use of the <code>alb.ingress.kubernetes.io/group.name</code> annotation disable-restricted-sg-rules boolean false Disable the usage of restricted security group rules enable-backend-security-group boolean true Enable sharing of security groups for backend traffic enable-endpoint-slices boolean false Use EndpointSlices instead of Endpoints for pod endpoint and TargetGroupBinding resolution for load balancers with IP targets. enable-leader-election boolean true Enable leader election for the load balancer controller manager. Enabling this will ensure there is only one active controller manager enable-pod-readiness-gate-inject boolean true If enabled, targetHealth readiness gate will get injected to the pod spec for the matching endpoint pods enable-shield boolean true Enable Shield addon for ALB enable-waf boolean true Enable WAF addon for ALB enable-wafv2 boolean true Enable WAF V2 addon for ALB external-managed-tags stringList AWS Tag keys that will be managed externally. Specified Tags are ignored during reconciliation feature-gates stringMap A set of key=value pairs to enable or disable features health-probe-bind-addr string :61779 The address the health probes binds to ingress-class string alb Name of the ingress class this controller satisfies ingress-max-concurrent-reconciles int 3 Maximum number of concurrently running reconcile loops for ingress kubeconfig string in-cluster config Path to the kubeconfig file containing authorization and API server information leader-election-id string aws-load-balancer-controller-leader Name of the leader election ID to use for this controller leader-election-namespace string Name of the leader election ID to use for this controller load-balancer-class string service.k8s.aws/nlb Name of the load balancer class specified in service <code>spec.loadBalancerClass</code> reconciled by this controller log-level string info Set the controller log level - info, debug metrics-bind-addr string :8080 The address the metric endpoint binds to service-max-concurrent-reconciles int 3 Maximum number of concurrently running reconcile loops for service sync-period duration 10h0m0s Period at which the controller forces the repopulation of its local object stores targetgroupbinding-max-concurrent-reconciles int 3 Maximum number of concurrently running reconcile loops for targetGroupBinding targetgroupbinding-max-exponential-backoff-delay duration 16m40s Maximum duration of exponential backoff for targetGroupBinding reconcile failures lb-stabilization-monitor-interval duration 2m Interval at which the controller monitors the state of load balancer after creation tolerate-non-existent-backend-service boolean true Whether to allow rules which refer to backend services that do not exist (When enabled, it will return 503 error if backend service not exist) tolerate-non-existent-backend-action boolean true Whether to allow rules which refer to backend actions that do not exist (When enabled, it will return 503 error if backend action not exist) watch-namespace string Namespace the controller watches for updates to Kubernetes objects, If empty, all namespaces are watched. webhook-bind-port int 9443 The TCP port the Webhook server binds to webhook-cert-dir string /tmp/k8s-webhook-server/serving-certs The directory that contains the server key and certificate webhook-cert-file string tls.crt The server certificate name webhook-key-file string tls.key The server key name"},{"location":"deploy/configurations/#disable-ingress-class-annotation","title":"disable-ingress-class-annotation","text":"<p><code>--disable-ingress-class-annotation</code> controls whether to disable new usage of the <code>kubernetes.io/ingress.class</code> annotation.</p> <p>Once disabled:</p> <ul> <li> <p>you can no longer create Ingresses with the value of the <code>kubernetes.io/ingress.class</code> annotation equal to <code>alb</code> (can be overridden via <code>--ingress-class</code> flag of this controller).</p> </li> <li> <p>you can no longer update Ingresses to set the value of the <code>kubernetes.io/ingress.class</code> annotation equal to <code>alb</code> (can be overridden via <code>--ingress-class</code> flag of this controller).</p> </li> <li> <p>you can still create Ingresses with a <code>kubernetes.io/ingress.class</code> annotation that has other values (for example: \"nginx\")</p> </li> </ul>"},{"location":"deploy/configurations/#disable-ingress-group-name-annotation","title":"disable-ingress-group-name-annotation","text":"<p><code>--disable-ingress-group-name-annotation</code> controls whether to disable new usage of <code>alb.ingress.kubernetes.io/group.name</code> annotation.</p> <p>Once disabled:</p> <ul> <li>you can no longer create Ingresses with the <code>alb.ingress.kubernetes.io/group.name</code> annotation.</li> <li>you can no longer alter the value of an <code>alb.ingress.kubernetes.io/group.name</code> annotation on an existing Ingress.</li> </ul>"},{"location":"deploy/configurations/#sync-period","title":"sync-period","text":"<p><code>--sync-period</code> defines a fixed interval for the controller to reconcile all resources even if there is no change, default to 10 hr. Please be mindful that frequent reconciliations may incur unnecessary AWS API usage.</p> <p>As best practice, we do not recommend users to manually modify the resources managed by the controller. And users should not depend on the controller auto-reconciliation to revert the manual modification, or to mitigate any security risks.</p>"},{"location":"deploy/configurations/#lb-stabilization-monitor-interval","title":"lb-stabilization-monitor-interval","text":"<p><code>--lb-stabilization-monitor-interval</code> defines a fixed interval for the controller to monitor the state of load balancer after the creation for stabilization, default to 2m. It monitors the load balancer state so that once it becomes active it can make the required updates like capacity reservation for the active load balancer. It calls DescribeLoadBalancer API at a fixed interval to monitor the state. Please be mindful that lower value will result into frequent calls which may incur unnecessary AWS API usage.</p>"},{"location":"deploy/configurations/#waf-addons","title":"waf-addons","text":"<p>By default, the controller assumes sole ownership of the WAF addons associated to the provisioned ALBs, via the flag <code>--enable-waf</code> and <code>--enable-wafv2</code>. And the users should disable them accordingly if they want a third party like AWS Firewall Manager to associate or remove the WAF-ACL of the ALBs. Once disabled, the controller shall not take any actions on the waf addons of the provisioned ALBs.</p>"},{"location":"deploy/configurations/#throttle-config","title":"throttle config","text":"<p>Controller uses the following default throttle config:</p> <p><pre><code>WAF Regional:^AssociateWebACL|DisassociateWebACL=0.5:1,WAF Regional:^GetWebACLForResource|ListResourcesForWebACL=1:1,WAFV2:^AssociateWebACL|DisassociateWebACL=0.5:1,WAFV2:^GetWebACLForResource|ListResourcesForWebACL=1:1,Elastic Load Balancing v2:^RegisterTargets|^DeregisterTargets=4:20,Elastic Load Balancing v2:.*=10:40\n</code></pre> Client side throttling enables gradual scaling of the api calls. Additional throttle config can be specified via the <code>--aws-api-throttle</code> flag. You can get the ServiceID from the API definition in AWS SDK. For e.g, ELBv2 it is Elastic Load Balancing v2.</p> <p>Here is an example of throttle config to specify client side throttling of ELBv2 calls.</p> <pre><code>--aws-api-throttle=Elastic Load Balancing v2:RegisterTargets|DeregisterTargets=4:20,Elastic Load Balancing v2:.*=10:40\n</code></pre>"},{"location":"deploy/configurations/#instance-metadata","title":"Instance metadata","text":"<p>If running on EC2, the default values are obtained from the instance metadata service.</p>"},{"location":"deploy/configurations/#feature-gates","title":"Feature Gates","text":"<p>There are a set of key=value pairs that describe AWS load balancer controller features. You can use it as flags <code>--feature-gates=key1=value1,key2=value2</code></p> Features-gate Supported Key Type Default Value Description ListenerRulesTagging string true Enable or disable tagging AWS load balancer listeners and rules WeightedTargetGroups string true Enable or disable weighted target groups ServiceTypeLoadBalancerOnly string false If enabled, controller will be limited to reconciling service of type <code>LoadBalancer</code> EndpointsFailOpen string true Enable or disable allowing endpoints with <code>ready:unknown</code> state in the target groups. EnableServiceController string true Toggles support for <code>Service</code> type resources. EnableIPTargetType string true Used to toggle support for target-type <code>ip</code> across <code>Ingress</code> and <code>Service</code> type resources. EnableRGTAPI string false If enabled, the tagging manager will describe resource tags via RGT APIs, otherwise via ELB APIs. In order to enable RGT API, <code>tag:GetResources</code> is needed in controller IAM policy. SubnetsClusterTagCheck string true Enable or disable the check for <code>kubernetes.io/cluster/${cluster-name}</code> during subnet auto-discovery NLBHealthCheckAdvancedConfiguration string true Enable or disable advanced health check configuration for NLB, for example health check timeout ALBSingleSubnet string false If enabled, controller will allow using only 1 subnet for provisioning ALB, which need to get whitelisted by ELB in advance NLBSecurityGroup string true Enable or disable all NLB security groups actions including frontend sg creation, backend sg creation, and backend sg modifications LBCapacityReservation string true Enable or disable the capacity reservation feature on ALB and NLB"},{"location":"deploy/installation/","title":"AWS Load Balancer Controller installation","text":"<p>The AWS Load Balancer controller (LBC) provisions AWS Network Load Balancer (NLB) and Application Load Balancer (ALB) resources. The LBC watches for new <code>service</code> or <code>ingress</code> Kubernetes resources and configures AWS resources.</p> <p>The LBC is supported by AWS. Some clusters may be using the legacy \"in-tree\" functionality to provision AWS load balancers. The AWS Load Balancer Controller should be installed instead.</p> <p>Existing AWS ALB Ingress Controller users</p> <p>The AWS ALB Ingress controller must be uninstalled before installing the AWS Load Balancer Controller. Please follow our migration guide to do a migration.</p> <p>When using AWS Load Balancer Controller v2.5+</p> <p>The AWS LBC provides a mutating webhook for service resources to set the <code>spec.loadBalancerClass</code> field for service of type LoadBalancer on create. This makes the AWS LBC the default controller for service of type LoadBalancer. You can disable this feature and revert to set Cloud Controller Manager (in-tree controller) as the default by setting the helm chart value enableServiceMutatorWebhook to false with <code>--set enableServiceMutatorWebhook=false</code> . You will no longer be able to provision new Classic Load Balancer (CLB) from your kubernetes service unless you disable this feature. Existing CLB will continue to work fine.</p>"},{"location":"deploy/installation/#supported-kubernetes-versions","title":"Supported Kubernetes versions","text":"<ul> <li>AWS Load Balancer Controller v2.0.0~v2.1.3 requires Kubernetes 1.15+</li> <li>AWS Load Balancer Controller v2.2.0~v2.3.1 requires Kubernetes 1.16-1.21</li> <li>AWS Load Balancer Controller v2.4.0+ requires Kubernetes 1.19+</li> <li>AWS Load Balancer Controller v2.5.0+ requires Kubernetes 1.22+</li> </ul>"},{"location":"deploy/installation/#deployment-considerations","title":"Deployment considerations","text":""},{"location":"deploy/installation/#additional-requirements-for-non-eks-clusters","title":"Additional requirements for non-EKS clusters:","text":"<ul> <li>Ensure subnets are tagged appropriately for auto-discovery to work</li> <li>For IP targets, pods must have IPs from the VPC subnets. You can configure the <code>amazon-vpc-cni-k8s</code> plugin for this purpose.</li> </ul>"},{"location":"deploy/installation/#additional-requirements-for-isolated-cluster","title":"Additional requirements for isolated cluster:","text":"<p>Isolated clusters are clusters without internet access, and instead reply on VPC endpoints for all required connects. When installing the AWS LBC in isolated clusters, you need to disable shield, waf and wafv2 via controller flags <code>--enable-shield=false, --enable-waf=false, --enable-wafv2=false</code></p>"},{"location":"deploy/installation/#using-the-amazon-ec2-instance-metadata-server-version-2-imdsv2","title":"Using the Amazon EC2 instance metadata server version 2 (IMDSv2)","text":"<p>We recommend blocking the access to instance metadata by requiring the instance to use IMDSv2 only. For more information, please refer to the AWS guidance here. If you are using the IMDSv2, set the hop limit to 2 or higher in order to allow the LBC to perform the metadata introspection.</p> <p>You can set the IMDSv2 as follows: <pre><code>aws ec2 modify-instance-metadata-options --http-put-response-hop-limit 2 --http-tokens required --region &lt;region&gt; --instance-id &lt;instance-id&gt;\n</code></pre></p> <p>Instead of depending on IMDSv2, you can specify the AWS Region via the controller flag <code>--aws-region</code>, and the AWS VPC via controller flag <code>--aws-vpc-id</code> or by specifying vpc tags via the flag <code>--aws-vpc-tags</code> and an optional flag <code>--aws-vpc-tag-key</code> if you have a different key for the tag other than \"Name\". When both flags <code>--aws-vpc-id</code> and <code>--aws-vpc-tags</code> are specified, the controller prioritizes <code>--aws-vpc-id</code>and ignores the other flag.</p>"},{"location":"deploy/installation/#configure-iam","title":"Configure IAM","text":"<p>The controller runs on the worker nodes, so it needs access to the AWS ALB/NLB APIs with IAM permissions.</p> <p>The IAM permissions can either be setup using IAM roles for service accounts (IRSA) or can be attached directly to the worker node IAM roles. The best practice is using IRSA if you're using Amazon EKS. If you're using kOps or self-hosted Kubernetes, you must manually attach polices to node instances.</p>"},{"location":"deploy/installation/#option-a-recommended-iam-roles-for-service-accounts-irsa","title":"Option A: Recommended, IAM roles for service accounts (IRSA)","text":"<p>The reference IAM policies contain the following permissive configuration: <pre><code>{\n    \"Effect\": \"Allow\",\n    \"Action\": [\n        \"ec2:AuthorizeSecurityGroupIngress\",\n        \"ec2:RevokeSecurityGroupIngress\"\n    ],\n    \"Resource\": \"*\"\n},\n</code></pre></p> <p>We recommend further scoping down this configuration based on the VPC ID or cluster name resource tag.</p> <p>Example condition for VPC ID: <pre><code>    \"Condition\": {\n        \"ArnEquals\": {\n            \"ec2:Vpc\": \"arn:aws:ec2:&lt;REGION&gt;:&lt;ACCOUNT-ID&gt;:vpc/&lt;VPC-ID&gt;\"\n        }\n    }\n</code></pre></p> <p>Example condition for cluster name resource tag: <pre><code>    \"Condition\": {\n        \"Null\": {\n            \"aws:ResourceTag/kubernetes.io/cluster/&lt;CLUSTER-NAME&gt;\": \"false\"\n        }\n    }\n</code></pre></p> <ol> <li> <p>Create an IAM OIDC provider. You can skip this step if you already have one for your cluster.     <pre><code>eksctl utils associate-iam-oidc-provider \\\n    --region &lt;region-code&gt; \\\n    --cluster &lt;your-cluster-name&gt; \\\n    --approve\n</code></pre></p> </li> <li> <p>Download an IAM policy for the LBC using one of the following commands:<p>     If your cluster is in a US Gov Cloud region:     <pre><code>curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.12.0/docs/install/iam_policy_us-gov.json\n</code></pre>     If your cluster is in a China region:     <pre><code>curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.12.0/docs/install/iam_policy_cn.json\n</code></pre>     If your cluster is in any other region:     <pre><code>curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.12.0/docs/install/iam_policy.json\n</code></pre></p> <li> <p>Create an IAM policy named <code>AWSLoadBalancerControllerIAMPolicy</code>. If you downloaded a different policy, replace <code>iam-policy</code> with the name of the policy that you downloaded.     <pre><code>aws iam create-policy \\\n    --policy-name AWSLoadBalancerControllerIAMPolicy \\\n    --policy-document file://iam-policy.json\n</code></pre>     Take note of the policy ARN that's returned.</p> </li> <li> <p>Create an IAM role and Kubernetes <code>ServiceAccount</code> for the LBC. Use the ARN from the previous step.     <pre><code>eksctl create iamserviceaccount \\\n--cluster=&lt;cluster-name&gt; \\\n--namespace=kube-system \\\n--name=aws-load-balancer-controller \\\n--attach-policy-arn=arn:aws:iam::&lt;AWS_ACCOUNT_ID&gt;:policy/AWSLoadBalancerControllerIAMPolicy \\\n--override-existing-serviceaccounts \\\n--region &lt;region-code&gt; \\\n--approve\n</code></pre></p> </li>"},{"location":"deploy/installation/#option-b-attach-iam-policies-to-nodes","title":"Option B: Attach IAM policies to nodes","text":"<p>If you're not setting up IAM roles for service accounts, apply the IAM policies from the following URL at a minimum. Please be aware of the possibility that the controller permissions may be assumed by other users in a pod after retrieving the node role credentials, so the best practice would be using IRSA instead of attaching IAM policy directly. <pre><code>curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.12.0/docs/install/iam_policy.json\n</code></pre></p>"},{"location":"deploy/installation/#special-iam-cases","title":"Special IAM cases","text":""},{"location":"deploy/installation/#you-only-want-the-lbc-to-add-and-remove-ips-to-already-existing-target-groups","title":"You only want the LBC to add and remove IPs to already existing target groups:","text":"<p>The following IAM permissions subset is for those using <code>TargetGroupBinding</code> only and don't plan to use the LBC to manage security group rules:</p> <pre><code>{\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"ec2:DescribeVpcs\",\n                \"ec2:DescribeSecurityGroups\",\n                \"ec2:DescribeInstances\",\n                \"elasticloadbalancing:DescribeTargetGroups\",\n                \"elasticloadbalancing:DescribeTargetHealth\",\n                \"elasticloadbalancing:ModifyTargetGroup\",\n                \"elasticloadbalancing:ModifyTargetGroupAttributes\",\n                \"elasticloadbalancing:RegisterTargets\",\n                \"elasticloadbalancing:DeregisterTargets\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"*\"\n        }\n    ],\n    \"Version\": \"2012-10-17\"\n}\n</code></pre>"},{"location":"deploy/installation/#you-only-want-the-lbc-to-add-and-remove-ips-to-already-existing-target-groups-also-in-other-accounts-assuming-roles","title":"You only want the LBC to add and remove IPs to already existing target groups, also in other accounts, assuming roles","text":"<p>On the other hand, if you plan to use the LBC to manage also target groups in different accounts, you will need to add <code>\"sts:AssumeRole\"</code> to your list of permissions, in other words:</p> <pre><code>{\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"ec2:DescribeVpcs\",\n                \"ec2:DescribeSecurityGroups\",\n                \"ec2:DescribeInstances\",\n                \"elasticloadbalancing:DescribeTargetGroups\",\n                \"elasticloadbalancing:DescribeTargetHealth\",\n                \"elasticloadbalancing:ModifyTargetGroup\",\n                \"elasticloadbalancing:ModifyTargetGroupAttributes\",\n                \"elasticloadbalancing:RegisterTargets\",\n                \"elasticloadbalancing:DeregisterTargets\",\n                \"sts:AssumeRole\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": \"*\"\n        }\n    ],\n    \"Version\": \"2012-10-17\"\n}\n</code></pre> <p>The assumed roles will need the exactly the same permissions, without <code>\"sts:AssumeRole\"</code>. The assumed role will need a to allow to be assumed by the main role, something like this:</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::999999999999999:user/test-alb-controller\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"very-secret-string\"\n                }\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"deploy/installation/#network-configuration","title":"Network configuration","text":"<p>Review the worker nodes security group docs. Your node security group must permit incoming traffic on TCP port 9443 from the Kubernetes control plane. This is needed for webhook access.</p> <p>If you use eksctl, this is the default configuration.</p> <p>If you use custom networking, please refer to the EKS Best Practices Guides for network configuration.</p>"},{"location":"deploy/installation/#add-controller-to-cluster","title":"Add controller to cluster","text":"<p>We recommend using the Helm chart to install the controller. The chart supports Fargate and facilitates updating the controller.</p> Helm <p>If you want to run the controller on Fargate, use the Helm chart, since it doesn't depend on the <code>cert-manager</code>.</p> YAML manifests"},{"location":"deploy/installation/#detailed-instructions","title":"Detailed instructions","text":"<p>Follow the instructions in the aws-load-balancer-controller Helm chart.</p>"},{"location":"deploy/installation/#summary","title":"Summary","text":"<ol> <li>Add the EKS chart repo to Helm <pre><code>helm repo add eks https://aws.github.io/eks-charts\n</code></pre></li> <li> <p>If upgrading the chart via <code>helm upgrade</code>, install the <code>TargetGroupBinding</code> CRDs. <pre><code>wget https://raw.githubusercontent.com/aws/eks-charts/master/stable/aws-load-balancer-controller/crds/crds.yaml\nkubectl apply -f crds.yaml\n</code></pre></p> <p>Tip</p> <p>The <code>helm install</code> command automatically applies the CRDs, but <code>helm upgrade</code> doesn't.</p> </li> </ol> <p>Helm install command for clusters with IRSA: <pre><code>helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=&lt;cluster-name&gt; --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller\n</code></pre></p> <p>Helm install command for clusters not using IRSA: <pre><code>helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=&lt;cluster-name&gt;\n</code></pre></p>"},{"location":"deploy/installation/#install-cert-manager","title":"Install <code>cert-manager</code>","text":"<pre><code>kubectl apply --validate=false -f https://github.com/cert-manager/cert-manager/releases/download/v1.12.3/cert-manager.yaml\n</code></pre>"},{"location":"deploy/installation/#apply-yaml","title":"Apply YAML","text":"<ol> <li>Download the spec for the LBC. <pre><code>wget https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.12.0/v2_12_0_full.yaml\n</code></pre></li> <li>Edit the saved yaml file, go to the Deployment spec, and set the controller <code>--cluster-name</code> arg value to your EKS cluster name <pre><code>apiVersion: apps/v1\nkind: Deployment\n. . .\nname: aws-load-balancer-controller\nnamespace: kube-system\nspec:\n    . . .\n    template:\n        spec:\n            containers:\n                - args:\n                    - --cluster-name=&lt;your-cluster-name&gt;\n</code></pre></li> <li>If you use IAM roles for service accounts, we recommend that you delete the <code>ServiceAccount</code> from the yaml spec. If you delete the installation section from the yaml spec, deleting the <code>ServiceAccount</code> preserves the <code>eksctl</code> created <code>iamserviceaccount</code>. <pre><code>apiVersion: v1\nkind: ServiceAccount\n</code></pre></li> <li>Apply the yaml file <pre><code>kubectl apply -f v2_12_0_full.yaml\n</code></pre></li> <li>Optionally download the default ingressclass and ingressclass params <pre><code>wget https://github.com/kubernetes-sigs/aws-load-balancer-controller/releases/download/v2.12.0/v2_12_0_ingclass.yaml\n</code></pre></li> <li>Apply the ingressclass and params <pre><code>kubectl apply -f v2_12_0_ingclass.yaml\n</code></pre></li> </ol>"},{"location":"deploy/installation/#create-update-strategy","title":"Create Update Strategy","text":"<p>The controller doesn't receive security updates automatically. You need to manually upgrade to a newer version when it becomes available.</p> <p>You can upgrade using <code>helm upgrade</code> or another strategy to manage the controller deployment.</p>"},{"location":"deploy/pod_readiness_gate/","title":"Pod readiness gate","text":"<p>AWS Load Balancer controller supports \u00bbPod readiness gates\u00ab to indicate that pod is registered to the ALB/NLB and healthy to receive traffic. The controller automatically injects the necessary readiness gate configuration to the pod spec via mutating webhook during pod creation.</p> <p>For readiness gate configuration to be injected to the pod spec, you need to apply the label <code>elbv2.k8s.aws/pod-readiness-gate-inject: enabled</code> to the pod namespace. However, note that this only works with <code>target-type: ip</code>, since when using <code>target-type: instance</code>, it's the node used as backend, the ALB itself is not aware of pod/podReadiness in such case.</p> <p>The pod readiness gate is needed under certain circumstances to achieve full zero downtime rolling deployments. Consider the following example:</p> <ul> <li>Low number of replicas in a deployment</li> <li>Start a rolling update of the deployment</li> <li>Rollout of new pods takes less time than it takes the AWS Load Balancer controller to register the new pods and for their health state turn \u00bbHealthy\u00ab in the target group</li> <li>At some point during this rolling update, the target group might only have registered targets that are in \u00bbInitial\u00ab or \u00bbDraining\u00ab state; this results in service outage</li> </ul> <p>In order to avoid this situation, the AWS Load Balancer controller can set the readiness condition on the pods that constitute your ingress or service backend. The condition status on a pod will be set to <code>True</code> only when the corresponding target in the ALB/NLB target group shows a health state of \u00bbHealthy\u00ab. This prevents the rolling update of a deployment from terminating old pods until the newly created pods are \u00bbHealthy\u00ab in the ALB/NLB target group and ready to take traffic.</p> <p>upgrading from AWS ALB ingress controller</p> <p>If you have a pod spec with legacy readiness gate configuration, ensure you label the namespace and create the Service/Ingress objects before applying the pod/deployment manifest. The load balancer controller will remove all legacy readiness-gate configuration and add new ones during pod creation.</p>"},{"location":"deploy/pod_readiness_gate/#configuration","title":"Configuration","text":"<p>Pod readiness gate support is enabled by default on the AWS load balancer controller. You need to apply the readiness gate inject label to each of the namespace that you would like to use this feature. You can create and label a namespace as follows -</p> <p><pre><code>$ kubectl create namespace readiness\nnamespace/readiness created\n\n$ kubectl label namespace readiness elbv2.k8s.aws/pod-readiness-gate-inject=enabled\nnamespace/readiness labeled\n\n$ kubectl describe namespace readiness\nName:         readiness\nLabels:       elbv2.k8s.aws/pod-readiness-gate-inject=enabled\nAnnotations:  &lt;none&gt;\nStatus:       Active\n</code></pre> Once labelled, the controller will add the pod readiness gates config to all the pods created subsequently that meet all the following conditions</p> <ul> <li>There exists a service matching the pod labels in the same namespace</li> <li>There exists at least one target group binding that refers to the matching service</li> <li>The target type is IP</li> </ul> <p>The readiness gates have the prefix <code>target-health.elbv2.k8s.aws</code> and the controller injects the config to the pod spec only during pod creation.</p> <p>create ingress or service before pod</p> <p>To ensure all of your pods in a namespace get the readiness gate config, you need create your Ingress or Service and label the namespace before creating the pods</p>"},{"location":"deploy/pod_readiness_gate/#failurepolicy","title":"FailurePolicy","text":"<p>The <code>failurePolicy</code> of a webhook determines how errors, such as unrecognized or timeout errors, are handled by the admission webhook.</p> <ul> <li><code>failurePolicy: Fail</code>: When applied to a pod mutation webhook, this setting will prevent the launch of any pods in labeled namespaces if the AWSLoadBalancerController pods are unavailable. While this can help avoid incomplete or faulty deployments, it could also delay the cluster's recovery in extreme scenarios, such as an API Server outage.</li> <li><code>failurePolicy: Ignore</code>: Setting this policy allows Kubernetes to proceed with pod deployments even if the AWSLoadBalancerController pods are unavailable. This can lead to availability risks for applications since Kubernetes may terminate application pods before the new pods have become healthy in the TargetGroups</li> </ul> <p>To strike a balance between reliability and availability, the default failurePolicy for pod mutation webhooks that inject readiness gates is configured as follows:</p> <ul> <li><code>failurePolicy: Ignore</code> (for versions &gt; v2.11.0)</li> <li><code>failurePolicy: Fail</code> (for versions &lt;= v2.11.0) You can customize the behavior using Helm chart settings, e.g. <code>--set podMutatorWebhookConfig.failurePolicy=Fail</code></li> </ul> <p>Recommended settings</p> <p>For optimal reliability &amp; availability, it is recommended to use <code>failurePolicy: Fail</code> combined with an explicit Object Selector</p>"},{"location":"deploy/pod_readiness_gate/#object-selector","title":"Object Selector","text":"<p>The default webhook configuration matches all pods in the namespaces containing the label <code>elbv2.k8s.aws/pod-readiness-gate-inject=enabled</code>. You can modify the webhook configuration further to select specific pods from the labeled namespace by specifying the <code>objectSelector</code>. For example, in order to select resources with <code>elbv2.k8s.aws/pod-readiness-gate-inject: enabled</code> label, you can add the following <code>objectSelector</code> to the webhook: <pre><code>  objectSelector:\n    matchLabels:\n      elbv2.k8s.aws/pod-readiness-gate-inject: enabled\n</code></pre> To edit, <pre><code>$ kubectl edit mutatingwebhookconfigurations aws-load-balancer-webhook\n  ...\n  name: mpod.elbv2.k8s.aws\n  namespaceSelector:\n    matchExpressions:\n    - key: elbv2.k8s.aws/pod-readiness-gate-inject\n      operator: In\n      values:\n      - enabled\n  objectSelector:\n    matchLabels:\n      elbv2.k8s.aws/pod-readiness-gate-inject: enabled\n  ...\n</code></pre> When you specify multiple selectors, pods matching all the conditions will get mutated.</p>"},{"location":"deploy/pod_readiness_gate/#upgrading-from-aws-alb-ingress-controller","title":"Upgrading from AWS ALB Ingress controller","text":"<p>If you have a pod spec with the AWS ALB ingress controller (aka v1) style readiness-gate configuration, the controller will automatically remove the legacy readiness gates config and add new ones during pod creation if the pod namespace is labelled correctly. Other than the namespace labeling, no further configuration is necessary. The legacy readiness gates have the <code>target-health.alb.ingress.k8s.aws</code> prefix.</p>"},{"location":"deploy/pod_readiness_gate/#disabling-the-readiness-gate-inject","title":"Disabling the readiness gate inject","text":"<p>You can specify the controller flag <code>--enable-pod-readiness-gate-inject=false</code> during controller startup to disable the controller from modifying the pod spec.</p>"},{"location":"deploy/pod_readiness_gate/#checking-the-pod-condition-status","title":"Checking the pod condition status","text":"<p>The status of the readiness gates can be verified with <code>kubectl get pod -o wide</code>: <pre><code>NAME                          READY   STATUS    RESTARTS   AGE   IP         NODE                       READINESS GATES\nnginx-test-5744b9ff84-7ftl9   1/1     Running   0          81s   10.1.2.3   ip-10-1-2-3.ec2.internal   0/1\n</code></pre></p> <p>When the target is registered and healthy in the ALB/NLB, the output will look like: <pre><code>NAME                          READY   STATUS    RESTARTS   AGE   IP         NODE                       READINESS GATES\nnginx-test-5744b9ff84-7ftl9   1/1     Running   0          81s   10.1.2.3   ip-10-1-2-3.ec2.internal   1/1\n</code></pre></p> <p>If a readiness gate doesn't get ready, you can check the reason via:</p> <pre><code>$ kubectl get pod nginx-test-545d8f4d89-l7rcl -o yaml | grep -B7 'type: target-health'\nstatus:\n  conditions:\n  - lastProbeTime: null\n    lastTransitionTime: null\n    message: Initial health checks in progress\n    reason: Elb.InitialHealthChecking\n    status: \"True\"\n    type: target-health.elbv2.k8s.aws/k8s-readines-perf1000-7848e5026b\n</code></pre>"},{"location":"deploy/security_groups/","title":"Security Groups for Load Balancers","text":"<p>Use security groups to limit client connections to your load balancers, and restrict connections with nodes. The AWS Load Balancer Controller (LBC) defines two classifications of security groups: frontend and backend.</p> <ul> <li>Frontend Security Groups: Determine the clients that can access the load balancers.</li> <li>Backend Security Groups: Permit the load balancer to connect to targets, such as EC2 instances or ENIs.</li> </ul>"},{"location":"deploy/security_groups/#frontend-security-groups","title":"Frontend Security Groups","text":"<p>Frontend security groups control access to load balancers by specifying which clients can connect to them.</p> <p>Use cases for Frontent Security Groups include:</p> <ul> <li>Placing the load balancer behind another service, such as AWS Web Application Firewall or AWS CloudFront.</li> <li>Blocking the IP address range (CIDR) of a region.</li> <li>Configuring the Load Balancer for private or internal use, by specifying internal CIDRs and Security Groups. </li> </ul> <p>In the default configuration, the LBC automatically creates one security group per load balancer, allowing traffic from <code>inbound-cidrs</code> to <code>listen-ports</code>.</p>"},{"location":"deploy/security_groups/#configuration","title":"Configuration","text":"<p>Apply custom frontend security groups with an annotation. This disables automatic generation of frontend security groups. </p> <ul> <li>For Ingress resources, use the <code>alb.ingress.kubernetes.io/security-groups</code> annotation.</li> <li>For Service resources, use the <code>service.beta.kubernetes.io/aws-load-balancer-security-groups</code> annotation.</li> <li>The annotation must be set to one or more security group IDs or security group names.</li> </ul>"},{"location":"deploy/security_groups/#backend-security-groups","title":"Backend Security Groups","text":"<p>Backend Security Groups control traffic between AWS Load Balancers and their target EC2 instances or ENIs. For example, backend security groups can restrict the ports load balancers may access on nodes.</p> <ul> <li>Backend security groups permit traffic from AWS Load Balancers to their targets. </li> <li>LBC uses a single, shared backend security group, attaching it to each load balancer and using as the traffic source in the security group rules it adds to targets.</li> <li>When configuring security group rules at the ENI/Instance level, use the Security Group ID of the backend security group. Avoid using the IP addresses of a specific AWS Load Balancer, these IPs are dynamic and the security group rules aren't updated automatically.</li> </ul>"},{"location":"deploy/security_groups/#configuration_1","title":"Configuration","text":"<p>Enable or Disable: Use <code>--enable-backend-security-group</code> (default <code>true</code>) to enable/disable the shared backend security group.</p> <p>You can turn off the shared backend security group feature by setting it to <code>false</code>. However, if you have a high number of Ingress resources with frontend security groups auto-generated by the controller, you might run into security group rule limits on the instance/ENI security groups.</p> <p>Specification: Use <code>--backend-security-group</code> to pass in a security group ID to use as a custom shared backend security group. </p> <p>If <code>--backend-security-group</code> is left empty, a security group with the following attributes will be created:</p> <pre><code>name: k8s-traffic-&lt;cluster_name&gt;-&lt;hash_of_cluster_name&gt;\ntags: \n    elbv2.k8s.aws/cluster: &lt;cluster_name&gt;\n    elbv2.k8s.aws/resource: backend-sg\n</code></pre>"},{"location":"deploy/security_groups/#coordination-of-frontend-and-backend-security-groups","title":"Coordination of Frontend and Backend Security Groups","text":"<ul> <li>If the LBC auto-creates the frontend security group for a load balancer, it automatically adds the security group rules to allow traffic from the load balancer to the backend instances/ENIs.</li> <li>If the frontend security groups are manually specified, the LBC will not by default add any rules to the backend security group.</li> </ul>"},{"location":"deploy/security_groups/#enable-autogeneration-of-backend-security-group-rules","title":"Enable Autogeneration of Backend Security Group Rules","text":"<ul> <li>If using custom frontend security groups, the LBC can be configured to automatically manage backend security group rules.</li> <li>To enable managing backend security group rules, apply an additional annotation to Ingress and Service resources.</li> <li>For Ingress resources, set the <code>alb.ingress.kubernetes.io/manage-backend-security-group-rules</code> annotation to <code>true</code>.</li> <li>For Service resources, set the <code>service.beta.kubernetes.io/aws-load-balancer-manage-backend-security-group-rules</code> annotation to <code>true</code>.</li> <li>If management of backend security group rules is enabled with an annotation on a Service or Ingress, then <code>--enable-backend-security-group</code> must be set to true.</li> <li>These annotations are ignored when using auto-generated frontend security groups. </li> </ul>"},{"location":"deploy/security_groups/#port-range-restrictions","title":"Port Range Restrictions","text":"<p>From version v2.3.0 onwards, the controller restricts port ranges in the backend security group rules by default. This improves the security of the default configuration. The LBC should generate the necessary rules to permit traffic, based on the Service and Ingress resources. </p> <p>If needed, set the controller flag <code>--disable-restricted-sg-rules</code> to <code>true</code> to permit traffic to all ports. This may be appropriate for backwards compatability, or troubleshooting. </p>"},{"location":"deploy/subnet_discovery/","title":"Subnet auto-discovery","text":"<p>By default, the AWS Load Balancer Controller (LBC) auto-discovers network subnets that it can create AWS Network Load Balancers (NLB) and AWS Application Load Balancers (ALB) in. ALBs require at least two subnets across Availability Zones by default,  set Feature Gate ALBSingleSubnet to \"true\" allows using only 1 subnet for provisioning ALB. NLBs require one subnet. The subnets must be tagged appropriately for auto-discovery to work. The controller chooses one subnet from each Availability Zone. During auto-discovery, the controller considers subnets with at least eight available IP addresses. In the case of multiple qualified tagged subnets in an Availability Zone, the controller chooses the first one in lexicographical  order by the subnet IDs. For more information about the subnets for the LBC, see Application Load Balancers  and Network Load Balancers. If you used <code>eksctl</code> or an Amazon EKS AWS CloudFormation template to create your VPC after March 26, 2020, then the subnets are tagged appropriately when they're created. For  more information about the Amazon EKS AWS CloudFormation VPC templates, see Creating a VPC for your Amazon EKS cluster.</p>"},{"location":"deploy/subnet_discovery/#public-subnets","title":"Public subnets","text":"<p>Public subnets are used for internet-facing load balancers. These subnets must have the following tags:</p> Key Value <code>kubernetes.io/role/elb</code> <code>1</code>  or ``"},{"location":"deploy/subnet_discovery/#private-subnets","title":"Private subnets","text":"<p>Private subnets are used for internal load balancers. These subnets must have the following tags:</p> Key Value <code>kubernetes.io/role/internal-elb</code> <code>1</code>  or ``"},{"location":"deploy/subnet_discovery/#common-tag","title":"Common tag","text":"<p>In version v2.1.1 and older of the LBC, both the public and private subnets must be tagged with the cluster name as follows:</p> Key Value <code>kubernetes.io/cluster/${cluster-name}</code> <code>owned</code> or <code>shared</code> <p><code>${cluster-name}</code> is the name of the Kubernetes cluster.</p> <p>The cluster tag is not required in versions v2.1.2 to v2.4.1, unless a cluster tag for another cluster is present.</p> <p>With versions v2.4.2 and later, you can disable the cluster tag check completely by specifying the feature gate <code>SubnetsClusterTagCheck=false</code></p>"},{"location":"deploy/upgrade/migrate_v1_v2/","title":"Migrate from v1 to v2","text":"<p>This document contains the information necessary to migrate from an existing installation of AWSALBIngressController(v1) to the new AWSLoadBalancerController(v2).</p>"},{"location":"deploy/upgrade/migrate_v1_v2/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWSALBIngressController &gt;=v1.1.3</li> </ul> <p>If you have AWSALBIngressController(&lt;1.1.3) installed, you need to upgrade to version&gt;=v1.1.3(e.g. v1.1.9) first.</p>"},{"location":"deploy/upgrade/migrate_v1_v2/#backwards-compatibility","title":"Backwards compatibility","text":"<p>The AWSLoadBalancerController(v2.0.1) is backwards-compatible with AWSALBIngressController(&gt;=v1.1.3).</p> <p>It supports existing AWS resources provisioned by AWSALBIngressController(&gt;=v1.1.3) for Ingress resources with below caveats:</p> <ol> <li> <p>The AWS LoadBalancer resource created for your Ingress will be preserved. If migrating from &lt;v1.1.3, a new AWS LoadBalancer resource will be created and the old AWS LoadBalancer will remain in the account. However, the old AWS LoadBalancer will not be used for the ingress resource.</p> </li> <li> <p>If security-groups annotation isn't used, the SecurityGroup rule on worker node's SecurityGroup that allow LoadBalancer traffic should be manually adjusted post migration.</p> <p>details</p> <p>when security-groups annotation isn't used:</p> <ul> <li>a managed SecurityGroup will be created and attached to ALB. This SecurityGroup will be preserved.</li> <li>an inbound rule will be added to your worker node securityGroups which allow traffic from the above managed SecurityGroup for ALB.<ul> <li>The AWSALBIngressController didn't add any description for that inbound rule.</li> <li>The AWSLoadBalancerController will use <code>elbv2.k8s.aws/targetGroupBinding=shared</code> for that inbound rule</li> </ul> </li> <li>You'll need to manually add <code>elbv2.k8s.aws/targetGroupBinding=shared</code> description to that inbound rule so that AWSLoadBalancerController can delete such rule when you delete your Ingress.<ul> <li>The following shell pipeline can be used to update the rules automatically. Replace <code>$REGION</code> and <code>$SG_ID</code> with your own values. After running it change <code>DryRun: true</code> to <code>DryRun: false</code> to have it actually update your security group:   <pre><code>aws --region $REGION ec2 update-security-group-rule-descriptions-ingress --cli-input-json \"$(aws --region $REGION ec2 describe-security-groups --group-ids $SG_ID | jq '.SecurityGroups[0] | {DryRun: true, GroupId: .GroupId ,IpPermissions: (.IpPermissions | map(select(.FromPort==0 and .ToPort==65535) | .UserIdGroupPairs |= map(.Description=\"elbv2.k8s.aws/targetGroupBinding=shared\"))) }' -M)\"\n</code></pre></li> </ul> </li> </ul> <p>sample</p> <p>inbound rule on worker node securityGroups that allow traffic from the managed LB securityGroup before migration:</p> Type Protocol Port range Source Description - optional All TCP TCP 0 - 65535 sg-008c920b1(managed LB SG) - <p>inbound rule on worker node securityGroups that allow traffic from the managed LB securityGroup after migration:</p> Type Protocol Port range Source Description - optional All TCP TCP 0 - 65535 sg-008c920b1(managed LB SG) elbv2.k8s.aws/targetGroupBinding=shared </li> <li> <p>If you have used podReadinessGate feature, please refer PodReadinessGate for the guide about new readinessGate configuration.</p> <p>old pod readinessGate</p> <p>once configured properly, AWS Load Balancer Controller will automatically inject the new format of podReadinessGates into your pods, and remove old podReadinessGates if any.</p> <p>However, we still recommend you to remove the old podReadinessGates from your Deployments since it's not used.</p> </li> </ol>"},{"location":"deploy/upgrade/migrate_v1_v2/#upgrade-steps","title":"Upgrade steps","text":"<ol> <li> <p>Determine existing installed AWSALBIngressController version. <pre><code>foo@bar:~$ kubectl describe deployment  -n kube-system  alb-ingress-controller | grep Image\n    Image:      docker.io/amazon/aws-alb-ingress-controller:v1.1.9\n</code></pre></p> </li> <li> <p>Uninstalling existing AWSALBIngressController(&gt;=v1.1.3).</p> <p>Existing AWSALBIngressController needs to be uninstalled first before install new AWSLoadBalancerController.</p> <p>Existing Ingress resources do not need to be deleted.</p> </li> <li> <p>Install new AWSLoadBalancerController</p> <ol> <li>Install AWSLoadBalancerController(v2.5.0) by following the installation instructions</li> <li>Grant additional IAM policy needed for migration to the controller.</li> </ol> </li> <li> <p>Verify all Ingresses works as expected.</p> </li> </ol>"},{"location":"examples/echo_server/","title":"walkthrough: echoserver","text":"<p>In this walkthrough, you'll</p> <ul> <li>Create a cluster with EKS</li> <li>Deploy an aws-load-balancer-controller</li> <li>Create deployments and ingress resources in the cluster</li> <li>Verify access to the service</li> <li>(Optional) Use external-dns to create a DNS record pointing to the load balancer created by the aws-load-balancer-controller.<ul> <li>This assumes you have a route53 hosted zone available. Otherwise you can access the service using the load balancer DNS.</li> </ul> </li> </ul>"},{"location":"examples/echo_server/#create-the-eks-cluster","title":"Create the EKS cluster","text":"<ol> <li> <p>Install <code>eksctl</code>: https://eksctl.io</p> </li> <li> <p>Create EKS cluster via eksctl</p> <pre><code>eksctl create cluster\n</code></pre> <pre><code>2018-08-14T11:19:09-07:00 [\u2139]  setting availability zones to [us-west-2c us-west-2a us-west-2b]\n2018-08-14T11:19:09-07:00 [\u2139]  importing SSH public key \"/Users/kamador/.ssh/id_rsa.pub\" as \"eksctl-exciting-gopher-1534270749-b7:71:da:f6:f3:63:7a:ee:ad:7a:10:37:28:ff:44:d1\"\n2018-08-14T11:19:10-07:00 [\u2139]  creating EKS cluster \"exciting-gopher-1534270749\" in \"us-west-2\" region\n2018-08-14T11:19:10-07:00 [\u2139]  creating ServiceRole stack \"EKS-exciting-gopher-1534270749-ServiceRole\"\n2018-08-14T11:19:10-07:00 [\u2139]  creating VPC stack \"EKS-exciting-gopher-1534270749-VPC\"\n2018-08-14T11:19:50-07:00 [\u2714]  created ServiceRole stack \"EKS-exciting-gopher-1534270749-ServiceRole\"\n2018-08-14T11:20:30-07:00 [\u2714]  created VPC stack \"EKS-exciting-gopher-1534270749-VPC\"\n2018-08-14T11:20:30-07:00 [\u2139]  creating control plane \"exciting-gopher-1534270749\"\n2018-08-14T11:31:52-07:00 [\u2714]  created control plane \"exciting-gopher-1534270749\"\n2018-08-14T11:31:52-07:00 [\u2139]  creating DefaultNodeGroup stack \"EKS-exciting-gopher-1534270749-DefaultNodeGroup\"\n2018-08-14T11:35:33-07:00 [\u2714]  created DefaultNodeGroup stack \"EKS-exciting-gopher-1534270749-DefaultNodeGroup\"\n2018-08-14T11:35:33-07:00 [\u2714]  all EKS cluster \"exciting-gopher-1534270749\" resources has been created\n2018-08-14T11:35:33-07:00 [\u2714]  saved kubeconfig as \"/Users/kamador/.kube/config\"\n2018-08-14T11:35:34-07:00 [\u2139]  the cluster has 0 nodes\n2018-08-14T11:35:34-07:00 [\u2139]  waiting for at least 2 nodes to become ready\n2018-08-14T11:36:05-07:00 [\u2139]  the cluster has 2 nodes\n2018-08-14T11:36:05-07:00 [\u2139]  node \"ip-192-168-139-176.us-west-2.compute.internal\" is ready\n2018-08-14T11:36:05-07:00 [\u2139]  node \"ip-192-168-214-126.us-west-2.compute.internal\" is ready\n2018-08-14T11:36:05-07:00 [\u2714]  EKS cluster \"exciting-gopher-1534270749\" in \"us-west-2\" region is ready\n</code></pre> </li> </ol>"},{"location":"examples/echo_server/#setup-the-aws-load-balancer-controller","title":"Setup the AWS Load Balancer controller","text":"<ol> <li> <p>Refer to the installation instructions to setup the controller</p> </li> <li> <p>Verify the deployment was successful and the controller started.</p> <pre><code>kubectl logs -n kube-system --tail -1 -l app.kubernetes.io/name=aws-load-balancer-controller\n</code></pre> <p>Should display output similar to the following.</p> <pre><code>{\"level\":\"info\",\"ts\":1602778062.2588625,\"logger\":\"setup\",\"msg\":\"version\",\"GitVersion\":\"v2.0.0-rc3-13-gcdc8f715-dirty\",\"GitCommit\":\"cdc8f715919cc65ca8161b6083c4091222632d6b\",\"BuildDate\":\"2020-10-15T15:58:31+0000\"}\n{\"level\":\"info\",\"ts\":1602778065.4515743,\"logger\":\"controller-runtime.metrics\",\"msg\":\"metrics server is starting to listen\",\"addr\":\":8080\"}\n{\"level\":\"info\",\"ts\":1602778065.4536595,\"logger\":\"controller-runtime.webhook\",\"msg\":\"registering webhook\",\"path\":\"/mutate-v1-pod\"}\n{\"level\":\"info\",\"ts\":1602778065.4537156,\"logger\":\"controller-runtime.webhook\",\"msg\":\"registering webhook\",\"path\":\"/mutate-elbv2-k8s-aws-v1beta1-targetgroupbinding\"}\n{\"level\":\"info\",\"ts\":1602778065.4537542,\"logger\":\"controller-runtime.webhook\",\"msg\":\"registering webhook\",\"path\":\"/validate-elbv2-k8s-aws-v1beta1-targetgroupbinding\"}\n{\"level\":\"info\",\"ts\":1602778065.4537594,\"logger\":\"setup\",\"msg\":\"starting manager\"}\nI1015 16:07:45.453851       1 leaderelection.go:242] attempting to acquire leader lease  kube-system/aws-load-balancer-controller-leader...\n{\"level\":\"info\",\"ts\":1602778065.5544264,\"logger\":\"controller-runtime.manager\",\"msg\":\"starting metrics server\",\"path\":\"/metrics\"}\n{\"level\":\"info\",\"ts\":1602778065.5544496,\"logger\":\"controller-runtime.webhook.webhooks\",\"msg\":\"starting webhook server\"}\n{\"level\":\"info\",\"ts\":1602778065.5549548,\"logger\":\"controller-runtime.certwatcher\",\"msg\":\"Updated current TLS certificate\"}\n{\"level\":\"info\",\"ts\":1602778065.5550802,\"logger\":\"controller-runtime.webhook\",\"msg\":\"serving webhook server\",\"host\":\"\",\"port\":9443}\n{\"level\":\"info\",\"ts\":1602778065.5551715,\"logger\":\"controller-runtime.certwatcher\",\"msg\":\"Starting certificate watcher\"}\nI1015 16:08:03.662023       1 leaderelection.go:252] successfully acquired lease kube-system/aws-load-balancer-controller-leader\n{\"level\":\"info\",\"ts\":1602778083.663017,\"logger\":\"controller-runtime.controller\",\"msg\":\"Starting EventSource\",\"controller\":\"targetGroupBinding\",\"source\":\"kind source: /, Kind=\"}\n{\"level\":\"info\",\"ts\":1602778083.6631303,\"logger\":\"controller-runtime.controller\",\"msg\":\"Starting EventSource\",\"controller\":\"targetGroupBinding\",\"source\":\"kind source: /, Kind=\"}\n{\"level\":\"info\",\"ts\":1602778083.6633205,\"logger\":\"controller-runtime.controller\",\"msg\":\"Starting EventSource\",\"controller\":\"ingress\",\"source\":\"channel source: 0xc0007340f0\"}\n{\"level\":\"info\",\"ts\":1602778083.6633654,\"logger\":\"controller-runtime.controller\",\"msg\":\"Starting EventSource\",\"controller\":\"ingress\",\"source\":\"channel source: 0xc000734140\"}\n{\"level\":\"info\",\"ts\":1602778083.6633892,\"logger\":\"controller-runtime.controller\",\"msg\":\"Starting EventSource\",\"controller\":\"ingress\",\"source\":\"kind source: /, Kind=\"}\n{\"level\":\"info\",\"ts\":1602778083.663441,\"logger\":\"controller-runtime.controller\",\"msg\":\"Starting EventSource\",\"controller\":\"ingress\",\"source\":\"kind source: /, Kind=\"}\n{\"level\":\"info\",\"ts\":1602778083.6634624,\"logger\":\"controller-runtime.controller\",\"msg\":\"Starting EventSource\",\"controller\":\"ingress\",\"source\":\"kind source: /, Kind=\"}\n{\"level\":\"info\",\"ts\":1602778083.6635776,\"logger\":\"controller-runtime.controller\",\"msg\":\"Starting EventSource\",\"controller\":\"service\",\"source\":\"kind source: /, Kind=\"}\n{\"level\":\"info\",\"ts\":1602778083.6636262,\"logger\":\"controller-runtime.controller\",\"msg\":\"Starting Controller\",\"controller\":\"service\"}\n{\"level\":\"info\",\"ts\":1602778083.7634695,\"logger\":\"controller-runtime.controller\",\"msg\":\"Starting EventSource\",\"controller\":\"targetGroupBinding\",\"source\":\"kind source: /, Kind=\"}\n{\"level\":\"info\",\"ts\":1602778083.7637022,\"logger\":\"controller-runtime.controller\",\"msg\":\"Starting workers\",\"controller\":\"service\",\"worker count\":3}\n{\"level\":\"info\",\"ts\":1602778083.7641861,\"logger\":\"controller-runtime.controller\",\"msg\":\"Starting Controller\",\"controller\":\"ingress\"}\n{\"level\":\"info\",\"ts\":1602778083.8641882,\"logger\":\"controller-runtime.controller\",\"msg\":\"Starting Controller\",\"controller\":\"targetGroupBinding\"}\n{\"level\":\"info\",\"ts\":1602778083.864236,\"logger\":\"controller-runtime.controller\",\"msg\":\"Starting workers\",\"controller\":\"targetGroupBinding\",\"worker count\":3}\n{\"level\":\"info\",\"ts\":1602778083.8643816,\"logger\":\"controller-runtime.controller\",\"msg\":\"Starting workers\",\"controller\":\"ingress\",\"worker count\":3}\n</code></pre> </li> </ol>"},{"location":"examples/echo_server/#deploy-the-echoserver-resources","title":"Deploy the echoserver resources","text":"<ol> <li> <p>Deploy all the echoserver resources (namespace, service, deployment)</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.12.0/docs/examples/echoservice/echoserver-namespace.yaml &amp;&amp;\\\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.12.0/docs/examples/echoservice/echoserver-service.yaml &amp;&amp;\\\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.12.0/docs/examples/echoservice/echoserver-deployment.yaml\n</code></pre> </li> <li> <p>List all the resources to ensure they were created.</p> <pre><code>kubectl get -n echoserver deploy,svc\n</code></pre> <p>Should resolve similar to the following.</p> <pre><code>NAME             CLUSTER-IP   EXTERNAL-IP   PORT(S)        AGE\nsvc/echoserver   10.3.31.76   &lt;nodes&gt;       80:31027/TCP   4d\n\nNAME                DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\ndeploy/echoserver   1         1         1            1           4d\n</code></pre> </li> </ol>"},{"location":"examples/echo_server/#deploy-ingress-for-echoserver","title":"Deploy ingress for echoserver","text":"<ol> <li> <p>Download the echoserver ingress manifest locally.</p> <pre><code>wget https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.12.0/docs/examples/echoservice/echoserver-ingress.yaml\n</code></pre> </li> <li> <p>Configure the subnets, either by add annotation to the ingress or add tags to subnets. This step is optional in lieu of auto-discovery.</p> <p>Tip</p> <p>If you'd like to use external dns, alter the host field to a domain that you own in Route 53. Assuming you managed <code>example.com</code> in Route 53.</p> <ul> <li> <p>Edit the <code>alb.ingress.kubernetes.io/subnets</code> annotation to include at least two subnets. Subnets must be from different Availability Zones.     <pre><code>eksctl get cluster exciting-gopher-1534270749\n</code></pre></p> <pre><code>NAME                        VERSION STATUS         CREATED          VPC                     SUBNETS                             SECURITYGROUPS\nexciting-gopher-1534270749  1.10    ACTIVE  2018-08-14T18:20:32Z    vpc-0aa01b07b3c922c9c   subnet-05e1c98ed0f5b109e,subnet-07f5bb81f661df61b,subnet-0a4e6232630820516  sg-05ceb5eee9fd7cac4\n</code></pre> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n    name: echoserver\n    namespace: echoserver\n    annotations:\n        alb.ingress.kubernetes.io/scheme: internet-facing\n        alb.ingress.kubernetes.io/target-type: ip\n        alb.ingress.kubernetes.io/subnets: subnet-05e1c98ed0f5b109e,subnet-07f5bb81f661df61b,subnet-0a4e6232630820516\n        alb.ingress.kubernetes.io/tags: Environment=dev,Team=test\nspec:\n    rules:\n    - http:\n        paths:\n</code></pre> </li> <li> <p>Adding tags to subnets for auto-discovery(instead of <code>alb.ingress.kubernetes.io/subnets</code> annotation)</p> <p>you must include the following tags on desired subnets.</p> <ul> <li><code>kubernetes.io/cluster/$CLUSTER_NAME</code> where <code>$CLUSTER_NAME</code> is the same <code>CLUSTER_NAME</code> specified in the above step.</li> <li><code>kubernetes.io/role/internal-elb</code> should be set to <code>1</code> or an empty tag value for internal load balancers.</li> <li><code>kubernetes.io/role/elb</code> should be set to <code>1</code> or an empty tag value for internet-facing load balancers.</li> </ul> <p>An example of a subnet with the correct tags for the cluster <code>joshcalico</code> is as follows.</p> <p></p> </li> </ul> </li> <li> <p>Deploy the ingress resource for echoserver</p> <pre><code>kubectl apply -f echoserver-ingress.yaml\n</code></pre> </li> <li> <p>Verify the aws-load-balancer-controller creates the resources</p> <pre><code>kubectl logs -n kube-system --tail -1 -l app.kubernetes.io/name=aws-load-balancer-controller | grep 'echoserver\\/echoserver'\n</code></pre> <p>You should see similar to the following.</p> <pre><code>{\"level\":\"info\",\"ts\":1602803965.264764,\"logger\":\"controllers.ingress\",\"msg\":\"successfully built model\",\"model\":\"{\\\"id\\\":\\\"echoserver/echoserver\\\",\\\"resources\\\":{\\\"AWS::EC2::SecurityGroup\\\":{\\\"ManagedLBSecurityGroup\\\":{\\\"spec\\\":{\\\"groupName\\\":\\\"k8s-echoserv-echoserv-4e1e34cae5\\\",\\\"description\\\":\\\"[k8s] Managed SecurityGroup for LoadBalancer\\\",\\\"tags\\\":{\\\"Environment\\\":\\\"dev\\\",\\\"Team\\\":\\\"test\\\"},\\\"ingress\\\":[{\\\"ipProtocol\\\":\\\"tcp\\\",\\\"fromPort\\\":80,\\\"toPort\\\":80,\\\"ipRanges\\\":[{\\\"cidrIP\\\":\\\"0.0.0.0/0\\\"}]}]}}},\\\"AWS::ElasticLoadBalancingV2::Listener\\\":{\\\"80\\\":{\\\"spec\\\":{\\\"loadBalancerARN\\\":{\\\"$ref\\\":\\\"#/resources/AWS::ElasticLoadBalancingV2::LoadBalancer/LoadBalancer/status/loadBalancerARN\\\"},\\\"port\\\":80,\\\"protocol\\\":\\\"HTTP\\\",\\\"defaultActions\\\":[{\\\"type\\\":\\\"fixed-response\\\",\\\"fixedResponseConfig\\\":{\\\"contentType\\\":\\\"text/plain\\\",\\\"statusCode\\\":\\\"404\\\"}}]}}},\\\"AWS::ElasticLoadBalancingV2::ListenerRule\\\":{\\\"80:1\\\":{\\\"spec\\\":{\\\"listenerARN\\\":{\\\"$ref\\\":\\\"#/resources/AWS::ElasticLoadBalancingV2::Listener/80/status/listenerARN\\\"},\\\"priority\\\":1,\\\"actions\\\":[{\\\"type\\\":\\\"forward\\\",\\\"forwardConfig\\\":{\\\"targetGroups\\\":[{\\\"targetGroupARN\\\":{\\\"$ref\\\":\\\"#/resources/AWS::ElasticLoadBalancingV2::TargetGroup/echoserver/echoserver-echoserver:80/status/targetGroupARN\\\"}}]}}],\\\"conditions\\\":[{\\\"field\\\":\\\"host-header\\\",\\\"hostHeaderConfig\\\":{\\\"values\\\":[\\\"echoserver.example.com\\\"]}},{\\\"field\\\":\\\"path-pattern\\\",\\\"pathPatternConfig\\\":{\\\"values\\\":[\\\"/\\\"]}}]}}},\\\"AWS::ElasticLoadBalancingV2::LoadBalancer\\\":{\\\"LoadBalancer\\\":{\\\"spec\\\":{\\\"name\\\":\\\"k8s-echoserv-echoserv-d4d6bd65d0\\\",\\\"type\\\":\\\"application\\\",\\\"scheme\\\":\\\"internet-facing\\\",\\\"ipAddressType\\\":\\\"ipv4\\\",\\\"subnetMapping\\\":[{\\\"subnetID\\\":\\\"subnet-01b35707c23b0a43b\\\"},{\\\"subnetID\\\":\\\"subnet-0f7814a7ab4dfcc2c\\\"}],\\\"securityGroups\\\":[{\\\"$ref\\\":\\\"#/resources/AWS::EC2::SecurityGroup/ManagedLBSecurityGroup/status/groupID\\\"}],\\\"tags\\\":{\\\"Environment\\\":\\\"dev\\\",\\\"Team\\\":\\\"test\\\"}}}},\\\"AWS::ElasticLoadBalancingV2::TargetGroup\\\":{\\\"echoserver/echoserver-echoserver:80\\\":{\\\"spec\\\":{\\\"name\\\":\\\"k8s-echoserv-echoserv-d989093207\\\",\\\"targetType\\\":\\\"instance\\\",\\\"port\\\":1,\\\"protocol\\\":\\\"HTTP\\\",\\\"healthCheckConfig\\\":{\\\"port\\\":\\\"traffic-port\\\",\\\"protocol\\\":\\\"HTTP\\\",\\\"path\\\":\\\"/\\\",\\\"matcher\\\":{\\\"httpCode\\\":\\\"200\\\"},\\\"intervalSeconds\\\":15,\\\"timeoutSeconds\\\":5,\\\"healthyThresholdCount\\\":2,\\\"unhealthyThresholdCount\\\":2},\\\"tags\\\":{\\\"Environment\\\":\\\"dev\\\",\\\"Team\\\":\\\"test\\\"}}}},\\\"K8S::ElasticLoadBalancingV2::TargetGroupBinding\\\":{\\\"echoserver/echoserver-echoserver:80\\\":{\\\"spec\\\":{\\\"template\\\":{\\\"metadata\\\":{\\\"name\\\":\\\"k8s-echoserv-echoserv-d989093207\\\",\\\"namespace\\\":\\\"echoserver\\\",\\\"creationTimestamp\\\":null},\\\"spec\\\":{\\\"targetGroupARN\\\":{\\\"$ref\\\":\\\"#/resources/AWS::ElasticLoadBalancingV2::TargetGroup/echoserver/echoserver-echoserver:80/status/targetGroupARN\\\"},\\\"targetType\\\":\\\"instance\\\",\\\"serviceRef\\\":{\\\"name\\\":\\\"echoserver\\\",\\\"port\\\":80},\\\"networking\\\":{\\\"ingress\\\":[{\\\"from\\\":[{\\\"securityGroup\\\":{\\\"groupID\\\":{\\\"$ref\\\":\\\"#/resources/AWS::EC2::SecurityGroup/ManagedLBSecurityGroup/status/groupID\\\"}}}],\\\"ports\\\":[{\\\"protocol\\\":\\\"TCP\\\"}]}]}}}}}}}}\"}\n{\"level\":\"info\",\"ts\":1602803966.411922,\"logger\":\"controllers.ingress\",\"msg\":\"creating targetGroup\",\"stackID\":\"echoserver/echoserver\",\"resourceID\":\"echoserver/echoserver-echoserver:80\"}\n{\"level\":\"info\",\"ts\":1602803966.6606336,\"logger\":\"controllers.ingress\",\"msg\":\"created targetGroup\",\"stackID\":\"echoserver/echoserver\",\"resourceID\":\"echoserver/echoserver-echoserver:80\",\"arn\":\"arn:aws:elasticloadbalancing:us-west-2:019453415603:targetgroup/k8s-echoserv-echoserv-d989093207/63225ae3ead3deb6\"}\n{\"level\":\"info\",\"ts\":1602803966.798019,\"logger\":\"controllers.ingress\",\"msg\":\"creating loadBalancer\",\"stackID\":\"echoserver/echoserver\",\"resourceID\":\"LoadBalancer\"}\n{\"level\":\"info\",\"ts\":1602803967.5472538,\"logger\":\"controllers.ingress\",\"msg\":\"created loadBalancer\",\"stackID\":\"echoserver/echoserver\",\"resourceID\":\"LoadBalancer\",\"arn\":\"arn:aws:elasticloadbalancing:us-west-2:019453415603:loadbalancer/app/k8s-echoserv-echoserv-d4d6bd65d0/4b4ebe8d6e1ef0c1\"}\n{\"level\":\"info\",\"ts\":1602803967.5863476,\"logger\":\"controllers.ingress\",\"msg\":\"creating listener\",\"stackID\":\"echoserver/echoserver\",\"resourceID\":\"80\"}\n{\"level\":\"info\",\"ts\":1602803967.6436293,\"logger\":\"controllers.ingress\",\"msg\":\"created listener\",\"stackID\":\"echoserver/echoserver\",\"resourceID\":\"80\",\"arn\":\"arn:aws:elasticloadbalancing:us-west-2:019453415603:listener/app/k8s-echoserv-echoserv-d4d6bd65d0/4b4ebe8d6e1ef0c1/6e13477f9d840da0\"}\n{\"level\":\"info\",\"ts\":1602803967.6528971,\"logger\":\"controllers.ingress\",\"msg\":\"creating listener rule\",\"stackID\":\"echoserver/echoserver\",\"resourceID\":\"80:1\"}\n{\"level\":\"info\",\"ts\":1602803967.7160048,\"logger\":\"controllers.ingress\",\"msg\":\"created listener rule\",\"stackID\":\"echoserver/echoserver\",\"resourceID\":\"80:1\",\"arn\":\"arn:aws:elasticloadbalancing:us-west-2:019453415603:listener-rule/app/k8s-echoserv-echoserv-d4d6bd65d0/4b4ebe8d6e1ef0c1/6e13477f9d840da0/23ef859380e792e8\"}\n{\"level\":\"info\",\"ts\":1602803967.8484688,\"logger\":\"controllers.ingress\",\"msg\":\"successfully deployed model\",\"ingressGroup\":\"echoserver/echoserver\"}\n</code></pre> </li> <li> <p>Check the events of the ingress to see what has occur.</p> <pre><code>kubectl describe ing -n echoserver echoserver\n</code></pre> <p>You should see similar to the following.</p> <pre><code>Name:                   echoserver\nNamespace:              echoserver\nAddress:                joshcalico-echoserver-echo-2ad7-1490890749.us-east-2.elb.amazonaws.com\nDefault backend:        default-http-backend:80 (10.2.1.28:8080)\nRules:\n  Host                          Path    Backends\n  ----                          ----    --------\n  *\n                                /       echoserver:80 (&lt;none&gt;)\nAnnotations:\nEvents:\n  FirstSeen     LastSeen        Count   From                    SubObjectPath   Type            Reason  Message\n  ---------     --------        -----   ----                    -------------   --------        ------  -------\n  3m            3m              1       ingress-controller                      Normal          CREATE  Ingress echoserver/echoserver\n  3m            32s             3       ingress-controller                      Normal          UPDATE  Ingress echoserver/echoserver\n</code></pre> <p>The address seen above is the ALB's DNS name. This will be referenced via records created by external-dns if you choose to set it up.</p> </li> </ol>"},{"location":"examples/echo_server/#verify-that-you-can-access-the-service","title":"Verify that you can access the service","text":"<p>Make a curl request to the echoserver service and verify that it returns a response payload. Use the address from the output of <code>kubectl describe ing</code> command above.</p> <pre><code>curl &lt;load-balancer-dns-name&gt;\n</code></pre> <p>You should get back a valid response.</p>"},{"location":"examples/echo_server/#optional-use-external-dns-to-create-a-dns-record","title":"(Optional) Use external-dns to create a DNS record","text":"<ol> <li> <p>Deploy external-dns to your cluster using these instructions - Setup external-dns</p> </li> <li> <p>Update your ingress resource and add <code>spec.rules[0].host</code> and set the value to your domain name. The example below uses <code>echoserver.example.org</code>.</p> </li> </ol> <p><pre><code>     spec:\n         rules:\n         - host: echoserver.example.org\n           http:\n             paths:\n</code></pre> 1. external-dns will then create a DNS record for the host you specified. This assumes you have the hosted zone corresponding to the domain you are trying to create a record in.</p> <ol> <li> <p>Annotate the ingress with the external-dns specific configuration</p> <pre><code>annotations:\n  kubernetes.io/ingress.class: alb\n  alb.ingress.kubernetes.io/scheme: internet-facing\n\n  # external-dns specific configuration for creating route53 record-set\n  external-dns.alpha.kubernetes.io/hostname: my-app.test-dns.com # give your domain name here\n</code></pre> </li> <li> <p>Verify the DNS has propagated</p> <pre><code>dig echoserver.example.org\n</code></pre> <pre><code>;; QUESTION SECTION:\n;echoserver.example.org.  IN      A\n\n;; ANSWER SECTION:\nechoserver.example.org. 60 IN     A       13.59.147.105\nechoserver.example.org. 60 IN     A       18.221.65.39\nechoserver.example.org. 60 IN     A       52.15.186.25\n</code></pre> </li> <li> <p>Once it has, you can make a call to echoserver and it should return a response payload.</p> <pre><code>curl echoserver.example.org\n</code></pre> <pre><code>CLIENT VALUES:\nclient_address=10.0.50.185\ncommand=GET\nreal path=/\nquery=nil\nrequest_version=1.1\nrequest_uri=http://echoserver.example.org:8080/\n\nSERVER VALUES:\nserver_version=nginx: 1.10.0 - lua: 10001\n\nHEADERS RECEIVED:\naccept=*/*\nhost=echoserver.example.org\nuser-agent=curl/7.54.0\nx-amzn-trace-id=Root=1-59c08da5-113347df69640735312371bd\nx-forwarded-for=67.173.237.250\nx-forwarded-port=80\nx-forwarded-proto=http\nBODY:\n</code></pre> </li> </ol>"},{"location":"examples/echo_server/#kube2iam-setup","title":"Kube2iam setup","text":"<p>follow below steps if you want to use kube2iam to provide the AWS credentials</p> <ol> <li> <p>configure the proper policy     The policy to be used can be fetched from https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.12.0/docs/install/iam_policy.json</p> </li> <li> <p>configure the proper role and create the trust relationship     You have to find which role is associated with your K8S nodes. Once you found take note of the full arn:</p> <pre><code>arn:aws:iam::XXXXXXXXXXXX:role/k8scluster-node\n</code></pre> </li> <li> <p>create the role, called k8s-lb-controller, attach the above policy and add a Trust Relationship like:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Service\": \"ec2.amazonaws.com\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    },\n    {\n      \"Sid\": \"\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::XXXXXXXXXXXX:role/k8scluster-node\"\n      },\n      \"Action\": \"sts:AssumeRole\"\n    }\n  ]\n}\n</code></pre> <p>The new role will have a similar arn:</p> <pre><code>arn:aws:iam:::XXXXXXXXXXXX:role/k8s-lb-controller\n</code></pre> </li> <li> <p>update the alb-load-balancer-controller deployment</p> <p>Add the annotations in the template's metadata point</p> <pre><code>spec:\nreplicas: 1\nselector:\nmatchLabels:\n  app.kubernetes.io/component: controller\n  app.kubernetes.io/name: aws-load-balancer-controller\nstrategy:\n  rollingUpdate:\n    maxSurge: 1\n    maxUnavailable: 1\n  type: RollingUpdate\ntemplate:\n  metadata:\n    annotations:\n      iam.amazonaws.com/role: arn:aws:iam:::XXXXXXXXXXXX:role/k8s-lb-controller\n</code></pre> </li> </ol>"},{"location":"examples/grpc_server/","title":"walkthrough: grpcserver","text":"<p>In this walkthrough, you'll</p> <ul> <li>Deploy a grpc service to an existing EKS cluster</li> <li>Send a test message to the hosted service over TLS</li> </ul>"},{"location":"examples/grpc_server/#prerequsites","title":"Prerequsites","text":"<p>The following resources are required prior to deployment:</p> <ul> <li>EKS cluster</li> <li>aws-load-balancer-controller</li> <li>external-dns</li> </ul> <p>See echo_server.md and external_dns.md for setup instructions for those resources.</p>"},{"location":"examples/grpc_server/#create-an-acm-certificate","title":"Create an ACM certificate","text":"<p>NOTE: An ACM certificate is required for this demo as the application uses the <code>grpc.secure_channel</code> method.</p> <p>If you already have an ACM certificate (including wildcard certificates) for the domain you would like to use in this example, you can skip this step.</p> <ul> <li>Request a certificate for a domain you own using the steps described in the official AWS ACM documentation.</li> <li>Once the status for the certificate is \"Issued\" continue to the next step.</li> </ul>"},{"location":"examples/grpc_server/#deploy-the-grpcserver-manifests","title":"Deploy the grpcserver manifests","text":"<ol> <li> <p>Deploy all the manifests from GitHub.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/examples/grpc/grpcserver-namespace.yaml\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/examples/grpc/grpcserver-service.yaml\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/examples/grpc/grpcserver-deployment.yaml\n</code></pre> </li> <li> <p>Confirm that all resources were created.</p> <pre><code>kubectl get -n grpcserver all\n</code></pre> <p>You should see the pod, service, and deployment.</p> <pre><code>NAME                             READY   STATUS    RESTARTS   AGE\npod/grpcserver-5455b7d4d-jshk5   1/1     Running   0          35m\n\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)     AGE\nservice/grpcserver   ClusterIP   None         &lt;none&gt;        50051/TCP   77m\n\nNAME                         READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/grpcserver   1/1     1            1           77m\n\nNAME                                   DESIRED   CURRENT   READY   AGE\nreplicaset.apps/grpcserver-5455b7d4d   1         1         1       35m\n</code></pre> </li> </ol>"},{"location":"examples/grpc_server/#customize-the-ingress-for-grpcserver","title":"Customize the ingress for grpcserver","text":"<ol> <li> <p>Download the grpcserver ingress manifest.</p> <pre><code>wget https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/examples/grpc/grpcserver-ingress.yaml\n</code></pre> </li> <li> <p>Change the domain name from <code>grpcserver.example.com</code> to your desired domain.</p> </li> <li> <p>The example manifest assumes that you have tagged your subnets for the aws-load-balancer-controller. Otherwise add your subnets using the alb.ingress.kubernetes.io/subnets annotation.</p> </li> <li> <p>Deploy the ingress resource for grpcserver.</p> <pre><code>kubectl apply -f grpcserver-ingress.yaml\n</code></pre> </li> <li> <p>Wait a few minutes for the ALB to provision and for DNS to update.</p> </li> <li> <p>Check the <code>aws-load-balancer-controller</code> logs to ensure the ALB is created. Also ensure that <code>external-dns</code> creates a DNS record that points your domain to the ALB.</p> <pre><code>kubectl logs -n kube-system --tail -1 -l app.kubernetes.io/name=aws-load-balancer-controller | grep 'grpcserver\\/grpcserver'\nkubectl logs -n kube-system --tail -1 -l app.kubernetes.io/name=external-dns | grep 'YOUR_DOMAIN_NAME'\n</code></pre> </li> <li> <p>Next check that your ingress shows the correct ALB address and custom domain name.</p> <pre><code>kubectl get ingress -n grpcserver grpcserver\n</code></pre> <p>You should see similar to the following.</p> <pre><code>NNAME         CLASS    HOSTS              ADDRESS     PORTS    AGE\ngrpcserver     alb   YOUR_DOMAIN_NAME   ALB-DNS-NAME   80      90m\n</code></pre> </li> <li> <p>Finally, test your secure gRPC service by running the greeter client, substituting <code>YOUR_DOMAIN_NAME</code> for the domain you used in the ingress manifest.</p> <pre><code>docker run --rm -it --env BACKEND=YOUR_DOMAIN_NAME placeexchange/grpc-demo:latest python greeter_client.py\n</code></pre> <p>You should see the following response. <pre><code>Greeter client received: Hello, you!\n</code></pre></p> </li> </ol>"},{"location":"examples/secrets_access/","title":"RBAC configuration for secrets resources","text":"<p>In this walkthrough, you will</p> <ul> <li>configure RBAC permissions for the controller to access specific secrets resource in a particular namespace.</li> </ul>"},{"location":"examples/secrets_access/#create-role","title":"Create Role","text":"<ol> <li> <p>Prepare the role manifest with the appropriate name, namespace, and secretName, for example:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n    name: example-role\n    namespace: example-namespace\nrules:\n  - apiGroups:\n       - \"\"\n    resourceNames:\n      - example-secret\n    resources:\n      - secrets\n    verbs:\n      - get\n      - list\n      - watch\n</code></pre> </li> <li> <p>Apply the role manifest</p> <pre><code>kubectl apply -f role.yaml\n</code></pre> </li> </ol>"},{"location":"examples/secrets_access/#create-rolebinding","title":"Create RoleBinding","text":"<ol> <li> <p>Prepare the rolebinding manifest with the appropriate name, namespace and role reference. For example:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n    name: example-rolebinding\n    namespace: example-namespace\nroleRef:\n    apiGroup: rbac.authorization.k8s.io\n    kind: Role\n    name: example-role\nsubjects:\n  - kind: ServiceAccount\n    name: aws-load-balancer-controller\n    namespace: kube-system\n</code></pre> </li> <li> <p>Apply the rolebinding manifest</p> <pre><code>kubectl apply -f rolebinding.yaml\n</code></pre> </li> </ol>"},{"location":"guide/ingress/annotations/","title":"Ingress annotations","text":"<p>You can add annotations to kubernetes Ingress and Service objects to customize their behavior.</p> <ul> <li>Annotation keys and values can only be strings. Advanced format should be encoded as below:<ul> <li>boolean: 'true'</li> <li>integer: '42'</li> <li>stringList: s1,s2,s3</li> <li>stringMap: k1=v1,k2=v2</li> <li>json: 'jsonContent'</li> </ul> </li> <li>Annotations applied to Service have higher priority over annotations applied to Ingress. <code>Location</code> column below indicates where that annotation can be applied to.</li> <li>Annotations that configures LoadBalancer / Listener behaviors have different merge behavior when IngressGroup feature is been used. <code>MergeBehavior</code> column below indicates how such annotation will be merged.<ul> <li>Exclusive: such annotation should only be specified on a single Ingress within IngressGroup or specified with same value across all Ingresses within IngressGroup.</li> <li>Merge: such annotation can be specified on all Ingresses within IngressGroup, and will be merged together.</li> </ul> </li> </ul>"},{"location":"guide/ingress/annotations/#annotations","title":"Annotations","text":"Name Type Default Location MergeBehavior alb.ingress.kubernetes.io/load-balancer-name string N/A Ingress Exclusive alb.ingress.kubernetes.io/group.name string N/A Ingress N/A alb.ingress.kubernetes.io/group.order integer 0 Ingress N/A alb.ingress.kubernetes.io/tags stringMap N/A Ingress,Service Merge alb.ingress.kubernetes.io/ip-address-type ipv4 | dualstack |  dualstack-without-public-ipv4 ipv4 Ingress Exclusive alb.ingress.kubernetes.io/scheme internal | internet-facing internal Ingress Exclusive alb.ingress.kubernetes.io/subnets stringList N/A Ingress Exclusive alb.ingress.kubernetes.io/security-groups stringList N/A Ingress Exclusive alb.ingress.kubernetes.io/manage-backend-security-group-rules boolean N/A Ingress Exclusive alb.ingress.kubernetes.io/customer-owned-ipv4-pool string N/A Ingress Exclusive alb.ingress.kubernetes.io/load-balancer-attributes stringMap N/A Ingress Exclusive alb.ingress.kubernetes.io/wafv2-acl-arn string N/A Ingress Exclusive alb.ingress.kubernetes.io/waf-acl-id string N/A Ingress Exclusive alb.ingress.kubernetes.io/shield-advanced-protection boolean N/A Ingress Exclusive alb.ingress.kubernetes.io/listen-ports json '[{\"HTTP\": 80}]' | '[{\"HTTPS\": 443}]' Ingress Merge alb.ingress.kubernetes.io/ssl-redirect integer N/A Ingress Exclusive alb.ingress.kubernetes.io/inbound-cidrs stringList 0.0.0.0/0, ::/0 Ingress Exclusive alb.ingress.kubernetes.io/security-group-prefix-lists stringList pl-00000000, pl-1111111 Ingress Exclusive alb.ingress.kubernetes.io/certificate-arn stringList N/A Ingress Merge alb.ingress.kubernetes.io/ssl-policy string ELBSecurityPolicy-2016-08 Ingress Exclusive alb.ingress.kubernetes.io/target-type instance | ip instance Ingress,Service N/A alb.ingress.kubernetes.io/backend-protocol HTTP | HTTPS HTTP Ingress,Service N/A alb.ingress.kubernetes.io/backend-protocol-version string HTTP1 Ingress,Service N/A alb.ingress.kubernetes.io/target-group-attributes stringMap N/A Ingress,Service N/A alb.ingress.kubernetes.io/healthcheck-port integer | traffic-port traffic-port Ingress,Service N/A alb.ingress.kubernetes.io/healthcheck-protocol HTTP | HTTPS HTTP Ingress,Service N/A alb.ingress.kubernetes.io/healthcheck-path string / | /AWS.ALB/healthcheck Ingress,Service N/A alb.ingress.kubernetes.io/healthcheck-interval-seconds integer '15' Ingress,Service N/A alb.ingress.kubernetes.io/healthcheck-timeout-seconds integer '5' Ingress,Service N/A alb.ingress.kubernetes.io/healthy-threshold-count integer '2' Ingress,Service N/A alb.ingress.kubernetes.io/unhealthy-threshold-count integer '2' Ingress,Service N/A alb.ingress.kubernetes.io/success-codes string '200' | '12' Ingress,Service N/A alb.ingress.kubernetes.io/auth-type none|oidc|cognito none Ingress,Service N/A alb.ingress.kubernetes.io/auth-idp-cognito json N/A Ingress,Service N/A alb.ingress.kubernetes.io/auth-idp-oidc json N/A Ingress,Service N/A alb.ingress.kubernetes.io/auth-on-unauthenticated-request authenticate|allow|deny authenticate Ingress,Service N/A alb.ingress.kubernetes.io/auth-scope string openid Ingress,Service N/A alb.ingress.kubernetes.io/auth-session-cookie string AWSELBAuthSessionCookie Ingress,Service N/A alb.ingress.kubernetes.io/auth-session-timeout integer '604800' Ingress,Service N/A alb.ingress.kubernetes.io/actions.${action-name} json N/A Ingress N/A alb.ingress.kubernetes.io/conditions.${conditions-name} json N/A Ingress N/A alb.ingress.kubernetes.io/target-node-labels stringMap N/A Ingress,Service N/A alb.ingress.kubernetes.io/mutual-authentication json N/A Ingress Exclusive alb.ingress.kubernetes.io/multi-cluster-target-group boolean N/A Ingress, Service N/A alb.ingress.kubernetes.io/listener-attributes.${Protocol}-${Port} stringMap N/A Ingress Merge alb.ingress.kubernetes.io/minimum-load-balancer-capacity stringMap N/A Ingress Exclusive alb.ingress.kubernetes.io/ipam-ipv4-pool-id string N/A Ingress Exclusive"},{"location":"guide/ingress/annotations/#ingressgroup","title":"IngressGroup","text":"<p>IngressGroup feature enables you to group multiple Ingress resources together. The controller will automatically merge Ingress rules for all Ingresses within IngressGroup and support them with a single ALB. In addition, most annotations defined on an Ingress only apply to the paths defined by that Ingress.</p> <p>By default, Ingresses don't belong to any IngressGroup, and we treat it as a \"implicit IngressGroup\" consisting of the Ingress itself.</p> <ul> <li> <p><code>alb.ingress.kubernetes.io/group.name</code> specifies the group name that this Ingress belongs to.</p> <ul> <li>Ingresses with same <code>group.name</code> annotation will form an \"explicit IngressGroup\".</li> <li>groupName must consist of lower case alphanumeric characters, <code>-</code> or <code>.</code>, and must start and end with an alphanumeric character.</li> <li>groupName must be no more than 63 character.</li> </ul> <p>Security Risk</p> <p>IngressGroup feature should only be used when all Kubernetes users with RBAC permission to create/modify Ingress resources are within trust boundary.</p> <p>If you turn your Ingress to belong a \"explicit IngressGroup\" by adding <code>group.name</code> annotation, other Kubernetes users may create/modify their Ingresses to belong to the same IngressGroup, and can thus add more rules or overwrite existing rules with higher priority to the ALB for your Ingress.</p> <p>We'll add more fine-grained access-control in future versions.</p> <p>Rename behavior</p> <p>The ALB for an IngressGroup is found by searching for an AWS tag <code>ingress.k8s.aws/stack</code> tag with the name of the IngressGroup as its value. For an implicit IngressGroup, the value is <code>namespace/ingressname</code>.</p> <p>When the groupName of an IngressGroup for an Ingress is changed, the Ingress will be moved to a new IngressGroup and be supported by the ALB for the new IngressGroup. If the ALB for the new IngressGroup doesn't exist, a new ALB will be created.</p> <p>If an IngressGroup no longer contains any Ingresses, the ALB for that IngressGroup will be deleted and any deletion protection of that ALB will be ignored.</p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/group.name: my-team.awesome-group\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/group.order</code> specifies the order across all Ingresses within IngressGroup.</p> <ul> <li>You can explicitly denote the order using a number between -1000 and 1000</li> <li>The smaller the order, the rule will be evaluated first. All Ingresses without an explicit order setting get order value as 0</li> <li>Rules with the same order are sorted lexicographically by the Ingress\u2019s namespace/name.</li> </ul> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/group.order: '10'\n</code></pre> </li> </ul>"},{"location":"guide/ingress/annotations/#traffic-listening","title":"Traffic Listening","text":"<p>Traffic Listening can be controlled with the following annotations:</p> <ul> <li> <p><code>alb.ingress.kubernetes.io/listen-ports</code> specifies the ports that ALB listens on.</p> <p>Merge Behavior</p> <p><code>listen-ports</code> is merged across all Ingresses in IngressGroup.</p> <ul> <li>You can define different listen-ports per Ingress, Ingress rules will only impact the ports defined for that Ingress.</li> <li>If same listen-port is defined by multiple Ingress within IngressGroup, Ingress rules will be merged with respect to their group order within IngressGroup.</li> </ul> <p>Default</p> <ul> <li>defaults to <code>'[{\"HTTP\": 80}]'</code> or <code>'[{\"HTTPS\": 443}]'</code> depending on whether <code>certificate-arn</code> is specified.</li> </ul> <p>You may not have duplicate load balancer ports defined.</p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\": 443}, {\"HTTP\": 8080}, {\"HTTPS\": 8443}]'\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/ssl-redirect</code> enables SSLRedirect and specifies the SSL port that redirects to.</p> <p>Merge Behavior</p> <p><code>ssl-redirect</code> is exclusive across all Ingresses in IngressGroup.</p> <ul> <li>Once defined on a single Ingress, it impacts every Ingress within IngressGroup.</li> </ul> <ul> <li>Once enabled SSLRedirect, every HTTP listener will be configured with a default action which redirects to HTTPS, other rules will be ignored.</li> <li>The SSL port that redirects to must exists on LoadBalancer. See alb.ingress.kubernetes.io/listen-ports for the listen ports configuration.</li> </ul> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/ssl-redirect: '443'\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/ip-address-type</code> specifies the IP address type of ALB.</p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/ip-address-type: ipv4\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/customer-owned-ipv4-pool</code> specifies the customer-owned IPv4 address pool for ALB on Outpost.</p> <p>This annotation should be treated as immutable. To remove or change coIPv4Pool, you need to recreate Ingress.</p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/customer-owned-ipv4-pool: ipv4pool-coip-xxxxxxxx\n</code></pre> </li> </ul>"},{"location":"guide/ingress/annotations/#traffic-routing","title":"Traffic Routing","text":"<p>Traffic Routing can be controlled with following annotations:</p> <ul> <li> <p><code>alb.ingress.kubernetes.io/load-balancer-name</code> specifies the custom name to use for the load balancer. Name longer than 32 characters will be treated as an error.</p> <p>Merge Behavior</p> <p><code>name</code> is exclusive across all Ingresses in an IngressGroup.</p> <ul> <li>Once defined on a single Ingress, it impacts every Ingress within the IngressGroup.</li> </ul> <p>Annotation Behavior</p> <ul> <li>This annotation takes effect only during the creation of the Ingress. If the Ingress already exists, the change will not be applied until the Ingress is deleted and recreated.</li> </ul> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/load-balancer-name: custom-name\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/target-type</code> specifies how to route traffic to pods. You can choose between <code>instance</code> and <code>ip</code>:</p> <ul> <li> <p><code>instance</code> mode will route traffic to all ec2 instances within cluster on NodePort opened for your service.</p> <p>service must be of type \"NodePort\" or \"LoadBalancer\" to use <code>instance</code> mode</p> </li> <li> <p><code>ip</code> mode will route traffic directly to the pod IP.</p> <p>network plugin must use secondary IP addresses on ENI for pod IP to use <code>ip</code> mode. e.g.</p> <ul> <li>amazon-vpc-cni-k8s</li> </ul> <p><code>ip</code> mode is required for sticky sessions to work with Application Load Balancers. The Service type does not matter, when using <code>ip</code> mode.</p> </li> </ul> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/target-type: instance\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/target-node-labels</code> specifies which nodes to include in the target group registration for <code>instance</code> target type.</p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/target-node-labels: label1=value1, label2=value2\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/backend-protocol</code> specifies the protocol used when route traffic to pods.</p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/backend-protocol: HTTPS\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/backend-protocol-version</code> specifies the application protocol used to route traffic to pods. Only valid when HTTP or HTTPS is used as the backend protocol.</p> <p>Example</p> <ul> <li>HTTP2     <pre><code>alb.ingress.kubernetes.io/backend-protocol-version: HTTP2\n</code></pre></li> <li>GRPC     <pre><code>alb.ingress.kubernetes.io/backend-protocol-version: GRPC\n</code></pre></li> </ul> </li> <li> <p><code>alb.ingress.kubernetes.io/subnets</code> specifies the Availability Zones that the ALB will route traffic to. See Load Balancer subnets for more details.</p> <p>You must specify at least two subnets in different AZs unless utilizing the outpost locale, in which case a single subnet suffices. Either subnetID or subnetName(Name tag on subnets) can be used.</p> <p>You must not mix subnets from different locales: availability-zone, local-zone, wavelength-zone, outpost.</p> <p>Tip</p> <p>You can enable subnet auto discovery to avoid specifying this annotation on every Ingress. See Subnet Discovery for instructions.</p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/subnets: subnet-xxxx, mySubnet\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/ipam-ipv4-pool-id</code> Specifies the IPv4 IPAM Pool ID which will be used by your load balancer to assign IP addresses.</p> </li> </ul> <p>!!!note \"\"   The chosen IPAM pool is always the prioritized source when assigning public IPv4 addresses.   If there are no more assignable IP addresses in the IPAM pool, AWS managed IPv4 addresses are assigned.</p> <p>!!!tip   To remove an IPAM pool associated to your ALB, remove the annotation from your ingress.</p> <p>!!!example   <pre><code>alb.ingress.kubernetes.io/ipam-ipv4-pool-id: ipam-pool-0f995c17c00375b48\n</code></pre></p> <ul> <li> <p><code>alb.ingress.kubernetes.io/actions.${action-name}</code> Provides a method for configuring custom actions on a listener, such as Redirect Actions.</p> <p>The <code>action-name</code> in the annotation must match the serviceName in the Ingress rules, and servicePort must be <code>use-annotation</code>.</p> <p>use ARN in forward Action</p> <p>ARN can be used in forward action(both simplified schema and advanced schema), it must be an targetGroup created outside of k8s, typically an targetGroup for legacy application.</p> <p>use ServiceName/ServicePort in forward Action</p> <p>ServiceName/ServicePort can be used in forward action(advanced schema only).</p> <p>Auth related annotations on Service object will only be respected if a single TargetGroup in is used.</p> <p>Example</p> <ul> <li>response-503: return fixed 503 response</li> <li>redirect-to-eks: redirect to an external url</li> <li>forward-single-tg: forward to a single targetGroup [simplified schema]</li> <li>forward-multiple-tg: forward to multiple targetGroups with different weights and stickiness config [advanced schema]</li> </ul> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  namespace: default\n  name: ingress\n  annotations:\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/actions.response-503: &gt;\n      {\"type\":\"fixed-response\",\"fixedResponseConfig\":{\"contentType\":\"text/plain\",\"statusCode\":\"503\",\"messageBody\":\"503 error text\"}}\n    alb.ingress.kubernetes.io/actions.redirect-to-eks: &gt;\n      {\"type\":\"redirect\",\"redirectConfig\":{\"host\":\"aws.amazon.com\",\"path\":\"/eks/\",\"port\":\"443\",\"protocol\":\"HTTPS\",\"query\":\"k=v\",\"statusCode\":\"HTTP_302\"}}\n    alb.ingress.kubernetes.io/actions.forward-single-tg: &gt;\n      {\"type\":\"forward\",\"targetGroupARN\": \"arn-of-your-target-group\"}\n    alb.ingress.kubernetes.io/actions.forward-multiple-tg: &gt;\n      {\"type\":\"forward\",\"forwardConfig\":{\"targetGroups\":[{\"serviceName\":\"service-1\",\"servicePort\":\"http\",\"weight\":20},{\"serviceName\":\"service-2\",\"servicePort\":80,\"weight\":20},{\"targetGroupARN\":\"arn-of-your-non-k8s-target-group\",\"weight\":60}],\"targetGroupStickinessConfig\":{\"enabled\":true,\"durationSeconds\":200}}}\nspec:\n  ingressClassName: alb\n  rules:\n    - http:\n        paths:\n          - path: /503\n            pathType: Exact\n            backend:\n              service:\n                name: response-503\n                port:\n                  name: use-annotation\n          - path: /eks\n            pathType: Exact\n            backend:\n              service:\n                name: redirect-to-eks\n                port:\n                  name: use-annotation\n          - path: /path1\n            pathType: Exact\n            backend:\n              service:\n                name: forward-single-tg\n                port:\n                  name: use-annotation\n          - path: /path2\n            pathType: Exact\n            backend:\n              service:\n                name: forward-multiple-tg\n                port:\n                  name: use-annotation\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/conditions.${conditions-name}</code> Provides a method for specifying routing conditions in addition to original host/path condition on Ingress spec.</p> <p>The <code>conditions-name</code> in the annotation must match the serviceName in the Ingress rules. It can be a either real serviceName or an annotation based action name when servicePort is <code>use-annotation</code>.</p> <p>limitations</p> <p>General ALB limitations applies:</p> <ol> <li> <p>Each rule can optionally include up to one of each of the following conditions: host-header, http-request-method, path-pattern, and source-ip. Each rule can also optionally include one or more of each of the following conditions: http-header and query-string.</p> </li> <li> <p>You can specify up to three match evaluations per condition.</p> </li> <li> <p>You can specify up to five match evaluations per rule.</p> </li> </ol> <p>Refer ALB documentation for more details.</p> <p>Example</p> <ul> <li>rule-path1:<ul> <li>Host is www.example.com OR anno.example.com</li> <li>Path is /path1</li> </ul> </li> <li>rule-path2:<ul> <li>Host is www.example.com</li> <li>Path is /path2 OR /anno/path2</li> </ul> </li> <li>rule-path3:<ul> <li>Host is www.example.com</li> <li>Path is /path3</li> <li>Http header HeaderName is HeaderValue1 OR HeaderValue2</li> </ul> </li> <li>rule-path4:<ul> <li>Host is www.example.com</li> <li>Path is /path4</li> <li>Http request method is GET OR HEAD</li> </ul> </li> <li>rule-path5:<ul> <li>Host is www.example.com</li> <li>Path is /path5</li> <li>Query string is paramA:valueA1 OR paramA:valueA2</li> </ul> </li> <li>rule-path6:<ul> <li>Host is www.example.com</li> <li>Path is /path6</li> <li>Source IP is192.168.0.0/16 OR 172.16.0.0/16</li> </ul> </li> <li>rule-path7:<ul> <li>Host is www.example.com</li> <li>Path is /path7</li> <li>Http header HeaderName is HeaderValue</li> <li>Query string is paramA:valueA</li> <li>Query string is paramB:valueB</li> </ul> </li> </ul> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  namespace: default\n  name: ingress\n  annotations:\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/actions.rule-path1: &gt;\n      {\"type\":\"fixed-response\",\"fixedResponseConfig\":{\"contentType\":\"text/plain\",\"statusCode\":\"200\",\"messageBody\":\"Host is www.example.com OR anno.example.com\"}}\n    alb.ingress.kubernetes.io/conditions.rule-path1: &gt;\n      [{\"field\":\"host-header\",\"hostHeaderConfig\":{\"values\":[\"anno.example.com\"]}}]\n    alb.ingress.kubernetes.io/actions.rule-path2: &gt;\n      {\"type\":\"fixed-response\",\"fixedResponseConfig\":{\"contentType\":\"text/plain\",\"statusCode\":\"200\",\"messageBody\":\"Path is /path2 OR /anno/path2\"}}\n    alb.ingress.kubernetes.io/conditions.rule-path2: &gt;\n      [{\"field\":\"path-pattern\",\"pathPatternConfig\":{\"values\":[\"/anno/path2\"]}}]\n    alb.ingress.kubernetes.io/actions.rule-path3: &gt;\n      {\"type\":\"fixed-response\",\"fixedResponseConfig\":{\"contentType\":\"text/plain\",\"statusCode\":\"200\",\"messageBody\":\"Http header HeaderName is HeaderValue1 OR HeaderValue2\"}}\n    alb.ingress.kubernetes.io/conditions.rule-path3: &gt;\n      [{\"field\":\"http-header\",\"httpHeaderConfig\":{\"httpHeaderName\": \"HeaderName\", \"values\":[\"HeaderValue1\", \"HeaderValue2\"]}}]\n    alb.ingress.kubernetes.io/actions.rule-path4: &gt;\n      {\"type\":\"fixed-response\",\"fixedResponseConfig\":{\"contentType\":\"text/plain\",\"statusCode\":\"200\",\"messageBody\":\"Http request method is GET OR HEAD\"}}\n    alb.ingress.kubernetes.io/conditions.rule-path4: &gt;\n      [{\"field\":\"http-request-method\",\"httpRequestMethodConfig\":{\"Values\":[\"GET\", \"HEAD\"]}}]\n    alb.ingress.kubernetes.io/actions.rule-path5: &gt;\n      {\"type\":\"fixed-response\",\"fixedResponseConfig\":{\"contentType\":\"text/plain\",\"statusCode\":\"200\",\"messageBody\":\"Query string is paramA:valueA1 OR paramA:valueA2\"}}\n    alb.ingress.kubernetes.io/conditions.rule-path5: &gt;\n      [{\"field\":\"query-string\",\"queryStringConfig\":{\"values\":[{\"key\":\"paramA\",\"value\":\"valueA1\"},{\"key\":\"paramA\",\"value\":\"valueA2\"}]}}]\n    alb.ingress.kubernetes.io/actions.rule-path6: &gt;\n      {\"type\":\"fixed-response\",\"fixedResponseConfig\":{\"contentType\":\"text/plain\",\"statusCode\":\"200\",\"messageBody\":\"Source IP is 192.168.0.0/16 OR 172.16.0.0/16\"}}\n    alb.ingress.kubernetes.io/conditions.rule-path6: &gt;\n      [{\"field\":\"source-ip\",\"sourceIpConfig\":{\"values\":[\"192.168.0.0/16\", \"172.16.0.0/16\"]}}]\n    alb.ingress.kubernetes.io/actions.rule-path7: &gt;\n      {\"type\":\"fixed-response\",\"fixedResponseConfig\":{\"contentType\":\"text/plain\",\"statusCode\":\"200\",\"messageBody\":\"multiple conditions applies\"}}\n    alb.ingress.kubernetes.io/conditions.rule-path7: &gt;\n      [{\"field\":\"http-header\",\"httpHeaderConfig\":{\"httpHeaderName\": \"HeaderName\", \"values\":[\"HeaderValue\"]}},{\"field\":\"query-string\",\"queryStringConfig\":{\"values\":[{\"key\":\"paramA\",\"value\":\"valueA\"}]}},{\"field\":\"query-string\",\"queryStringConfig\":{\"values\":[{\"key\":\"paramB\",\"value\":\"valueB\"}]}}]\nspec:\n  ingressClassName: alb\n  rules:\n    - host: www.example.com\n      http:\n        paths:\n          - path: /path1\n            pathType: Exact\n            backend:\n              service:\n                name: rule-path1\n                port:\n                  name: use-annotation\n          - path: /path2\n            pathType: Exact\n            backend:\n              service:\n                name: rule-path2\n                port:\n                  name: use-annotation\n          - path: /path3\n            pathType: Exact\n            backend:\n              service:\n                name: rule-path3\n                port:\n                  name: use-annotation\n          - path: /path4\n            pathType: Exact\n            backend:\n              service:\n                name: rule-path4\n                port:\n                  name: use-annotation\n          - path: /path5\n            pathType: Exact\n            backend:\n              service:\n                name: rule-path5\n                port:\n                  name: use-annotation\n          - path: /path6\n            pathType: Exact\n            backend:\n              service:\n                name: rule-path6\n                port:\n                  name: use-annotation\n          - path: /path7\n            pathType: Exact\n            backend:\n              service:\n                name: rule-path7\n                port:\n                  name: use-annotation\n</code></pre> <p>Note</p> <p>If you are using <code>alb.ingress.kubernetes.io/target-group-attributes</code> with <code>stickiness.enabled=true</code>, you should add <code>TargetGroupStickinessConfig</code> under <code>alb.ingress.kubernetes.io/actions.weighted-routing</code></p> <p>Example</p> <pre><code>    apiVersion: networking.k8s.io/v1\n    kind: Ingress\n    metadata:\n    namespace: default\n    name: ingress\n    annotations:\n        alb.ingress.kubernetes.io/scheme: internet-facing\n        alb.ingress.kubernetes.io/target-type: ip\n        alb.ingress.kubernetes.io/target-group-attributes: stickiness.enabled=true,stickiness.lb_cookie.duration_seconds=60\n        alb.ingress.kubernetes.io/actions.weighted-routing: |\n        {\n            \"type\":\"forward\",\n            \"forwardConfig\":{\n            \"targetGroups\":[\n                {\n                \"serviceName\":\"service-1\",\n                \"servicePort\":\"80\",\n                \"weight\":50\n                },\n                {\n                \"serviceName\":\"service-2\",\n                \"servicePort\":\"80\",\n                \"weight\":50\n                }\n            ],\n            \"TargetGroupStickinessConfig\": {\n                \"Enabled\": true,\n                \"DurationSeconds\": 120\n            }\n            }\n        }\n    spec:\n    ingressClassName: alb\n    rules:\n        - host: www.example.com\n        http:\n            paths:\n            - path: /\n                pathType: Prefix\n                backend:\n                service:\n                    name: weighted-routing\n                    port:\n                    name: use-annotation\n</code></pre> </li> </ul>"},{"location":"guide/ingress/annotations/#access-control","title":"Access control","text":"<p>Access control for LoadBalancer can be controlled with following annotations:</p> <ul> <li> <p><code>alb.ingress.kubernetes.io/scheme</code> specifies whether your LoadBalancer will be internet facing. See Load balancer scheme in the AWS documentation for more details.</p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/scheme: internal\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/inbound-cidrs</code> specifies the CIDRs that are allowed to access LoadBalancer.</p> <p>Merge Behavior</p> <p><code>inbound-cidrs</code> is merged across all Ingresses in IngressGroup, but is exclusive per listen-port.</p> <ul> <li>the <code>inbound-cidrs</code> will only impact the ports defined for that Ingress.</li> <li>if same listen-port is defined by multiple Ingress within IngressGroup, <code>inbound-cidrs</code> should only be defined on one of the Ingress.</li> </ul> <p>Default</p> <ul> <li><code>0.0.0.0/0</code> will be used if the IPAddressType is \"ipv4\"</li> <li><code>0.0.0.0/0</code> and <code>::/0</code> will be used if the IPAddressType is \"dualstack\"</li> </ul> <p>this annotation will be ignored if <code>alb.ingress.kubernetes.io/security-groups</code> is specified.</p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/inbound-cidrs: 10.0.0.0/24\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/security-group-prefix-lists</code> specifies the managed prefix lists that are allowed to access LoadBalancer.</p> <p>Merge Behavior</p> <p><code>security-group-prefix-lists</code> is merged across all Ingresses in IngressGroup, but is exclusive per listen-port.</p> <ul> <li>the <code>security-group-prefix-lists</code> will only impact the ports defined for that Ingress.</li> <li>if same listen-port is defined by multiple Ingress within IngressGroup, <code>security-group-prefix-lists</code> should only be defined on one of the Ingress.</li> </ul> <p>This annotation will be ignored if <code>alb.ingress.kubernetes.io/security-groups</code> is specified.</p> <p>If you'd like to use this annotation, make sure your security group rule quota is enough. If you'd like to know how the managed prefix list affects your quota, see the reference in the AWS documentation for more details.</p> <p>If you only use this annotation without <code>inbound-cidrs</code>, the controller managed security group would ignore the <code>inbound-cidrs</code> default settings.</p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/security-group-prefix-lists: pl-000000, pl-111111\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/security-groups</code> specifies the securityGroups you want to attach to LoadBalancer.</p> <p>When this annotation is not present, the controller will automatically create one security group, the security group will be attached to the LoadBalancer and allow access from <code>inbound-cidrs</code> and <code>security-group-prefix-lists</code> to the <code>listen-ports</code>. Also, the securityGroups for Node/Pod will be modified to allow inbound traffic from this securityGroup.</p> <p>If you specify this annotation, you need to configure the security groups on your Node/Pod to allow inbound traffic from the load balancer. You could also set the <code>manage-backend-security-group-rules</code> if you want the controller to manage the access rules.</p> <p>Both name or ID of securityGroups are supported. Name matches a <code>Name</code> tag, not the <code>groupName</code> attribute.</p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/security-groups: sg-xxxx, nameOfSg1, nameOfSg2\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/manage-backend-security-group-rules</code> specifies whether you want the controller to configure security group rules on Node/Pod for traffic access when you specify <code>security-groups</code>.</p> <p>This annotation applies only in case you specify the security groups via <code>security-groups</code> annotation. If set to true, controller attaches an additional shared backend security group to your load balancer. This backend security group is used in the Node/Pod security group rules.</p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/manage-backend-security-group-rules: \"true\"\n</code></pre> </li> </ul>"},{"location":"guide/ingress/annotations/#authentication","title":"Authentication","text":"<p>ALB supports authentication with Cognito or OIDC. See Authenticate Users Using an Application Load Balancer for more details.</p> <p>HTTPS only</p> <p>Authentication is only supported for HTTPS listeners. See TLS for configuring HTTPS listeners.</p> <ul> <li> <p><code>alb.ingress.kubernetes.io/auth-type</code> specifies the authentication type on targets.</p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/auth-type: cognito\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/auth-idp-cognito</code> specifies the cognito idp configuration.</p> <p>If you are using Amazon Cognito Domain, the <code>userPoolDomain</code> should be set to the domain prefix(my-domain) instead of full domain(https://my-domain.auth.us-west-2.amazoncognito.com)</p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/auth-idp-cognito: '{\"userPoolARN\":\"arn:aws:cognito-idp:us-west-2:xxx:userpool/xxx\",\"userPoolClientID\":\"my-clientID\",\"userPoolDomain\":\"my-domain\"}'\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/auth-idp-oidc</code> specifies the oidc idp configuration.</p> <p>You need to create an secret within the same namespace as Ingress to hold your OIDC clientID and clientSecret. The format of secret is as below: <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  namespace: testcase\n  name: my-k8s-secret\ndata:\n  clientID: base64 of your plain text clientId\n  clientSecret: base64 of your plain text clientSecret\n</code></pre></p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/auth-idp-oidc: '{\"issuer\":\"https://example.com\",\"authorizationEndpoint\":\"https://authorization.example.com\",\"tokenEndpoint\":\"https://token.example.com\",\"userInfoEndpoint\":\"https://userinfo.example.com\",\"secretName\":\"my-k8s-secret\"}'\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/auth-on-unauthenticated-request</code> specifies the behavior if the user is not authenticated.</p> <p>options:</p> <ul> <li>authenticate: try authenticate with configured IDP.</li> <li>deny: return an HTTP 401 Unauthorized error.</li> <li>allow: allow the request to be forwarded to the target.</li> </ul> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/auth-on-unauthenticated-request: authenticate\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/auth-scope</code> specifies the set of user claims to be requested from the IDP(cognito or oidc), in a space-separated list.</p> <p>options:</p> <ul> <li>phone</li> <li>email</li> <li>profile</li> <li>openid</li> <li>aws.cognito.signin.user.admin</li> </ul> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/auth-scope: 'email openid'\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/auth-session-cookie</code> specifies the name of the cookie used to maintain session information</p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/auth-session-cookie: custom-cookie\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/auth-session-timeout</code> specifies the maximum duration of the authentication session, in seconds</p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/auth-session-timeout: '86400'\n</code></pre> </li> </ul>"},{"location":"guide/ingress/annotations/#health-check","title":"Health Check","text":"<p>Health check on target groups can be controlled with following annotations:</p> <ul> <li> <p><code>alb.ingress.kubernetes.io/healthcheck-protocol</code> specifies the protocol used when performing health check on targets.</p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/healthcheck-protocol: HTTPS\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/healthcheck-port</code> specifies the port used when performing health check on targets.</p> <p>When using <code>target-type: instance</code> with a service of type \"NodePort\", the healthcheck port can be set to <code>traffic-port</code> to automatically point to the correct port.</p> <p>Example</p> <ul> <li>set the healthcheck port to the traffic port     <pre><code>alb.ingress.kubernetes.io/healthcheck-port: traffic-port\n</code></pre></li> <li>set the healthcheck port to the NodePort(when target-type=instance) or TargetPort(when target-type=ip) of a named port     <pre><code>alb.ingress.kubernetes.io/healthcheck-port: my-port\n</code></pre></li> <li>set the healthcheck port to 80/tcp     <pre><code>alb.ingress.kubernetes.io/healthcheck-port: '80'\n</code></pre></li> </ul> </li> <li> <p><code>alb.ingress.kubernetes.io/healthcheck-path</code> specifies the HTTP path when performing health check on targets.</p> <p>Example</p> <ul> <li>HTTP     <pre><code>alb.ingress.kubernetes.io/healthcheck-path: /ping\n</code></pre></li> <li>GRPC     <pre><code>alb.ingress.kubernetes.io/healthcheck-path: /package.service/method\n</code></pre></li> </ul> </li> <li> <p><code>alb.ingress.kubernetes.io/healthcheck-interval-seconds</code> specifies the interval(in seconds) between health check of an individual target.</p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/healthcheck-interval-seconds: '10'\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/healthcheck-timeout-seconds</code> specifies the timeout(in seconds) during which no response from a target means a failed health check</p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/healthcheck-timeout-seconds: '8'\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/success-codes</code> specifies the HTTP or gRPC status code that should be expected when doing health checks against the specified health check path.</p> <p>Example</p> <ul> <li>use single value     <pre><code>alb.ingress.kubernetes.io/success-codes: '200'\n</code></pre></li> <li>use multiple values     <pre><code>alb.ingress.kubernetes.io/success-codes: 200,201\n</code></pre></li> <li>use range of value     <pre><code>alb.ingress.kubernetes.io/success-codes: 200-300\n</code></pre></li> <li>use gRPC single value     <pre><code>alb.ingress.kubernetes.io/success-codes: '0'\n</code></pre></li> <li>use gRPC multiple value     <pre><code>alb.ingress.kubernetes.io/success-codes: 0,1\n</code></pre></li> <li>use gRPC range of value     <pre><code>alb.ingress.kubernetes.io/success-codes: 0-5\n</code></pre></li> </ul> </li> <li> <p><code>alb.ingress.kubernetes.io/healthy-threshold-count</code> specifies the consecutive health checks successes required before considering an unhealthy target healthy.</p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/healthy-threshold-count: '2'\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/unhealthy-threshold-count</code> specifies the consecutive health check failures required before considering a target unhealthy.</p> <p>Example</p> <p><code>alb.ingress.kubernetes.io/unhealthy-threshold-count: '2'</code></p> </li> </ul>"},{"location":"guide/ingress/annotations/#tls","title":"TLS","text":"<p>TLS support can be controlled with the following annotations:</p> <ul> <li> <p><code>alb.ingress.kubernetes.io/certificate-arn</code> specifies the ARN of one or more certificate managed by AWS Certificate Manager</p> <p>The first certificate in the list will be added as default certificate. And remaining certificate will be added to the optional certificate list. See SSL Certificates for more details.</p> <p>Certificate Discovery</p> <p>TLS certificates for ALB Listeners can be automatically discovered with hostnames from Ingress resources. See Certificate Discovery for instructions.</p> <p>Example</p> <ul> <li>single certificate     <pre><code>alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-west-2:xxxxx:certificate/xxxxxxx\n</code></pre></li> <li>multiple certificates     <pre><code>alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-west-2:xxxxx:certificate/cert1,arn:aws:acm:us-west-2:xxxxx:certificate/cert2,arn:aws:acm:us-west-2:xxxxx:certificate/cert3\n</code></pre></li> </ul> </li> <li> <p><code>alb.ingress.kubernetes.io/ssl-policy</code> specifies the Security Policy that should be assigned to the ALB, allowing you to control the protocol and ciphers.</p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/ssl-policy: ELBSecurityPolicy-TLS-1-1-2017-01\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/mutual-authentication</code>  specifies the mutual authentication configuration that should be assigned to the Application Load Balancer secure listener ports. See Mutual authentication with TLS in the AWS documentation for more details.</p> <p>Note</p> <ul> <li>This annotation is not applicable for Outposts, Local Zones or Wavelength zones.</li> <li>\"Configuration Options\"<ul> <li><code>port: listen port</code> </li> <li>Must be a HTTPS port specified by listen-ports.</li> <li><code>mode: \"off\" (default) | \"passthrough\" | \"verify\"</code></li> <li><code>verify</code> mode requires an existing trust store resource.</li> <li>See Create a trust store in the AWS documentation for more details.</li> <li><code>trustStore: ARN (arn:aws:elasticloadbalancing:trustStoreArn) | Name (my-trust-store)</code></li> <li>Both ARN and Name of trustStore are supported values.</li> <li><code>trustStore</code> is required when mode is <code>verify</code>.</li> <li><code>ignoreClientCertificateExpiry : true | false (default)</code></li> <li><code>advertiseTrustStoreCaNames : \"on\" | \"off\" (default)</code></li> </ul> </li> <li>Once the Mutual Authentication is set, to turn it off, you will have to explicitly pass in this annotation with <code>mode : \"off\"</code>.</li> </ul> <p>Example</p> <ul> <li>listen-ports specifies four HTTPS ports: <code>80, 443, 8080, 8443</code></li> <li>listener <code>HTTPS:80</code> will be set to <code>passthrough</code> mode</li> <li>listener <code>HTTPS:443</code> will be set to <code>verify</code> mode, associated with trust store arn <code>arn:aws:elasticloadbalancing:trustStoreArn</code> and have <code>ignoreClientCertificateExpiry</code> set to <code>true</code></li> <li>listeners <code>HTTPS:8080</code> and <code>HTTPS:8443</code> remain in the default mode <code>off</code>.     <pre><code>alb.ingress.kubernetes.io/listen-ports: '[{\"HTTPS\": 80}, {\"HTTPS\": 443}, {\"HTTPS\": 8080}, {\"HTTPS\": 8443}]'\nalb.ingress.kubernetes.io/mutual-authentication: '[{\"port\": 80, \"mode\": \"passthrough\"},\n                                                   {\"port\": 443, \"mode\": \"verify\", \"trustStore\": \"arn:aws:elasticloadbalancing:trustStoreArn\", \"ignoreClientCertificateExpiry\" : true}]'\n</code></pre></li> </ul> <p>Note</p> <p>To avoid conflict errors in IngressGroup, this annotation should only be specified on a single Ingress within IngressGroup or specified with same value across all Ingresses within IngressGroup.</p> <p>Trust stores limit per Application Load Balancer</p> <p>A maximum of two different trust stores can be associated among listeners on the same ingress. See Quotas for your Application Load Balancers in the AWS documentation for more details.</p> </li> </ul>"},{"location":"guide/ingress/annotations/#custom-attributes","title":"Custom attributes","text":"<p>Custom attributes to LoadBalancers and TargetGroups can be controlled with following annotations:</p> <ul> <li> <p><code>alb.ingress.kubernetes.io/load-balancer-attributes</code> specifies Load Balancer Attributes that should be applied to the ALB.</p> <p>Only attributes defined in the annotation will be updated. To unset any AWS defaults(e.g. Disabling access logs after having them enabled once), the values need to be explicitly set to the original values(<code>access_logs.s3.enabled=false</code>) and omitting them is not sufficient.</p> <ul> <li>If <code>deletion_protection.enabled=true</code> is in annotation, the controller will not be able to delete the ALB during reconciliation. Once the attribute gets edited to <code>deletion_protection.enabled=false</code> during reconciliation, the deployer will force delete the resource.</li> <li>Please note, if the deletion protection is not enabled via annotation (e.g. via AWS console), the controller still deletes the underlying resource.</li> </ul> <p>Example</p> <ul> <li>enable access log to s3     <pre><code>alb.ingress.kubernetes.io/load-balancer-attributes: access_logs.s3.enabled=true,access_logs.s3.bucket=my-access-log-bucket,access_logs.s3.prefix=my-app\n</code></pre></li> <li>enable deletion protection     <pre><code>alb.ingress.kubernetes.io/load-balancer-attributes: deletion_protection.enabled=true\n</code></pre></li> <li>enable invalid header fields removal     <pre><code>alb.ingress.kubernetes.io/load-balancer-attributes: routing.http.drop_invalid_header_fields.enabled=true\n</code></pre></li> <li>enable http2 support     <pre><code>alb.ingress.kubernetes.io/load-balancer-attributes: routing.http2.enabled=true\n</code></pre></li> <li>set idle_timeout delay to 600 seconds     <pre><code>alb.ingress.kubernetes.io/load-balancer-attributes: idle_timeout.timeout_seconds=600\n</code></pre></li> <li>set client_keep_alive to 3600 seconds     <pre><code>alb.ingress.kubernetes.io/load-balancer-attributes: client_keep_alive.seconds=3600  \n</code></pre></li> <li>enable connection logs <pre><code>alb.ingress.kubernetes.io/load-balancer-attributes: connection_logs.s3.enabled=true,connection_logs.s3.bucket=my-connection-log-bucket,connection_logs.s3.prefix=my-app\n</code></pre></li> </ul> <ul> <li><code>alb.ingress.kubernetes.io/target-group-attributes</code> specifies Target Group Attributes which should be applied to Target Groups.</li> </ul> <p>Example</p> <ul> <li>set the slow start duration to 30 seconds (available range is 30-900 seconds)     <pre><code>alb.ingress.kubernetes.io/target-group-attributes: slow_start.duration_seconds=30\n</code></pre></li> <li>set the deregistration delay to 30 seconds (available range is 0-3600 seconds)     <pre><code>alb.ingress.kubernetes.io/target-group-attributes: deregistration_delay.timeout_seconds=30\n</code></pre></li> <li>enable sticky sessions (requires <code>alb.ingress.kubernetes.io/target-type</code> be set to <code>ip</code>)     <pre><code>alb.ingress.kubernetes.io/target-group-attributes: stickiness.enabled=true,stickiness.lb_cookie.duration_seconds=60\nalb.ingress.kubernetes.io/target-type: ip\n</code></pre></li> <li>set load balancing algorithm to least outstanding requests             <pre><code>alb.ingress.kubernetes.io/target-group-attributes: load_balancing.algorithm.type=least_outstanding_requests\n</code></pre></li> <li>enable Automated Target Weights(ATW) on HTTP/HTTPS target groups to increase application availability. Set your load balancing algorithm to weighted random and turn on anomaly mitigation (recommended)     <pre><code>alb.ingress.kubernetes.io/target-group-attributes: load_balancing.algorithm.type=weighted_random,load_balancing.algorithm.anomaly_mitigation=on\n</code></pre></li> </ul> </li> <li> <p><code>alb.ingress.kubernetes.io/multi-cluster-target-group</code> Allows you to share the created Target Group ARN with other Load Balancer Controller managed clusters.</p> <p>This feature does not offer any Deletion Protection. Deleting the resource will still delete the Target Group. If you need to support Target Groups shared with multiple clusters, it's recommended to use an out-of-band Target Group that is not managed by a Load Balancer Controller.</p> <ul> <li>It is not recommended to change this value frequently, if ever. The recommended way to set this value is on creation of the service or ingress.</li> </ul> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/multi-cluster-target-group: \"true\"\n</code></pre> </li> <li> <p><code>alb.ingress.kubernetes.io/listener-attributes.${Protocol}-${Port}</code> specifies Listener Attributes which should be applied to listener.</p> <p>Example</p> <ul> <li>Server header enablement attribute     <pre><code>alb.ingress.kubernetes.io/listener-attributes.HTTP-80: routing.http.response.server.enabled=true\n</code></pre></li> </ul> </li> </ul>"},{"location":"guide/ingress/annotations/#resource-tags","title":"Resource Tags","text":"<p>The AWS Load Balancer Controller automatically applies following tags to the AWS resources (ALB/TargetGroups/SecurityGroups/Listener/ListenerRule) it creates:</p> <ul> <li><code>elbv2.k8s.aws/cluster: ${clusterName}</code></li> <li><code>ingress.k8s.aws/stack: ${stackID}</code></li> <li><code>ingress.k8s.aws/resource: ${resourceID}</code></li> </ul> <p>In addition, you can use annotations to specify additional tags</p> <ul> <li> <p><code>alb.ingress.kubernetes.io/tags</code> specifies additional tags that will be applied to AWS resources created.   In case of target group, the controller will merge the tags from the ingress and the backend service giving precedence   to the values specified on the service when there is conflict.</p> <p>Example</p> <pre><code>alb.ingress.kubernetes.io/tags: Environment=dev,Team=test\n</code></pre> </li> </ul>"},{"location":"guide/ingress/annotations/#capacity-unit-reservation","title":"Capacity Unit Reservation","text":"<p>Load balancer capacity unit reservation can be configured via following annotations:</p> <ul> <li> <p><code>alb.ingress.kubernetes.io/minimum-load-balancer-capacity</code> specifies the   Capacity Unit Reservation to be configured.</p> <p>Example</p> <ul> <li>set the capacity unit reservation to 1000   <pre><code>alb.ingress.kubernetes.io/minimum-load-balancer-capacity: CapacityUnits=1000\n</code></pre></li> <li>reset the capacity unit reservation   <pre><code>alb.ingress.kubernetes.io/minimum-load-balancer-capacity: CapacityUnits=0\n</code></pre></li> </ul> <p>Notes</p> <ul> <li>If you specify this annotation, but remove it later, the capacity unit reservation is not reset. You need to reset the capacity by setting the capacity units to zero as show in the example above.</li> <li>If users do not want the controller to manage the capacity unit reservation on load balancer, they can disable the feature by setting controller command line feature gate flag <code>--feature-gates=LBCapacityReservation=true</code></li> </ul> </li> </ul>"},{"location":"guide/ingress/annotations/#addons","title":"Addons","text":"<ul> <li> <p><code>alb.ingress.kubernetes.io/waf-acl-id</code> specifies the identifier for the Amazon WAF Classic web ACL.</p> <p>Only Regional WAF Classic is supported.</p> <p>When this annotation is absent or empty, the controller will keep LoadBalancer WAF Classic settings unchanged. To disable WAF Classic, explicitly set the annotation value to 'none'.</p> <p>Example</p> <ul> <li>enable WAF Classic     <code>alb.ingress.kubernetes.io/waf-acl-id: 499e8b99-6671-4614-a86d-adb1810b7fbe</code></li> <li>disable WAF Classic     <code>alb.ingress.kubernetes.io/waf-acl-id: none</code></li> </ul> </li> <li> <p><code>alb.ingress.kubernetes.io/wafv2-acl-arn</code> specifies ARN for the Amazon WAFv2 web ACL.</p> <p>Only Regional WAFv2 is supported.</p> <p>When this annotation is absent or empty, the controller will keep LoadBalancer WAFv2 settings unchanged. To disable WAFv2, explicitly set the annotation value to 'none'.</p> <p>To get the WAFv2 Web ACL ARN from the Console, click the gear icon in the upper right and enable the ARN column.</p> <p>Example</p> <ul> <li>enable WAFv2     <code>alb.ingress.kubernetes.io/wafv2-acl-arn: arn:aws:wafv2:us-west-2:xxxxx:regional/webacl/xxxxxxx/3ab78708-85b0-49d3-b4e1-7a9615a6613b</code></li> <li>disable WAFV2     <code>alb.ingress.kubernetes.io/wafv2-acl-arn: none</code></li> </ul> </li> <li> <p><code>alb.ingress.kubernetes.io/shield-advanced-protection</code> turns on / off the AWS Shield Advanced protection for the load balancer.</p> <p>When this annotation is absent, the controller will keep LoadBalancer shield protection settings unchanged. To disable shield protection, explicitly set the annotation value to 'false'.</p> <p>Example</p> <ul> <li>enable shield protection     <code>alb.ingress.kubernetes.io/shield-advanced-protection: 'true'</code></li> <li>disable shield protection     <code>alb.ingress.kubernetes.io/shield-advanced-protection: 'false'</code></li> </ul> </li> </ul>"},{"location":"guide/ingress/cert_discovery/","title":"Certificate Discovery","text":"<p>TLS certificates for ALB Listeners can be automatically discovered with hostnames from Ingress resources if the <code>spec.certificateArn</code> in <code>IngressClassParams</code> or <code>alb.ingress.kubernetes.io/certificate-arn</code> annotation is not specified.</p> <p>The controller will attempt to discover TLS certificates from the <code>tls</code> field in Ingress and <code>host</code> field in Ingress rules.</p> <p>You need to explicitly specify to use HTTPS listener with listen-ports annotation.</p>"},{"location":"guide/ingress/cert_discovery/#discover-via-ingress-tls","title":"Discover via Ingress tls","text":"<p>Example</p> <pre><code>- attaches certs for `www.example.com` to the ALB\n    ```yaml\n    apiVersion: networking.k8s.io/v1\n    kind: Ingress\n    metadata:\n    namespace: default\n    name: ingress\n    annotations:\n      alb.ingress.kubernetes.io/listen-ports: '[{\"HTTPS\":443}]'\n    spec:\n      ingressClassName: alb\n      tls:\n      - hosts:\n        - www.example.com\n      rules:\n      - http:\n          paths:\n          - path: /users\n            pathType: Prefix\n            backend:\n              service:\n                name: user-service\n                port:\n                  number: 80\n    ```\n</code></pre>"},{"location":"guide/ingress/cert_discovery/#discover-via-ingress-rule-host","title":"Discover via Ingress rule host.","text":"<p>Example</p> <pre><code>- attaches a cert for `dev.example.com` or `*.example.com` to the ALB\n    ```yaml\n    apiVersion: networking.k8s.io/v1\n    kind: Ingress\n    metadata:\n    namespace: default\n    name: ingress\n    annotations:\n      alb.ingress.kubernetes.io/listen-ports: '[{\"HTTPS\":443}]'\n    spec:\n      ingressClassName: alb\n      rules:\n      - host: dev.example.com\n        http:\n          paths:\n          - path: /users\n            pathType: Prefix\n            backend:\n              service:\n                name: user-service\n                port:\n                  number: 80\n    ```\n</code></pre>"},{"location":"guide/ingress/ingress_class/","title":"IngressClass","text":"<p>Ingresses can be implemented by different controllers, often with different configuration. Each Ingress should specify a class, a reference to an IngressClass resource that contains additional configuration including the name of the controller that should implement the class. IngressClass resources contain an optional parameters field. This can be used to reference additional implementation-specific configuration for this class. For the AWS Load Balancer controller, the implementation-specific configuration is IngressClassParams in the <code>elbv2.k8s.aws</code> API group.</p> <p>Example</p> <ul> <li>specify controller as <code>ingress.k8s.aws/alb</code> to denote Ingresses should be managed by AWS Load Balancer Controller. <pre><code>apiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  name: awesome-class\nspec:\n  controller: ingress.k8s.aws/alb\n</code></pre></li> <li>specify additional configurations by referencing an IngressClassParams resource. <pre><code>apiVersion: networking.k8s.io/v1\nkind: IngressClass\nmetadata:\n  name: awesome-class\nspec:\n  controller: ingress.k8s.aws/alb\n  parameters:\n    apiGroup: elbv2.k8s.aws\n    kind: IngressClassParams\n    name: awesome-class-cfg\n</code></pre></li> </ul> <p>default IngressClass</p> <p>You can mark a particular IngressClass as the default for your cluster. Setting the <code>ingressclass.kubernetes.io/is-default-class</code> annotation to <code>true</code> on an IngressClass resource will ensure that new Ingresses without an <code>ingressClassName</code> field specified will be assigned this default IngressClass.</p>"},{"location":"guide/ingress/ingress_class/#deprecated-kubernetesioingressclass-annotation","title":"Deprecated <code>kubernetes.io/ingress.class</code> annotation","text":"<p>Before the IngressClass resource and <code>ingressClassName</code> field were added in Kubernetes 1.18, Ingress classes were specified with a <code>kubernetes.io/ingress.class</code> annotation on the Ingress. This annotation was never formally defined, but was widely supported by Ingress controllers.</p> <p>The newer <code>ingressClassName</code> field on Ingresses is a replacement for that annotation, but is not a direct equivalent. While the annotation was generally used to reference the name of the Ingress controller that should implement the Ingress, the field is a reference to an IngressClass resource that contains additional Ingress configuration, including the name of the Ingress controller.</p> <p>disable <code>kubernetes.io/ingress.class</code> annotation</p> <p>In order to maintain backwards-compatibility, <code>kubernetes.io/ingress.class</code> annotation is still supported currently. You can enforce IngressClass resource adoption by disabling the <code>kubernetes.io/ingress.class</code> annotation via --disable-ingress-class-annotation controller flag.</p>"},{"location":"guide/ingress/ingress_class/#ingressclassparams","title":"IngressClassParams","text":"<p>IngressClassParams is a CRD specific to the AWS Load Balancer Controller, which can be used along with IngressClass\u2019s parameter field. You can use IngressClassParams to enforce settings for a set of Ingresses.</p> <p>Example</p> <ul> <li>with scheme &amp; ipAddressType &amp; tags <pre><code>apiVersion: elbv2.k8s.aws/v1beta1\nkind: IngressClassParams\nmetadata:\n  name: awesome-class\nspec:\n  scheme: internal\n  ipAddressType: dualstack\n  tags:\n  - key: org\n    value: my-org\n</code></pre></li> <li>with namespaceSelector <pre><code>apiVersion: elbv2.k8s.aws/v1beta1\nkind: IngressClassParams\nmetadata:\n  name: awesome-class\nspec:\n  namespaceSelector:\n    matchLabels:\n      team: team-a\n</code></pre></li> <li>with IngressGroup <pre><code>apiVersion: elbv2.k8s.aws/v1beta1\nkind: IngressClassParams\nmetadata:\n  name: awesome-class\nspec:\n  group:\n    name: my-group\n</code></pre></li> <li>with loadBalancerAttributes <pre><code>apiVersion: elbv2.k8s.aws/v1beta1\nkind: IngressClassParams\nmetadata:\n    name: awesome-class\nspec:\n  loadBalancerAttributes:\n  - key: deletion_protection.enabled\n    value: \"true\"\n  - key: idle_timeout.timeout_seconds\n    value: \"120\"\n</code></pre></li> <li>with subnets.ids <pre><code>apiVersion: elbv2.k8s.aws/v1beta1\nkind: IngressClassParams\nmetadata:\n  name: awesome-class\nspec:\n  subnets:\n    ids:\n    - subnet-xxx\n    - subnet-123\n</code></pre></li> <li>with subnets.tags <pre><code>apiVersion: elbv2.k8s.aws/v1beta1\nkind: IngressClassParams\nmetadata:\n  name: class2048-config\nspec:\n  subnets:\n  tags:\n    kubernetes.io/role/internal-elb:\n    - \"1\"\n    myKey:\n    - myVal0\n    - myVal1\n</code></pre></li> <li>with certificateArn <pre><code>apiVersion: elbv2.k8s.aws/v1beta1\nkind: IngressClassParams\nmetadata:\nname: class2048-config\nspec:\n  certificateArn: ['arn:aws:acm:us-east-1:123456789:certificate/test-arn-1','arn:aws:acm:us-east-1:123456789:certificate/test-arn-2']\n</code></pre></li> <li>with minimumLoadBalancerCapacity.capacityUnits <pre><code>apiVersion: elbv2.k8s.aws/v1beta1\nkind: IngressClassParams\nmetadata:\n  name: class2048-config\nspec:\n  minimumLoadBalancerCapacity:\n    capacityUnits: 1000\n</code></pre></li> </ul>"},{"location":"guide/ingress/ingress_class/#ingressclassparams-specification","title":"IngressClassParams specification","text":""},{"location":"guide/ingress/ingress_class/#specnamespaceselector","title":"spec.namespaceSelector","text":"<p><code>namespaceSelector</code> is an optional setting that follows general Kubernetes label selector semantics.</p> <p>Cluster administrators can use the <code>namespaceSelector</code> field to restrict the namespaces of Ingresses that are allowed to specify the IngressClass.</p> <ol> <li>If <code>namespaceSelector</code> specified, only Ingresses in selected namespaces can use IngressClasses with this parameter. The controller will refuse to reconcile for Ingresses that violates <code>namespaceSelector</code>.</li> <li>If <code>namespaceSelector</code> un-specified, all Ingresses in any namespace can use IngressClasses with this parameter.</li> </ol>"},{"location":"guide/ingress/ingress_class/#specgroup","title":"spec.group","text":"<p><code>group</code> is an optional setting.  The only available sub-field is <code>group.name</code>.</p> <p>Cluster administrators can use <code>group.name</code> field to denote the groupName for all Ingresses belong to this IngressClass.</p> <ol> <li>If <code>group.name</code> specified, all Ingresses with this IngressClass will belong to the same IngressGroup specified and result in a single ALB. If <code>group.name</code> is not specified, Ingresses with this IngressClass can use the older / legacy <code>alb.ingress.kubernetes.io/group.name</code> annotation to specify their IngressGroup. Ingresses that belong to the same IngressClass can form different IngressGroups via that annotation.</li> </ol>"},{"location":"guide/ingress/ingress_class/#specscheme","title":"spec.scheme","text":"<p><code>scheme</code> is an optional setting. The available options are <code>internet-facing</code> or <code>internal</code>.</p> <p>Cluster administrators can use the <code>scheme</code> field to restrict the scheme for all Ingresses that belong to this IngressClass.</p> <ol> <li>If <code>scheme</code> specified, all Ingresses with this IngressClass will have the specified scheme.</li> <li>If <code>scheme</code> un-specified, Ingresses with this IngressClass can continue to use <code>alb.ingress.kubernetes.io/scheme annotation</code> to specify scheme.</li> </ol>"},{"location":"guide/ingress/ingress_class/#specinboundcidrs","title":"spec.inboundCIDRs","text":"<p>Cluster administrators can use the optional <code>inboundCIDRs</code> field to specify the CIDRs that are allowed to access the load balancers that belong to this IngressClass. If the field is specified, LBC will ignore the <code>alb.ingress.kubernetes.io/inbound-cidrs</code> annotation.</p>"},{"location":"guide/ingress/ingress_class/#speccertificatearn","title":"spec.certificateArn","text":"<p>Cluster administrators can use the optional <code>certificateARN</code> field to specify the ARN of the certificates for all Ingresses that belong to IngressClass with this IngressClassParams.</p> <p>If the field is specified, LBC will ignore the <code>alb.ingress.kubernetes.io/certificate-arn</code> annotation.</p>"},{"location":"guide/ingress/ingress_class/#specsslpolicy","title":"spec.sslPolicy","text":"<p>Cluster administrators can use the optional <code>sslPolicy</code> field to specify the SSL policy for the load balancers that belong to this IngressClass. If the field is specified, LBC will ignore the <code>alb.ingress.kubernetes.io/ssl-policy</code> annotation.</p>"},{"location":"guide/ingress/ingress_class/#specsubnets","title":"spec.subnets","text":"<p>Cluster administrators can use the optional <code>subnets</code> field to specify the subnets for the load balancers that belong to this IngressClass. They may specify either <code>ids</code> or <code>tags</code>. If the field is specified, LBC will ignore the <code>alb.ingress.kubernetes.io/subnets annotation</code> annotation.</p>"},{"location":"guide/ingress/ingress_class/#specsubnetsids","title":"spec.subnets.ids","text":"<p>If <code>ids</code> is specified, it must be a set of at least one resource ID of a subnet in the VPC. No two subnets may be in the same availability zone.</p>"},{"location":"guide/ingress/ingress_class/#specsubnetstags","title":"spec.subnets.tags","text":"<p>If <code>tags</code> is specified, it is a map of tag filters. The filters will match subnets in the VPC for which each listed tag key is present and has one of the corresponding tag values.</p> <p>Unless the <code>SubnetsClusterTagCheck</code> feature gate is disabled, subnets without a cluster tag and with the cluster tag for another cluster will be excluded.</p> <p>Within any given availability zone, subnets with a cluster tag will be chosen over subnets without, then the subnet with the lowest-sorting resource ID will be chosen.</p>"},{"location":"guide/ingress/ingress_class/#specipaddresstype","title":"spec.ipAddressType","text":"<p><code>ipAddressType</code> is an optional setting. The available options are <code>ipv4</code>, <code>dualstack</code>, or <code>dualstack-without-public-ipv4</code>.</p> <p>Cluster administrators can use <code>ipAddressType</code> field to restrict the ipAddressType for all Ingresses that belong to this IngressClass.</p> <ol> <li>If <code>ipAddressType</code> specified, all Ingresses with this IngressClass will have the specified ipAddressType.</li> <li>If <code>ipAddressType</code> un-specified, Ingresses with this IngressClass can continue to use <code>alb.ingress.kubernetes.io/ip-address-type</code> annotation to specify ipAddressType.</li> </ol>"},{"location":"guide/ingress/ingress_class/#spectags","title":"spec.tags","text":"<p><code>tags</code> is an optional setting.</p> <p>Cluster administrators can use <code>tags</code> field to specify the custom tags for AWS resources provisioned for all Ingresses belong to this IngressClass.</p> <ol> <li>If <code>tags</code> is set, AWS resources provisioned for all Ingresses with this IngressClass will have the specified tags.</li> <li>You can also use controller-level flag <code>--default-tags</code>  or <code>alb.ingress.kubernetes.io/tags</code> annotation to specify custom tags. These tags will be merged together based on tag-key. If same tag-key appears in multiple sources, the priority is as follows:<ol> <li>controller-level flag <code>--default-tags</code> will have the highest priority.</li> <li><code>spec.tags</code> in IngressClassParams will have the middle priority.</li> <li><code>alb.ingress.kubernetes.io/tags</code> annotation will have the lowest priority.</li> </ol> </li> </ol>"},{"location":"guide/ingress/ingress_class/#specloadbalancerattributes","title":"spec.loadBalancerAttributes","text":"<p><code>loadBalancerAttributes</code> is an optional setting.</p> <p>Cluster administrators can use <code>loadBalancerAttributes</code> field to specify the Load Balancer Attributes that should be applied to the load balancers that belong to this IngressClass. You can specify the list of load balancer attribute name and the desired value in the <code>spec.loadBalancerAttributes</code> field.</p> <ol> <li>If <code>loadBalancerAttributes</code> is set, the attributes defined will be applied to the load balancer that belong to this IngressClass. If you specify invalid keys or values for the load balancer attributes, the controller will fail to reconcile ingresses belonging to the particular ingress class.</li> <li>If <code>loadBalancerAttributes</code> un-specified, Ingresses with this IngressClass can continue to use <code>alb.ingress.kubernetes.io/load-balancer-attributes</code> annotation to specify the load balancer attributes.</li> </ol>"},{"location":"guide/ingress/ingress_class/#specminimumloadbalancercapacity","title":"spec.minimumLoadBalancerCapacity","text":"<p>Cluster administrators can use the optional <code>minimumLoadBalancerCapacity</code> field to specify the capacity reservation for the load balancers that belong to this IngressClass. They may specify <code>capacityUnits</code>. If the field is specified, LBC will ignore the <code>alb.ingress.kubernetes.io/minimum-load-balancer-capacity annotation</code> annotation.</p>"},{"location":"guide/ingress/ingress_class/#specminimumloadbalancercapacitycapacityunits","title":"spec.minimumLoadBalancerCapacity.capacityUnits","text":"<p>If <code>capacityUnits</code> is specified, it must be to valid positive value greater than 0. If set to 0, the LBC will reset the capacity reservation for the load balancer.</p>"},{"location":"guide/ingress/ingress_class/#specipv4ipampoolid","title":"spec.ipv4IPAMPoolId","text":"<p>The IPAM pool you choose will be the preferred source of public IPv4 addresses. If the pool is depleted, IPv4 addresses will be assigned by AWS. To remove the IPAM pool from your ALB, remove <code>spec.ipv4IPAMPoolId</code> from the IngressClass definition.</p>"},{"location":"guide/ingress/spec/","title":"Ingress specification","text":"<p>This document covers how ingress resources work in relation to The AWS Load Balancer Controller.</p> <ul> <li>Beginning from v2.4.3 of the AWS LBC, rules are ordered as follows:  <ul> <li><code>pathType: Exact</code> paths are always ordered first </li> <li>followed by <code>pathType: Prefix</code> paths, with the longest prefix first</li> <li>followed by <code>pathType: ImplementationSpecific</code> paths, in the order they are listed in the manifest</li> </ul> </li> </ul> <p>An example ingress, from example is as follows.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: \"2048-ingress\"\n  namespace: \"2048-game\"\n  labels:\n    app: 2048-nginx-ingress\nspec:\n  ingressClassName: alb\n  rules:\n    - host: 2048.example.com\n      http:\n        paths:\n          - path: /*\n            pathType: ImplementationSpecific\n            backend:\n              service:\n                name: \"service-2048\"\n                port:\n                  number: 80\n</code></pre> <p>The host field specifies the eventual Route 53-managed domain that will route to this service.</p> <p>The service, service-2048, must be of type NodePort in order for the provisioned ALB to route to it.(see echoserver-service.yaml)</p> <p>The AWS Load Balancer Controller does not support the <code>resource</code> field of <code>backend</code>.</p>"},{"location":"guide/integrations/external_dns/","title":"Setup External DNS","text":"<p>external-dns provisions DNS records based on the host information. This project will setup and manage records in Route 53 that point to controller deployed ALBs.</p>"},{"location":"guide/integrations/external_dns/#prerequisites","title":"Prerequisites","text":""},{"location":"guide/integrations/external_dns/#role-permissions","title":"Role Permissions","text":"<p>Adequate roles and policies must be configured in AWS and available to the node(s) running the external-dns. See external-dns tutorial.</p>"},{"location":"guide/integrations/external_dns/#installation","title":"Installation","text":"<ol> <li> <p>Download sample <code>external-dns</code> manifest</p> <pre><code>wget https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/examples/external-dns.yaml\n</code></pre> </li> <li> <p>Edit the <code>--domain-filter</code> flag to include your hosted zone(s)</p> <p>The following example is for a hosted zone <code>test-dns.com</code>:</p> <pre><code>args:\n- --source=service\n- --source=ingress\n- --domain-filter=test-dns.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones\n- --provider=aws\n- --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization\n- --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both)\n- --registry=txt\n- --txt-owner-id=my-identifier\n</code></pre> </li> <li> <p>Deploy external-dns</p> <pre><code>kubectl apply -f external-dns.yaml\n</code></pre> </li> <li> <p>Verify it deployed successfully.</p> <pre><code>kubectl logs -f $(kubectl get po | egrep -o 'external-dns[A-Za-z0-9-]+')\n</code></pre> <p>Should display output similar to the following: <pre><code>time=\"2019-12-11T10:26:05Z\" level=info msg=\"config: {Master: KubeConfig: RequestTimeout:30s IstioIngressGateway:istio-system/istio-ingressgateway Sources:[service ingress] Namespace: AnnotationFilter: FQDNTemplate: CombineFQDNAndAnnotation:false Compatibility: PublishInternal:false PublishHostIP:false ConnectorSourceServer:localhost:8080 Provider:aws GoogleProject: DomainFilter:[test-dns.com] ZoneIDFilter:[] AlibabaCloudConfigFile:/etc/kubernetes/alibaba-cloud.json AlibabaCloudZoneType: AWSZoneType:public AWSAssumeRole: AWSBatchChangeSize:4000 AWSBatchChangeInterval:1s AWSEvaluateTargetHealth:true AzureConfigFile:/etc/kubernetes/azure.json AzureResourceGroup: CloudflareProxied:false InfobloxGridHost: InfobloxWapiPort:443 InfobloxWapiUsername:admin InfobloxWapiPassword: InfobloxWapiVersion:2.3.1 InfobloxSSLVerify:true DynCustomerName: DynUsername: DynPassword: DynMinTTLSeconds:0 OCIConfigFile:/etc/kubernetes/oci.yaml InMemoryZones:[] PDNSServer:http://localhost:8081 PDNSAPIKey: PDNSTLSEnabled:false TLSCA: TLSClientCert: TLSClientCertKey: Policy:upsert-only Registry:txt TXTOwnerID:my-identifier TXTPrefix: Interval:1m0s Once:false DryRun:false LogFormat:text MetricsAddress::7979 LogLevel:info TXTCacheInterval:0s ExoscaleEndpoint:https://api.exoscale.ch/dns ExoscaleAPIKey: ExoscaleAPISecret: CRDSourceAPIVersion:externaldns.k8s.io/v1alpha CRDSourceKind:DNSEndpoint ServiceTypeFilter:[] RFC2136Host: RFC2136Port:0 RFC2136Zone: RFC2136Insecure:false RFC2136TSIGKeyName: RFC2136TSIGSecret: RFC2136TSIGSecretAlg: RFC2136TAXFR:false}\"\ntime=\"2019-12-11T10:26:05Z\" level=info msg=\"Created Kubernetes client https://10.100.0.1:443\"\n</code></pre></p> </li> </ol>"},{"location":"guide/integrations/external_dns/#usage","title":"Usage","text":"<ol> <li> <p>To create a record set in the subdomain, from your ingress which has been created by the ingress-controller, add the following annotation in the ingress objectresource:</p> <pre><code>annotations:\n  alb.ingress.kubernetes.io/scheme: internet-facing\n\n  # external-dns specific configuration for creating route53 record-set\n  external-dns.alpha.kubernetes.io/hostname: my-app.test-dns.com # give your domain name here\n</code></pre> </li> <li> <p>A snippet of the external-dns pod log indicating route53 update:</p> <pre><code>time=\"2019-12-11T10:26:08Z\" level=info msg=\"Desired change: CREATE my-app.test-dns.com A\"\ntime=\"2019-12-11T10:26:08Z\" level=info msg=\"Desired change: CREATE my-app.test-dns.com TXT\"\ntime=\"2019-12-11T10:26:08Z\" level=info msg=\"2 record(s) in zone my-app.test-dns.com. were successfully updated\"\n</code></pre> </li> <li> <p>External DNS configures <code>Simple</code> routing policy for the route53 records. You can configure <code>Weighted</code> policy by specifying the weight and the identifier via annotation. <code>Weighted</code> policy allows you to split the traffic between multiple load balancers. Here is an example to specify weight and identifier:    <pre><code> annotations:\n   # For creating weighted route53 records\n   external-dns.alpha.kubernetes.io/hostname: my-app.test-dns.com\n   external-dns.alpha.kubernetes.io/aws-weight: \"100\"\n   external-dns.alpha.kubernetes.io/set-identifier: \"3\"\n</code></pre>    You can refer to the External DNS documentation for further details [link]. </p> </li> </ol>"},{"location":"guide/service/annotations/","title":"Annotations","text":""},{"location":"guide/service/annotations/#service-annotations","title":"Service annotations","text":"<ul> <li>Annotation keys and values can only be strings. All other types below must be string-encoded, for example:<ul> <li>boolean: <code>\"true\"</code></li> <li>integer: <code>\"42\"</code></li> <li>stringList: <code>\"s1,s2,s3\"</code></li> <li>stringMap: <code>\"k1=v1,k2=v2\"</code></li> <li>json: <code>\"{ \\\"key\\\": \\\"value\\\" }\"</code></li> </ul> </li> </ul>"},{"location":"guide/service/annotations/#annotations","title":"Annotations","text":"<p>Warning</p> <p>These annotations are specific to the kubernetes service resources reconciled by the AWS Load Balancer Controller. Although the list was initially derived from the k8s in-tree <code>kube-controller-manager</code>, this documentation is not an accurate reference for the services reconciled by the in-tree controller. </p> Name Type Default Notes service.beta.kubernetes.io/load-balancer-source-ranges stringList service.beta.kubernetes.io/aws-load-balancer-security-group-prefix-lists stringList service.beta.kubernetes.io/aws-load-balancer-type string service.beta.kubernetes.io/aws-load-balancer-nlb-target-type string default <code>instance</code> in case of LoadBalancerClass service.beta.kubernetes.io/aws-load-balancer-name string service.beta.kubernetes.io/aws-load-balancer-internal boolean false deprecated, in favor of aws-load-balancer-scheme service.beta.kubernetes.io/aws-load-balancer-scheme string internal service.beta.kubernetes.io/aws-load-balancer-proxy-protocol string Set to <code>\"*\"</code> to enable service.beta.kubernetes.io/aws-load-balancer-ip-address-type string ipv4 ipv4 | dualstack service.beta.kubernetes.io/aws-load-balancer-access-log-enabled boolean false deprecated, in favor of aws-load-balancer-attributes service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name string deprecated, in favor of aws-load-balancer-attributes service.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix string deprecated, in favor of aws-load-balancer-attributes service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled boolean false deprecated, in favor of aws-load-balancer-attributes service.beta.kubernetes.io/aws-load-balancer-ssl-cert stringList service.beta.kubernetes.io/aws-load-balancer-ssl-ports stringList service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy string ELBSecurityPolicy-2016-08 service.beta.kubernetes.io/aws-load-balancer-backend-protocol string service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags stringMap service.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol string TCP service.beta.kubernetes.io/aws-load-balancer-healthcheck-port  integer | traffic-port traffic-port service.beta.kubernetes.io/aws-load-balancer-healthcheck-path string \"/\" for HTTP(S) protocols service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold integer 3 service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold integer 3 service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout integer 10 service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval integer 10 service.beta.kubernetes.io/aws-load-balancer-healthcheck-success-codes string 200-399 service.beta.kubernetes.io/aws-load-balancer-eip-allocations stringList internet-facing lb only. Length must match the number of subnets service.beta.kubernetes.io/aws-load-balancer-private-ipv4-addresses stringList internal lb only. Length must match the number of subnets service.beta.kubernetes.io/aws-load-balancer-ipv6-addresses stringList dualstack lb only. Length must match the number of subnets service.beta.kubernetes.io/aws-load-balancer-target-group-attributes stringMap service.beta.kubernetes.io/aws-load-balancer-subnets stringList service.beta.kubernetes.io/aws-load-balancer-alpn-policy string service.beta.kubernetes.io/aws-load-balancer-target-node-labels stringMap service.beta.kubernetes.io/aws-load-balancer-attributes stringMap service.beta.kubernetes.io/aws-load-balancer-security-groups stringList service.beta.kubernetes.io/aws-load-balancer-manage-backend-security-group-rules boolean true If <code>service.beta.kubernetes.io/aws-load-balancer-security-groups</code> is specified, this must also be explicitly specified otherwise it defaults to <code>false</code>. service.beta.kubernetes.io/aws-load-balancer-inbound-sg-rules-on-private-link-traffic string service.beta.kubernetes.io/aws-load-balancer-listener-attributes.${Protocol}-${Port} stringMap service.beta.kubernetes.io/aws-load-balancer-multi-cluster-target-group boolean false If specified, the controller will only operate on targets that exist within the cluster, ignoring targets from other sources. service.beta.kubernetes.io/aws-load-balancer-enable-prefix-for-ipv6-source-nat string off Optional annotation. dualstack lb only. Allowed values - on and off service.beta.kubernetes.io/aws-load-balancer-source-nat-ipv6-prefixes stringList Optional annotation. dualstack lb only. This annotation is only applicable when user has to set the service.beta.kubernetes.io/aws-load-balancer-enable-prefix-for-ipv6-source-nat to \"on\". Length must match the number of subnets service.beta.kubernetes.io/aws-load-balancer-minimum-load-balancer-capacity stringMap service.beta.kubernetes.io/aws-load-balancer-enable-icmp-for-path-mtu-discovery string If specified, a security group rule is added to the managed security group to allow explicit ICMP traffic for Path MTU discovery for IPv4 and dual-stack VPCs. Creates a rule for each source range if <code>service.beta.kubernetes.io/load-balancer-source-ranges</code> is present."},{"location":"guide/service/annotations/#traffic-routing","title":"Traffic Routing","text":"<p>Traffic Routing can be controlled with following annotations:</p> <ul> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-name</code> specifies the custom name to use for the load balancer. Name longer than 32 characters will be treated as an error.</p> <p>limitations</p> <ul> <li>If you modify this annotation after service creation, there is no effect.</li> </ul> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-name: custom-name\n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-type</code> specifies the load balancer type. This controller reconciles those service resources with this annotation set to either <code>nlb-ip</code> or <code>external</code>.</p> <p>Tip</p> <p>This annotation specifies the controller used to provision LoadBalancers (as specified in legacy-cloud-provider). Refer to lb-scheme to specify whether the LoadBalancer is internet-facing or internal.</p> <ul> <li>[Deprecated] For type <code>nlb-ip</code>, the controller will provision an NLB with targets registered by IP address. This value is supported for backwards compatibility.</li> <li>For type <code>external</code>, the NLB target type depends on the nlb-target-type annotation.</li> </ul> <p>limitations</p> <ul> <li>This annotation should not be modified after service creation.</li> </ul> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-type: external\n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-nlb-target-type</code> specifies the target type to configure for NLB. You can choose between <code>instance</code> and <code>ip</code>.</p> <ul> <li> <p><code>instance</code> mode will route traffic to all EC2 instances within cluster on the NodePort opened for your service. The kube-proxy on the individual worker nodes sets up the forwarding of the traffic from the NodePort to the pods behind the service.</p> <ul> <li>service must be of type <code>NodePort</code> or <code>LoadBalancer</code> for <code>instance</code> targets</li> <li>for k8s 1.22 and later if <code>spec.allocateLoadBalancerNodePorts</code> is set to <code>false</code>, <code>NodePort</code> must be allocated manually</li> </ul> <p>default value</p> <p>If you configure <code>spec.loadBalancerClass</code>, the controller defaults to <code>instance</code> target type</p> <p>NodePort allocation</p> <p>k8s version 1.22 and later support disabling NodePort allocation by setting the service field <code>spec.allocateLoadBalancerNodePorts</code> to <code>false</code>. If the NodePort is not allocated for a service port, the controller will fail to reconcile instance mode NLB.</p> </li> <li> <p><code>ip</code> mode will route traffic directly to the pod IP. In this mode, AWS NLB sends traffic directly to the Kubernetes pods behind the service, eliminating the need for an extra network hop through the worker nodes in the Kubernetes cluster.</p> <ul> <li><code>ip</code> target mode supports pods running on AWS EC2 instances and AWS Fargate</li> <li>network plugin must use native AWS VPC networking configuration for pod IP, for example Amazon VPC CNI plugin.</li> </ul> </li> </ul> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance\n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-subnets</code> specifies the Availability Zone the NLB will route traffic to. See Network Load Balancers for more details.</p> <p>Tip</p> <p>Subnets are auto-discovered if this annotation is not specified, see Subnet Discovery for further details.</p> <p>You must specify at least one subnet in any of the AZs, both subnetID or subnetName(Name tag on subnets) can be used.</p> <p>limitations</p> <ul> <li>Each subnets must be from a different Availability Zone</li> <li>Similar to any delete operation, removing an Availability Zone can be a potentially disruptive operation. We recommend you evaluate for any potential impact on existing connections, traffic flows, or production workloads. Refer to product documentation for prescriptive guidance on how to use this capability in a safe manner.</li> </ul> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-subnets: subnet-xxxx, mySubnet\n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-alpn-policy</code> allows you to configure the ALPN policies on the load balancer.</p> <p>supported policies</p> <ul> <li><code>HTTP1Only</code> Negotiate only HTTP/1.*. The ALPN preference list is http/1.1, http/1.0.</li> <li><code>HTTP2Only</code> Negotiate only HTTP/2. The ALPN preference list is h2.</li> <li><code>HTTP2Optional</code> Prefer HTTP/1.* over HTTP/2 (which can be useful for HTTP/2 testing). The ALPN preference list is http/1.1, http/1.0, h2.</li> <li><code>HTTP2Preferred</code> Prefer HTTP/2 over HTTP/1.*. The ALPN preference list is h2, http/1.1, http/1.0.</li> <li><code>None</code> Do not negotiate ALPN. This is the default.</li> </ul> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-alpn-policy: HTTP2Preferred\n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-target-node-labels</code> specifies which nodes to include in the target group registration for <code>instance</code> target type.</p> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-target-node-labels: label1=value1, label2=value2\n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-eip-allocations</code> specifies a list of elastic IP address configuration for an internet-facing NLB.</p> <p>Note</p> <ul> <li>This configuration is optional, and you can use it to assign static IP addresses to your NLB</li> <li>You must specify the same number of eip allocations as load balancer subnets annotation</li> <li>NLB must be internet-facing</li> </ul> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-eip-allocations: eipalloc-xyz, eipalloc-zzz\n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-private-ipv4-addresses</code> specifies a list of private IPv4 addresses for an internal NLB.</p> <p>Note</p> <ul> <li>NLB must be internal</li> <li>This configuration is optional, and you can use it to assign static IPv4 addresses to your NLB</li> <li>You must specify the same number of private IPv4 addresses as load balancer subnets annotation</li> <li>You must specify the IPv4 addresses from the load balancer subnet IPv4 ranges</li> </ul> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-private-ipv4-addresses: 192.168.10.15, 192.168.32.16\n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-ipv6-addresses</code> specifies a list of IPv6 addresses for an dualstack NLB.</p> <p>Note</p> <ul> <li>NLB must be dualstack</li> <li>This configuration is optional, and you can use it to assign static IPv6 addresses to your NLB</li> <li>You must specify the same number of private IPv6 addresses as load balancer subnets annotation</li> <li>You must specify the IPv6 addresses from the load balancer subnet IPv6 ranges</li> </ul> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-ipv6-addresses: 2600:1f13:837:8501::1, 2600:1f13:837:8504::1\n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-enable-icmp-for-path-mtu-discovery</code> enables the creation of security group rules to the managed security group to allow explicit ICMP traffic for Path MTU discovery for IPv4 and dual-stack VPCs. Creates a rule for each source range if <code>service.beta.kubernetes.io/load-balancer-source-ranges</code> is present.</p> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-enable-icmp-for-path-mtu-discovery: \"on\"\n</code></pre> </li> </ul>"},{"location":"guide/service/annotations/#traffic-listening","title":"Traffic Listening","text":"<p>Traffic Listening can be controlled with following annotations:</p> <ul> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-ip-address-type</code> specifies the IP address type of NLB.</p> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-ip-address-type: ipv4\n</code></pre> </li> </ul>"},{"location":"guide/service/annotations/#support-udp-based-services-over-ipv6","title":"Support UDP-based services over IPv6","text":"<p>You can configure dualstack NLB to support UDP-based services over IPv6 via the following annotations:</p> <ul> <li> <p>service.beta.kubernetes.io/aws-load-balancer-enable-prefix-for-ipv6-source-nat specifies whether Prefix for IPv6 source NAT is enabled or not. UDP-based support can be enabled for dualstack NLBs only if Prefix for IPv6 source NAT is enabled.</p> <p>Note</p> <ul> <li>Applicable to Network Load Balancers using dualstack IP address type.</li> <li>This configuration is optional, and you can use it to enable UDP support over IPv6.</li> <li>Allowed values are either \u201con\u201d or \u201coff\u201d</li> <li>Once the source prefix for source NATing is enabled, it cannot be disabled if load balancer has a UDP listener attached.</li> <li>Steps to disable the aws-load-balancer-enable-prefix-for-ipv6-source-nat after it is enabled and UDP listeners already attached.</li> <li>You will have to first remove the UDP listeners and apply the manifest.</li> <li>Update the manifest to set source NATing to \"off\" and then apply the manifest again.</li> </ul> <p>Example</p> <ul> <li>Enable prefix for IPv6 Source NAT <pre><code>service.beta.kubernetes.io/aws-load-balancer-enable-prefix-for-ipv6-source-nat: \"on\"\n</code></pre></li> </ul> </li> <li> <p>service.beta.kubernetes.io/aws-load-balancer-source-nat-ipv6-prefixes specifies a list of IPv6 prefixes that should be used for IPv6 source NATing.</p> <p>Note</p> <ul> <li>Applicable to Network Load Balancers using dualstack IP address type.</li> <li>This annotation can be specified only if service.beta.kubernetes.io/aws-load-balancer-enable-prefix-for-ipv6-source-nat annotation is set to \u201con\u201d.</li> <li>This configuration is optional and it can be used to specify custom IPv6 prefixes for IPv6 source NATing to support UDP based services routing in Network Load Balancers using dualstack IP address type.</li> <li>If service.beta.kubernetes.io/aws-load-balancer-enable-prefix-for-ipv6-source-nat annotation is set to \u201con\u201d, and you don\u2019t specify this annotation, then IPv6 prefix/CIDR for source NATing will be auto-assigned to each subnet.</li> <li>If you are specifying this annotation, you must specify the same number of items in the list as the load balancer subnets annotation and following the same order. Each item in the list can have value of either \u201cauto_assigned\u201d or a valid IPv6 prefix/CIDR with prefix length of 80 and it should be in range of the corresponding subnet CIDR.</li> <li>Once the source NAT IPv6 prefixes are set, the IPv6 prefixes cannot be updated if the load balancer has a UDP listener attached.</li> </ul> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-source-nat-ipv6-prefixes: 1025:0223:0009:6487:0001::/80, auto_assigned, 1025:0223:0010:6487:0001::/80\n</code></pre> </li> </ul>"},{"location":"guide/service/annotations/#resource-attributes","title":"Resource attributes","text":"<p>NLB resource attributes can be controlled via the following annotations:</p> <ul> <li> <p>service.beta.kubernetes.io/aws-load-balancer-proxy-protocol specifies whether to enable proxy protocol v2 on the target group. Set to '*' to enable proxy protocol v2. This annotation takes precedence over the annotation <code>service.beta.kubernetes.io/aws-load-balancer-target-group-attributes</code> for proxy protocol v2 configuration.</p> <p>The only valid value for this annotation is <code>*</code>.</p> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-target-group-attributes</code> specifies the Target Group Attributes to be configured.</p> <p>Example</p> <ul> <li>set the deregistration delay to 120 seconds (available range is 0-3600 seconds)     <pre><code>service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: deregistration_delay.timeout_seconds=120\n</code></pre></li> <li>enable source IP affinity     <pre><code>service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: stickiness.enabled=true,stickiness.type=source_ip\n</code></pre></li> <li>enable proxy protocol version 2     <pre><code>service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: proxy_protocol_v2.enabled=true\n</code></pre></li> <li>enable connection termination on deregistration     <pre><code>service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: deregistration_delay.connection_termination.enabled=true\n</code></pre></li> <li>enable client IP preservation <pre><code>service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: preserve_client_ip.enabled=true\n</code></pre></li> <li>disable immediate connection termination for unhealthy targets and configure a 30s draining interval (available range is  0-360000 seconds)     <pre><code>service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: target_health_state.unhealthy.connection_termination.enabled=false,target_health_state.unhealthy.draining_interval_seconds=30\n</code></pre></li> </ul> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-attributes</code> specifies Load Balancer Attributes that should be applied to the NLB.</p> <p>Only attributes defined in the annotation will be updated. To unset any AWS defaults(e.g. Disabling access logs after having them enabled once), the values need to be explicitly set to the original values(<code>access_logs.s3.enabled=false</code>) and omitting them is not sufficient. Custom attributes set in this annotation's config map will be overriden by annotation-specific attributes. For backwards compatibility, existing annotations for the individual load balancer attributes get precedence in case of ties.</p> <ul> <li>If <code>deletion_protection.enabled=true</code> is in the annotation, the controller will not be able to delete the NLB during reconciliation. Once the attribute gets edited to <code>deletion_protection.enabled=false</code> during reconciliation, the deployer will force delete the resource.</li> <li>Please note, if the deletion protection is not enabled via annotation (e.g. via AWS console), the controller still deletes the underlying resource.</li> </ul> <p>Example</p> <ul> <li>enable access log to s3 <pre><code>service.beta.kubernetes.io/aws-load-balancer-attributes: access_logs.s3.enabled=true,access_logs.s3.bucket=my-access-log-bucket,access_logs.s3.prefix=my-app\n</code></pre></li> <li>enable NLB deletion protection <pre><code>service.beta.kubernetes.io/aws-load-balancer-attributes: deletion_protection.enabled=true\n</code></pre></li> <li>enable cross zone load balancing <pre><code>service.beta.kubernetes.io/aws-load-balancer-attributes: load_balancing.cross_zone.enabled=true\n</code></pre></li> <li>enable client availability zone affinity <pre><code>service.beta.kubernetes.io/aws-load-balancer-attributes: dns_record.client_routing_policy=availability_zone_affinity\n</code></pre></li> </ul> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-listener-attributes.${Protocol}-${Port}</code> specifies listener attributes that should be applied to the listener.</p> <p>Only attributes defined in the annotation will be updated. To reset any AWS defaults, the values need to be explicitly set to the original values and omitting it is not sufficient.</p> <p>Example</p> <ul> <li>configure TCP idle timeout value.  <pre><code>service.beta.kubernetes.io/aws-load-balancer-listener-attributes.TCP-80: tcp.idle_timeout.seconds=400\n</code></pre></li> </ul> </li> <li> <p>the following annotations are deprecated in v2.3.0 release in favor of service.beta.kubernetes.io/aws-load-balancer-attributes</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-access-log-enabled\nservice.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-name\nservice.beta.kubernetes.io/aws-load-balancer-access-log-s3-bucket-prefix\nservice.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled \n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-multi-cluster-target-group</code> Allows you to share the created Target Group ARN with other Load Balancer Controller managed clusters.</p> <p>This feature does not offer any Deletion Protection. Deleting the service will still delete the Target Group. If you need to support Target Groups shared with multiple clusters, it's recommended to use an out-of-band Target Group that is not managed by a Load Balancer Controller.</p> <ul> <li>It is not recommended to change this value frequently, if ever. The recommended way to set this value is on creation of the service.</li> </ul> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-multi-cluster-target-group: \"true\"\n</code></pre> </li> </ul>"},{"location":"guide/service/annotations/#aws-resource-tags","title":"AWS Resource Tags","text":"<p>The AWS Load Balancer Controller automatically applies following tags to the AWS resources it creates (NLB/TargetGroups/Listener/ListenerRule):</p> <ul> <li><code>elbv2.k8s.aws/cluster: ${clusterName}</code></li> <li><code>service.k8s.aws/stack: ${stackID}</code></li> <li><code>service.k8s.aws/resource: ${resourceID}</code></li> </ul> <p>In addition, you can use annotations to specify additional tags</p> <ul> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags</code> specifies additional tags to apply to the AWS resources.</p> <ul> <li>you cannot override the default controller tags mentioned above or the tags specified in the <code>--default-tags</code> controller flag</li> <li>if any of the tag conflicts with the ones configured via <code>--external-managed-tags</code> controller flag, the controller fails to reconcile the service</li> </ul> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: Environment=dev,Team=test\n</code></pre> </li> </ul>"},{"location":"guide/service/annotations/#health-check","title":"Health Check","text":"<p>Health check on target groups can be configured with following annotations:</p> <ul> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol</code> specifies the target group health check protocol.</p> <ul> <li>you can specify <code>tcp</code>, or <code>http</code> or <code>https</code>, <code>tcp</code> is the default</li> <li><code>tcp</code> is the default health check protocol if the service <code>spec.externalTrafficPolicy</code> is <code>Cluster</code>, <code>http</code> if <code>Local</code></li> <li>if the service <code>spec.externalTrafficPolicy</code> is <code>Local</code>, do not use <code>tcp</code> for health check</li> <li>Supports only single protocol per service</li> </ul> <p>Example</p> <p><code>service.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol: http</code></p> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-healthcheck-port</code> specifies the TCP port to use for target group health check.</p> <p>default value</p> <ul> <li>if you do not specify the health check port, the default value will be <code>spec.healthCheckNodePort</code> when <code>externalTrafficPolicy=local</code> or <code>traffic-port</code> otherwise.</li> </ul> <p>Example</p> <ul> <li>set the health check port to <code>traffic-port</code> <pre><code>service.beta.kubernetes.io/aws-load-balancer-healthcheck-port: traffic-port\n</code></pre></li> <li>set the health check port to port <code>80</code> <pre><code>service.beta.kubernetes.io/aws-load-balancer-healthcheck-port: \"80\"\n</code></pre></li> </ul> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-healthcheck-path</code> specifies the http path for the health check in case of http/https protocol.</p> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-healthcheck-path: /healthz\n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold</code> specifies the consecutive health check successes required before a target is considered healthy.</p> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: \"3\"\n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold</code> specifies the consecutive health check failures before a target gets marked unhealthy.</p> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: \"3\"\n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval</code> specifies the interval between consecutive health checks.</p> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: \"10\"\n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-healthcheck-success-codes</code> specifies the http success codes for the health check in case of http/https protocol.</p> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-healthcheck-success-codes: \"200-399\"\n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout</code> specifies the target group health check timeout. The target has to respond within the timeout for a successful health check.</p> <p>Note</p> <p>The controller currently ignores the timeout configuration due to the limitations on the AWS NLB. The default timeout for TCP is 10s and HTTP is 6s.</p> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: \"10\"\n</code></pre> </li> </ul>"},{"location":"guide/service/annotations/#tls","title":"TLS","text":"<p>You can configure TLS support via the following annotations:</p> <ul> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-ssl-cert</code> specifies the ARN of one or more certificates managed by the AWS Certificate Manager.</p> <p>The first certificate in the list is the default certificate and remaining certificates are for the optional certificate list. See Server Certificates for further details.</p> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-ssl-cert: arn:aws:acm:us-west-2:xxxxx:certificate/xxxxxxx\n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-ssl-ports</code> specifies the frontend ports with TLS listeners.</p> <ul> <li>You must configure at least one certificate for TLS listeners</li> <li>You can specify a list of port names or port values, <code>*</code> does not match any ports</li> <li>If you don't specify this annotation, controller creates TLS listener for all the service ports</li> <li>Specify this annotation if you need both TLS and non-TLS listeners on the same load balancer</li> </ul> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-ssl-ports: 443, custom-port\n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy</code> specifies the Security Policy for NLB frontend connections, allowing you to control the protocol and ciphers.</p> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-ssl-negotiation-policy: ELBSecurityPolicy-TLS13-1-2-2021-06\n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-backend-protocol</code> specifies whether to use TLS for the backend traffic between the load balancer and the kubernetes pods.</p> <ul> <li>If you specify <code>ssl</code> as the backend protocol, NLB uses TLS connections for the traffic to your kubernetes pods in case of TLS listeners</li> <li>You can specify <code>ssl</code> or <code>tcp</code> (default)</li> </ul> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-backend-protocol: ssl\n</code></pre> </li> </ul>"},{"location":"guide/service/annotations/#access-control","title":"Access control","text":"<p>Load balancer access can be controlled via following annotations:</p> <ul> <li> <p><code>service.beta.kubernetes.io/load-balancer-source-ranges</code> specifies the CIDRs that are allowed to access the NLB.</p> <p>Tip</p> <ul> <li>We recommend specifying CIDRs in the service <code>spec.loadBalancerSourceRanges</code> instead</li> <li>For enhanced security with <code>internal</code> network load balancers, we recommend limiting access by specifying allowed source IP ranges.  This can be done using either the <code>service.beta.kubernetes.io/load-balancer-source-ranges</code> annotation or the <code>spec.loadBalancerSourceRanges</code> field.</li> </ul> <p>Default</p> <ul> <li><code>0.0.0.0/0</code> will be used if the IPAddressType is \"ipv4\"</li> <li><code>0.0.0.0/0</code> and <code>::/0</code> will be used if the IPAddressType is \"dualstack\"</li> </ul> <p>This annotation will be ignored in case preserve client IP is not enabled. - preserve client IP is disabled by default for <code>IP</code> targets - preserve client IP is enabled by default for <code>instance</code> targets</p> <p>Preserve client IP has no effect on traffic converted from IPv4 to IPv6 and on traffic converted from IPv6 to IPv4. The source IP of this type of traffic is always the private IP address of the Network Load Balancer. - This could cause the clients that have their traffic converted to bypass the specified CIDRs that are allowed to access the NLB.</p> <p>this annotation will be ignored if <code>service.beta.kubernetes.io/aws-load-balancer-security-groups</code> is specified.</p> <p>Example</p> <pre><code>service.beta.kubernetes.io/load-balancer-source-ranges: 10.0.0.0/24\n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-security-group-prefix-lists</code> specifies the managed prefix lists that are allowed to access the NLB.</p> <p>this annotation will be ignored if <code>service.beta.kubernetes.io/aws-load-balancer-security-groups</code> is specified.</p> <p>If you'd like to use this annotation, make sure your security group rule quota is enough. If you'd like to know how the managed prefix list affects your quota, see the reference in the AWS documentation for more details.</p> <p>If you only use this annotation without <code>load-balancer-source-ranges</code>, the controller managed security group would ignore the <code>load-balancer-source-ranges</code> default settings.</p> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-security-group-prefix-lists: pl-00000000, pl-1111111\n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-scheme</code> specifies whether the NLB will be internet-facing or internal.  Valid values are <code>internal</code>, <code>internet-facing</code>. If not specified, default is <code>internal</code>.</p> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-scheme: \"internet-facing\"\n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-internal</code> specifies whether the NLB will be internet-facing or internal.</p> <p>deprecation note</p> <p>This annotation is deprecated starting v2.2.0 release in favor of the new aws-load-balancer-scheme annotation. It will be supported, but in case of ties, the aws-load-balancer-scheme gets precedence.</p> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-internal: \"true\"\n</code></pre> <ul> <li><code>service.beta.kubernetes.io/aws-load-balancer-security-groups</code>  specifies the frontend securityGroups you want to attach to an NLB.</li> </ul> <p>When this annotation is not present, the controller will automatically create one security group. The security group will be attached to the LoadBalancer and allow access from <code>load-balancer-source-ranges</code> and <code>aws-load-balancer-security-group-prefix-lists</code> to the <code>listen-ports</code>. Also, the securityGroups for target instances/ENIs will be modified to allow inbound traffic from this securityGroup.</p> <p>If you specify this annotation, you need to configure the security groups on your target instances/ENIs to allow inbound traffic from the load balancer. You could also set the <code>manage-backend-security-group-rules</code> if you want the controller to manage the security group rules.</p> <p>Both name and ID of securityGroups are supported. Name matches a <code>Name</code> tag, not the <code>groupName</code> attribute.</p> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-security-groups: sg-xxxx, nameOfSg1, nameOfSg2\n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-manage-backend-security-group-rules</code> specifies whether the controller should automatically add the ingress rules to the instance/ENI security group.</p> <p>If you disable the automatic management of security group rules for an NLB (e.g.: by setting <code>service.beta.kubernetes.io/aws-load-balancer-security-groups</code>), you will need to manually add appropriate ingress rules to your EC2 instance or ENI security groups to allow access to the traffic and health check ports.</p> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-manage-backend-security-group-rules: \"false\"\n</code></pre> </li> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-inbound-sg-rules-on-private-link-traffic</code> specifies whether to apply security group rules to traffic sent to the load balancer through AWS PrivateLink. </p> <p>Example</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-inbound-sg-rules-on-private-link-traffic: \"off\"\n</code></pre> </li> </ul>"},{"location":"guide/service/annotations/#capacity-unit-reservation","title":"Capacity Unit Reservation","text":"<p>Load balancer capacity unit reservation can be configured via following annotations:</p> <ul> <li> <p><code>service.beta.kubernetes.io/aws-load-balancer-minimum-load-balancer-capacity</code> specifies the   Capacity Unit Reservation to be configured.</p> <p>Example</p> <ul> <li>set the capacity unit reservation to 1000   <pre><code>service.beta.kubernetes.io/aws-load-balancer-minimum-load-balancer-capacity: CapacityUnits=3000\n</code></pre></li> <li>reset the capacity unit reservation   <pre><code>service.beta.kubernetes.io/aws-load-balancer-minimum-load-balancer-capacity: CapacityUnits=0\n</code></pre></li> </ul> <p>Notes</p> <ul> <li>If you specify this annotation, but remove it later, the capacity unit reservation is not reset. You need to reset the capacity by setting the capacity units to zero as show in the example above.</li> <li>If users do not want the controller to manage the capacity unit reservation on load balancer, they can disable the feature by setting controller command line feature gate flag <code>--feature-gates=LBCapacityReservation=true</code></li> </ul> </li> </ul>"},{"location":"guide/service/annotations/#legacy-cloud-provider","title":"Legacy Cloud Provider","text":"<p>The AWS Load Balancer Controller manages Kubernetes Services in a compatible way with the AWS cloud provider's legacy service controller.</p> <ul> <li>For users on v2.5.0+, The AWS LBC provides a mutating webhook for service resources to set the <code>spec.loadBalancerCLass</code> field for Serive of type LoadBalancer, effectively making the AWS LBC the default controller for Service of type LoadBalancer.    Users can disable this feature and revert to using the AWS Cloud Controller Manager as the default service controller by setting the helm chart value <code>enableServiceMutatorWebhook</code> to false with <code>--set enableServiceMutatorWebhook=false</code> .</li> <li>For users on older versions, the annotation <code>service.beta.kubernetes.io/aws-load-balancer-type</code> is used to determine which controller reconciles the service. If the annotation value is <code>nlb-ip</code> or <code>external</code>,   recent versions of the legacy cloud provider ignore the Service resource so that the AWS LBC can take over. For all other values of the annotation, the legacy cloud provider will handle the service.    Note that this annotation should be specified during service creation and not edited later. Support for the annotation was added to the legacy cloud provider in Kubernetes v1.20, and is backported to v1.18.18+ and v1.19.10+.</li> </ul>"},{"location":"guide/service/nlb/","title":"Network Load Balancer","text":"<p>The AWS Load Balancer Controller (LBC) supports reconciliation for Kubernetes Service resources of type <code>LoadBalancer</code> by provisioning an AWS Network Load Balancer (NLB) with an <code>instance</code> or <code>ip</code> target type.</p> <p>Secure by default</p> <p>Since the  v2.2.0 release, the LBC provisions an <code>internal</code> NLB by default.</p> <p>To create an <code>internet-facing</code> NLB, the following annotation is required on your service:</p> <pre><code>service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing\n</code></pre> <p>For backwards compatibility, if the <code>service.beta.kubernetes.io/aws-load-balancer-scheme</code> annotation is absent, an existing NLB's scheme remains unchanged.</p>"},{"location":"guide/service/nlb/#prerequisites","title":"Prerequisites","text":"<ul> <li>LBC &gt;= v2.2.0</li> <li>For Kubernetes Service resources of type <code>LoadBalancer</code>:<ul> <li>Kubernetes &gt;= v1.20 or</li> <li>Kubernetes &gt;= v1.19.10 for 1.19 or</li> <li>Kubernetes &gt;= v1.18.18 for 1.18 or</li> <li>EKS &gt;= v1.16</li> </ul> </li> <li>For Kubernetes Service resources of type <code>NodePort</code>:<ul> <li>Kubernetes &gt;= v1.16</li> </ul> </li> <li>For <code>ip</code> target type:<ul> <li>Pods have native AWS VPC networking configured. For more information, see the Amazon VPC CNI plugin documentation.</li> </ul> </li> </ul>"},{"location":"guide/service/nlb/#configuration","title":"Configuration","text":"<p>By default, Kubernetes Service resources of type <code>LoadBalancer</code> get reconciled by the Kubernetes controller built into the <code>CloudProvider</code> component of the <code>kube-controller-manager</code> or the <code>cloud-controller-manager</code>(also known as the in-tree controller).</p> <p>In order for the LBC to manage the reconciliation of Kubernetes Service resources of type <code>LoadBalancer</code>, you need to offload the reconciliation from the in-tree controller to the LBC, explicitly.</p> With LoadBalancerClass <p>The LBC supports the <code>LoadBalancerClass</code> feature since the  v2.4.0 release for Kubernetes v1.22+ clusters.</p> <p>The <code>LoadBalancerClass</code> feature provides a <code>CloudProvider</code> agnostic way of offloading the reconciliation for Kubernetes Service resources of type <code>LoadBalancer</code> to an external controller.</p> <p>When you specify the <code>spec.loadBalancerClass</code> to be <code>service.k8s.aws/nlb</code> on a Kubernetes Service resource of type <code>LoadBalancer</code>, the LBC takes charge of reconciliation by provisioning an NLB.</p> <p>Warning</p> <ul> <li> <p>If you modify a Service resource with matching <code>spec.loadBalancerClass</code> by changing its <code>type</code> from <code>LoadBalancer</code> to anything else, the controller will cleanup the provisioned NLB for that Service.</p> </li> <li> <p>If the <code>spec.loadBalancerClass</code> is set to a <code>loadBalancerClass</code> that isn't recognized by the LBC, it ignores the Service resource, regardless of the <code>service.beta.kubernetes.io/aws-load-balancer-type</code> annotation.</p> </li> </ul> <p>Tip</p> <ul> <li> <p>By default, the NLB uses the <code>instance</code> target type. You can customize it using the <code>service.beta.kubernetes.io/aws-load-balancer-nlb-target-type</code> annotation.</p> </li> <li> <p>The LBC uses <code>service.k8s.aws/nlb</code> as the default <code>LoadBalancerClass</code>. You can customize it to a different value using the controller flag <code>--load-balancer-class</code>.</p> </li> </ul> <p>Example: instance mode</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: echoserver\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance\nspec:\n  selector:\n    app: echoserver\n  ports:\n    - port: 80\n      targetPort: 8080\n      protocol: TCP\n  type: LoadBalancer\n  loadBalancerClass: service.k8s.aws/nlb\n</code></pre> <p>Example: ip mode</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: echoserver\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip\nspec:\n  selector:\n    app: echoserver\n  ports:\n    - port: 80\n      targetPort: 8080\n      protocol: TCP\n  type: LoadBalancer\n  loadBalancerClass: service.k8s.aws/nlb\n</code></pre> With <code>service.beta.kubernetes.io/aws-load-balancer-type</code> annotation <p>The AWS in-tree controller supports an AWS specific way of offloading the reconciliation for Kubernetes Service resources of type <code>LoadBalancer</code> to an external controller.</p> <p>When you specify the <code>service.beta.kubernetes.io/aws-load-balancer-type</code> annotation to be <code>external</code> on a Kubernetes Service resource of type <code>LoadBalancer</code>, the in-tree controller ignores the Service resource. In addition, if you specify the <code>service.beta.kubernetes.io/aws-load-balancer-nlb-target-type</code> annotation on the Service resource, the LBC takes charge of reconciliation by provisioning an NLB.</p> <p>Warning</p> <ul> <li> <p>It's not recommended to modify or add the <code>service.beta.kubernetes.io/aws-load-balancer-type</code> annotation on an existing Service resource. If a change is desired, delete the existing Service resource and create a new one instead of modifying an existing Service.</p> </li> <li> <p>If you modify this annotation on an existing Service resource, you might end up with leaked LBC resources.</p> </li> </ul> <p>backwards compatibility for <code>nlb-ip</code> type</p> <p>For backwards compatibility, both the in-tree and LBC controller supports <code>nlb-ip</code> as a value for the <code>service.beta.kubernetes.io/aws-load-balancer-type</code> annotation. The controllers treats it as if you specified both of the following annotations: <pre><code>service.beta.kubernetes.io/aws-load-balancer-type: external\nservice.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip\n</code></pre></p> <p>Example: instance mode</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: echoserver\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: external\n    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: instance\nspec:\n  selector:\n    app: echoserver\n  ports:\n    - port: 80\n      targetPort: 8080\n      protocol: TCP\n  type: LoadBalancer\n</code></pre> <p>Example: ip mode</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: echoserver\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: external\n    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip\nspec:\n  selector:\n    app: echoserver\n  ports:\n    - port: 80\n      targetPort: 8080\n      protocol: TCP\n  type: LoadBalancer\n</code></pre>"},{"location":"guide/service/nlb/#protocols","title":"Protocols","text":"<p>The LBC supports both TCP and UDP protocols. The controller also configures TLS termination on your NLB if you configure the Service with a certificate annotation.</p> <p>In the case of TCP, an NLB with IP targets doesn't pass the client source IP address, unless you specifically configure it to using target group attributes. Your application pods might not see the actual client IP address, even if the NLB passes it along. For example, if you're using instance mode with <code>externalTrafficPolicy</code> set to <code>Cluster</code>. In such cases, you can configure NLB proxy protocol v2 using an annotation if you need visibility into the client source IP address on your application pods.</p> <p>To enable proxy protocol v2, apply the following annotation to your Service: <pre><code>service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: \"*\"\n</code></pre></p> <p>If you enable proxy protocol v2, NLB health checks with HTTP/HTTPS only work if the health check port supports proxy protocol v2. Due to this behavior, you shouldn't configure proxy protocol v2 with NLB instance mode and <code>externalTrafficPolicy</code> set to <code>Local</code>.</p>"},{"location":"guide/service/nlb/#subnet-tagging-requirements","title":"Subnet tagging requirements","text":"<p>See Subnet Discovery for details on configuring Elastic Load Balancing for public or private placement.</p>"},{"location":"guide/service/nlb/#security-group","title":"Security group","text":"<ul> <li>From v2.6.0, the AWS LBC creates and attaches frontend and backend security groups to NLB by default. For more information please see the security groups documentation </li> <li>In older versions, the controller by default adds inbound rules to the worker node security groups, to allow inbound traffic from an NLB.</li> </ul> <p>disable worker node security group rule management</p> <p>You can disable the worker node security group rule management using an annotation.</p>"},{"location":"guide/service/nlb/#worker-node-security-groups-selection","title":"Worker node security groups selection","text":"<p>The controller automatically selects the worker node security groups that it modifies to allow inbound traffic using the following rules:</p> <ul> <li>For <code>instance</code> mode, the security group of each backend worker node's primary elastic network interface (ENI) is selected.</li> <li>For <code>ip</code> mode, the security group of each backend pod's ENI is selected.</li> </ul> <p>Multiple security groups on an ENI</p> <p>If there are multiple security groups attached to an ENI, the controller expects only one security group tagged with following tags:</p> Key Value <code>kubernetes.io/cluster/${cluster-name}</code> <code>owned</code> or <code>shared</code> <p><code>${cluster-name}</code> is the name of the Kubernetes cluster.</p> <p>If it is possible for multiple security groups with the tag <code>kubernetes.io/cluster/${cluster-name}</code> to be on a target ENI, you may use the <code>--service-target-eni-security-group-tags</code> flag to specify additional tags that must also match in order for a security group to be used.</p>"},{"location":"guide/service/nlb/#worker-node-security-groups-rules","title":"Worker node security groups rules","text":"When client IP preservation is enabled Rule Protocol Port(s) IpRanges(s) Client Traffic <code>spec.ports[*].protocol</code> <code>spec.ports[*].port</code> Traffic Source CIDRs Health Check Traffic TCP Health Check Ports NLB Subnet CIDRs When client IP preservation is disabled Rule Protocol Port(s) IpRange(s) Client Traffic <code>spec.ports[*].protocol</code> <code>spec.ports[*].port</code> NLB Subnet CIDRs Health Check Traffic TCP Health Check Ports NLB Subnet CIDRs"},{"location":"guide/targetgroupbinding/spec/","title":"Specification","text":"<p>Packages:</p> <ul> <li> elbv2.k8s.aws/v1beta1 </li> </ul>"},{"location":"guide/targetgroupbinding/spec/#elbv2.k8s.aws/v1beta1","title":"elbv2.k8s.aws/v1beta1","text":"<p> <p>Package v1beta1 contains API Schema definitions for the elbv2 v1beta1 API group</p> </p> <p>Resource Types:</p> <ul><li> TargetGroupBinding </li></ul>"},{"location":"guide/targetgroupbinding/spec/#elbv2.k8s.aws/v1beta1.TargetGroupBinding","title":"TargetGroupBinding","text":"<p> <p>TargetGroupBinding is the Schema for the TargetGroupBinding API</p> </p> Field Description <code>apiVersion</code> string <code> elbv2.k8s.aws/v1beta1 </code> <code>kind</code> string  <code>TargetGroupBinding</code> <code>metadata</code>  Kubernetes meta/v1.ObjectMeta  <code>annotations</code>  Refer to the Kubernetes API documentation for the other fields of the <code>metadata</code> field.  <code>spec</code>  TargetGroupBindingSpec  <code>targetGroupARN</code>  string  <p>targetGroupARN is the Amazon Resource Name (ARN) for the TargetGroup.</p> <code>targetType</code>  TargetType  (Optional) <p>targetType is the TargetType of TargetGroup. If unspecified, it will be automatically inferred.</p> <code>serviceRef</code>  ServiceReference  <p>serviceRef is a reference to a Kubernetes Service and ServicePort.</p> <code>networking</code>  TargetGroupBindingNetworking  (Optional) <p>networking defines the networking rules to allow ELBV2 LoadBalancer to access targets in TargetGroup.</p> <code>status</code>  TargetGroupBindingStatus"},{"location":"guide/targetgroupbinding/spec/#elbv2.k8s.aws/v1beta1.IPBlock","title":"IPBlock","text":"<p> (Appears on: NetworkingPeer) </p> <p> <p>IPBlock defines source/destination IPBlock in networking rules.</p> </p> Field Description <code>cidr</code>  string  <p>CIDR is the network CIDR. Both IPV4 or IPV6 CIDR are accepted.</p>"},{"location":"guide/targetgroupbinding/spec/#elbv2.k8s.aws/v1beta1.NetworkingIngressRule","title":"NetworkingIngressRule","text":"<p> (Appears on: TargetGroupBindingNetworking) </p> <p> <p>NetworkingIngressRule defines a particular set of traffic that is allowed to access TargetGroup\u2019s targets.</p> </p> Field Description <code>from</code>  []NetworkingPeer  <p>List of peers which should be able to access the targets in TargetGroup. At least one NetworkingPeer should be specified.</p> <code>ports</code>  []NetworkingPort  <p>List of ports which should be made accessible on the targets in TargetGroup. If ports is empty or unspecified, it defaults to all ports with TCP.</p>"},{"location":"guide/targetgroupbinding/spec/#elbv2.k8s.aws/v1beta1.NetworkingPeer","title":"NetworkingPeer","text":"<p> (Appears on: NetworkingIngressRule) </p> <p> <p>NetworkingPeer defines the source/destination peer for networking rules.</p> </p> Field Description <code>ipBlock</code>  IPBlock  (Optional) <p>IPBlock defines an IPBlock peer. If specified, none of the other fields can be set.</p> <code>securityGroup</code>  SecurityGroup  (Optional) <p>SecurityGroup defines a SecurityGroup peer. If specified, none of the other fields can be set.</p>"},{"location":"guide/targetgroupbinding/spec/#elbv2.k8s.aws/v1beta1.NetworkingPort","title":"NetworkingPort","text":"<p> (Appears on: NetworkingIngressRule) </p> <p> <p>NetworkingPort defines the port and protocol for networking rules.</p> </p> Field Description <code>protocol</code>  NetworkingProtocol  <p>The protocol which traffic must match. If protocol is unspecified, it defaults to TCP.</p> <code>port</code>  k8s.io/apimachinery/pkg/util/intstr.IntOrString  (Optional) <p>The port which traffic must match. When NodePort endpoints(instance TargetType) is used, this must be a numerical port. When Port endpoints(ip TargetType) is used, this can be either numerical or named port on pods. if port is unspecified, it defaults to all ports.</p>"},{"location":"guide/targetgroupbinding/spec/#elbv2.k8s.aws/v1beta1.NetworkingProtocol","title":"NetworkingProtocol (<code>string</code> alias)","text":"<p> (Appears on: NetworkingPort) </p> <p> <p>NetworkingProtocol defines the protocol for networking rules.</p> </p>"},{"location":"guide/targetgroupbinding/spec/#elbv2.k8s.aws/v1beta1.SecurityGroup","title":"SecurityGroup","text":"<p> (Appears on: NetworkingPeer) </p> <p> <p>SecurityGroup defines reference to an AWS EC2 SecurityGroup.</p> </p> Field Description <code>groupID</code>  string  <p>GroupID is the EC2 SecurityGroupID.</p>"},{"location":"guide/targetgroupbinding/spec/#elbv2.k8s.aws/v1beta1.ServiceReference","title":"ServiceReference","text":"<p> (Appears on: TargetGroupBindingSpec) </p> <p> <p>ServiceReference defines reference to a Kubernetes Service and its ServicePort.</p> </p> Field Description <code>name</code>  string  <p>Name is the name of the Service.</p> <code>port</code>  k8s.io/apimachinery/pkg/util/intstr.IntOrString  <p>Port is the port of the ServicePort.</p>"},{"location":"guide/targetgroupbinding/spec/#elbv2.k8s.aws/v1beta1.TargetGroupBindingNetworking","title":"TargetGroupBindingNetworking","text":"<p> (Appears on: TargetGroupBindingSpec) </p> <p> <p>TargetGroupBindingNetworking defines the networking rules to allow ELBV2 LoadBalancer to access targets in TargetGroup.</p> </p> Field Description <code>ingress</code>  []NetworkingIngressRule  (Optional) <p>List of ingress rules to allow ELBV2 LoadBalancer to access targets in TargetGroup.</p>"},{"location":"guide/targetgroupbinding/spec/#elbv2.k8s.aws/v1beta1.TargetGroupBindingSpec","title":"TargetGroupBindingSpec","text":"<p> (Appears on: TargetGroupBinding) </p> <p> <p>TargetGroupBindingSpec defines the desired state of TargetGroupBinding</p> </p> Field Description <code>targetGroupARN</code>  string  <p>targetGroupARN is the Amazon Resource Name (ARN) for the TargetGroup.</p> <code>targetType</code>  TargetType  (Optional) <p>targetType is the TargetType of TargetGroup. If unspecified, it will be automatically inferred.</p> <code>serviceRef</code>  ServiceReference  <p>serviceRef is a reference to a Kubernetes Service and ServicePort.</p> <code>networking</code>  TargetGroupBindingNetworking  (Optional) <p>networking defines the networking rules to allow ELBV2 LoadBalancer to access targets in TargetGroup.</p>"},{"location":"guide/targetgroupbinding/spec/#elbv2.k8s.aws/v1beta1.TargetGroupBindingStatus","title":"TargetGroupBindingStatus","text":"<p> (Appears on: TargetGroupBinding) </p> <p> <p>TargetGroupBindingStatus defines the observed state of TargetGroupBinding</p> </p> Field Description <code>observedGeneration</code>  int64  (Optional) <p>The generation observed by the TargetGroupBinding controller.</p>"},{"location":"guide/targetgroupbinding/spec/#elbv2.k8s.aws/v1beta1.TargetType","title":"TargetType (<code>string</code> alias)","text":"<p> (Appears on: TargetGroupBindingSpec) </p> <p> <p>TargetType is the targetType of your ELBV2 TargetGroup.</p> <ul> <li>with <code>instance</code> TargetType, nodes with nodePort for your service will be registered as targets</li> <li>with <code>ip</code> TargetType, Pods with containerPort for your service will be registered as targets</li> </ul> </p> <p> Generated with <code>gen-crd-api-reference-docs</code> on git commit <code>21418f44</code>. </p>"},{"location":"guide/targetgroupbinding/targetgroupbinding/","title":"TargetGroupBinding","text":"<p>TargetGroupBinding is a custom resource (CR) that can expose your pods using an existing ALB TargetGroup or NLB TargetGroup.</p> <p>This will allow you to provision the load balancer infrastructure completely outside of Kubernetes but still manage the targets with Kubernetes Service.</p> <p>usage to support Ingress and Service</p> <p>The AWS LoadBalancer controller internally used TargetGroupBinding to support the functionality for Ingress and Service resource as well. It automatically creates TargetGroupBinding in the same namespace of the Service used.</p> <p>You can view all TargetGroupBindings in a namespace by <code>kubectl get targetgroupbindings -n &lt;your-namespace&gt; -o wide</code></p>"},{"location":"guide/targetgroupbinding/targetgroupbinding/#targettype","title":"TargetType","text":"<p>TargetGroupBinding CR supports TargetGroups of either <code>instance</code> or <code>ip</code> TargetType.</p> <p>If TargetType is not explicitly specified, a mutating webhook will automatically call AWS API to find the TargetType for your TargetGroup and set it to correct value.</p>"},{"location":"guide/targetgroupbinding/targetgroupbinding/#choosing-the-target-group","title":"Choosing the Target Group","text":"<p>One can either use <code>targetGroupARN</code> of <code>targetGroupName</code> to identify a Target Group. Although both are unique and immutable in an AWS region, one only has control of the <code>targetGroupName</code>, for <code>targetGroupARN</code> is generated by AWS and contain random characters.</p> <p>If you provide both <code>targetGroupARN</code> and <code>targetGroupName</code>, beware that <code>targetGroupARN</code> prevails.</p>"},{"location":"guide/targetgroupbinding/targetgroupbinding/#sample-yamls","title":"Sample YAMLs","text":"<pre><code>apiVersion: elbv2.k8s.aws/v1beta1\nkind: TargetGroupBinding\nmetadata:\n  name: my-tgb\nspec:\n  serviceRef:\n    name: awesome-service # route traffic to the awesome-service\n    port: 80\n  targetGroupName: &lt;name-of-the-targetGroup&gt;\n</code></pre> <pre><code>apiVersion: elbv2.k8s.aws/v1beta1\nkind: TargetGroupBinding\nmetadata:\n  name: my-tgb\nspec:\n  serviceRef:\n    name: awesome-service # route traffic to the awesome-service\n    port: 80\n  targetGroupARN: &lt;arn-to-targetGroup&gt;\n</code></pre>"},{"location":"guide/targetgroupbinding/targetgroupbinding/#vpcid","title":"VpcID","text":"<p>TargetGroupBinding CR supports the explicit definition of the Virtual Private Cloud (VPC) of your TargetGroup.</p> <p>If the VpcID is not explicitly specified, a mutating webhook will automatically call AWS API to find the VpcID for your TargetGroup and set it to correct value.</p>"},{"location":"guide/targetgroupbinding/targetgroupbinding/#sample-yaml","title":"Sample YAML","text":"<pre><code>apiVersion: elbv2.k8s.aws/v1beta1\nkind: TargetGroupBinding\nmetadata:\n  name: my-tgb\nspec:\n  serviceRef:\n    name: awesome-service # route traffic to the awesome-service\n    port: 80\n  targetGroupARN: &lt;arn-to-targetGroup&gt;\n  vpcID: &lt;vpcID&gt;\n</code></pre>"},{"location":"guide/targetgroupbinding/targetgroupbinding/#nodeselector","title":"NodeSelector","text":""},{"location":"guide/targetgroupbinding/targetgroupbinding/#default-node-selector","title":"Default Node Selector","text":"<p>For <code>TargetType: instance</code>, all nodes of a cluster that match the following selector are added to the target group by default:</p> <pre><code>matchExpressions:\n  - key: node-role.kubernetes.io/master\n    operator: DoesNotExist\n  - key: node.kubernetes.io/exclude-from-external-load-balancers\n    operator: DoesNotExist\n  - key: alpha.service-controller.kubernetes.io/exclude-balancer\n    operator: DoesNotExist\n  - key: eks.amazonaws.com/compute-type\n    operator: NotIn\n    values: [\"fargate\"]\n</code></pre>"},{"location":"guide/targetgroupbinding/targetgroupbinding/#custom-node-selector","title":"Custom Node Selector","text":"<p>TargetGroupBinding CR supports <code>NodeSelector</code> which is a LabelSelector. This will select nodes to attach to the <code>instance</code> TargetType target group and is merged with the default node selector above.</p> <pre><code>apiVersion: elbv2.k8s.aws/v1beta1\nkind: TargetGroupBinding\nmetadata:\n  name: my-tgb\nspec:\n  nodeSelector:\n    matchLabels:\n      foo: bar\n  ...\n</code></pre>"},{"location":"guide/targetgroupbinding/targetgroupbinding/#assumerole","title":"AssumeRole","text":"<p>Sometimes the AWS LoadBalancer controller needs to manipulate target groups from different AWS accounts. The way to do that is assuming a role from such an account. The following spec fields help you with that.</p> <ul> <li><code>iamRoleArnToAssume</code>: the ARN that you need to assume</li> <li><code>assumeRoleExternalId</code>: the external ID for the assume role operation. Optional, but recommended. It helps you to prevent the confused deputy problem ( https://docs.aws.amazon.com/IAM/latest/UserGuide/confused-deputy.html )</li> </ul> <pre><code>apiVersion: elbv2.k8s.aws/v1beta1\nkind: TargetGroupBinding\nmetadata:\n  name: peered-tg\n  namespace: nlb-game-2048-1\nspec:\n  assumeRoleExternalId: very-secret-string-2\n  iamRoleArnToAssume: arn:aws:iam::155642222660:role/tg-management-role\n  networking:\n    ingress:\n    - from:\n      - securityGroup:\n          groupID: sg-0b6a41a2fd959623f\n      ports:\n      - port: 80\n        protocol: TCP\n  serviceRef:\n    name: service-2048\n    port: 80\n  targetGroupARN: arn:aws:elasticloadbalancing:us-west-2:155642222660:targetgroup/peered-tg/6a4ecf7bfae473c1\n</code></pre> <p>In the following examples, we will refer to Cluster Owner (CO) and Target Group Owner (TGO) accounts.</p> <p>First, in the TGO account creates a role that will allow the AWS LBC in the CO account to assume it. For improved security, we only allow the AWS LBC role in CO account to assume the role.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"\",\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"AWS\": \"arn:aws:iam::565768096483:role/eksctl-awslbc-loadtest-addon-iamserviceaccoun-Role1-13RdJCMqV6p2\"\n            },\n            \"Action\": \"sts:AssumeRole\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"sts:ExternalId\": \"very-secret-string\"\n                }\n            }\n        }\n    ]\n}\n</code></pre> <p>Next, still in the TGO account we need to add the following permissions to the Role created in the first step.</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"VisualEditor0\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"elasticloadbalancing:RegisterTargets\",\n        \"elasticloadbalancing:DeregisterTargets\"\n      ],\n      \"Resource\": [\n        \"arn:aws:elasticloadbalancing:us-west-2:155642222660:targetgroup/tg1/*\",\n        \"arn:aws:elasticloadbalancing:us-west-2:155642222660:targetgroup/tg2/*\"\n        // add more here //\n      ]\n    },\n    {\n      \"Sid\": \"VisualEditor1\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"elasticloadbalancing:DescribeTargetGroups\",\n        \"elasticloadbalancing:DescribeTargetHealth\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n</code></pre> <p>Next, in the CO account, we need to allow the AWS LBC to perform the AssumeRole call. By default, this permission is not a part of the standard IAM policy that is vended with the LBC installation scripts. For improved security, it is possible to scope the AssumeRole permissions down to only roles that you know ahead of time the LBC will need to Assume.</p> <pre><code>        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sts:AssumeRole\"\n            ],\n            \"Resource\": \"*\"\n        }\n</code></pre>"},{"location":"guide/targetgroupbinding/targetgroupbinding/#sample-yaml_1","title":"Sample YAML","text":"<pre><code>apiVersion: elbv2.k8s.aws/v1beta1\nkind: TargetGroupBinding\nmetadata:\n  name: my-tgb\nspec:\n  iamRoleArnToAssume: \"arn:aws:iam::999999999999:role/alb-controller-policy-to-assume\"\n  assumeRoleExternalId: \"some-magic-string\"\n  ...\n</code></pre>"},{"location":"guide/targetgroupbinding/targetgroupbinding/#multicluster-target-group","title":"MultiCluster Target Group","text":"<p>TargetGroupBinding CRD supports sharing the same target group ARN among multiple clusters. Setting this flag will ensure the controller only operates on targets within the cluster.</p> <p>The default value is false, meaning that the controller assumes full control over the target group ARN and will deregister any targets that are not found within the cluster. To set this flag for TGBs managed by the controller use either: ALB: alb.ingress.kubernetes.io/multi-cluster-target-group: \"true\" NLB: service.beta.kubernetes.io/aws-load-balancer-multi-cluster-target-group: \"true\"</p> <p>It is not recommended to change this value after TGB creation. Changing between shared / not shared might lead to leaked targets.</p> <p>Only use this flag if you intend to share the target group ARN in multiple clusters. This flag will slow down reconciles and put a small additonal load on the kubernetes control plane.</p>"},{"location":"guide/targetgroupbinding/targetgroupbinding/#sample-yaml_2","title":"Sample YAML","text":"<pre><code>apiVersion: elbv2.k8s.aws/v1beta1\nkind: TargetGroupBinding\nmetadata:\n  name: my-tgb\nspec:\n  serviceRef:\n    name: awesome-service # route traffic to the awesome-service\n    port: 80\n  targetGroupARN: &lt;arn-to-targetGroup&gt;\n  multiClusterTargetGroup: true\n</code></pre>"},{"location":"guide/targetgroupbinding/targetgroupbinding/#reference","title":"Reference","text":"<p>See the reference for TargetGroupBinding CR</p>"},{"location":"guide/tasks/cognito_authentication/","title":"Setup Cognito/AWS Load Balancer Controller","text":"<p>This document describes how to install AWS Load Balancer Controller with AWS Cognito integration to minimal capacity, other options and or configurations may be required for production, and on an app to app basis.</p>"},{"location":"guide/tasks/cognito_authentication/#assumptions","title":"Assumptions","text":"<p>The following assumptions are observed regarding this procedure.</p> <ul> <li>ExternalDNS is installed to the cluster and will provide a custom URL for your ALB. To setup ExternalDNS refer to the install instructions.</li> </ul>"},{"location":"guide/tasks/cognito_authentication/#cognito-configuration","title":"Cognito Configuration","text":"<p>Configure Cognito for use with AWS Load Balancer Controller using the following links with specified caveats.</p> <ul> <li>Create Cognito user pool</li> <li>Configure application integration<ul> <li>On step 11.c for the <code>Callback URL</code> enter <code>https://&lt;your-domain&gt;/oauth2/idpresponse</code>.</li> <li>On step 11.d for <code>Allowed OAuth Flows</code> select <code>authorization code grant</code> and for <code>Allowed OAuth Scopes</code> select <code>openid</code>.</li> </ul> </li> </ul>"},{"location":"guide/tasks/cognito_authentication/#aws-load-balancer-controller-setup","title":"AWS Load Balancer Controller Setup","text":"<p>Install the AWS Load Balancer Controller using the install instructions with the following caveats.</p> <ul> <li>When setting up IAM Role Permissions, add the <code>cognito-idp:DescribeUserPoolClient</code> permission to the example policy.</li> </ul>"},{"location":"guide/tasks/cognito_authentication/#deploying-an-ingress","title":"Deploying an Ingress","text":"<p>Using the cognito-ingress-template you can fill in the <code>&lt;required&gt;</code> variables to create an ALB ingress connected to your Cognito user pool for authentication.</p>"},{"location":"guide/tasks/migrate_legacy_apps/","title":"Migrating From Legacy Apps with Manually Configured Target Groups","text":"<p>Many organizations are decomposing old legacy apps into smaller services and components.</p> <p>During the transition they may be running a hybrid ecosystem with some parts of the app running in ec2 instances, some in Kubernetes microservices, and possibly even some in serverless environments like Lambda.</p> <p>The existing clients of the application expect all endpoints under one DNS entry and it's desirable to be able to route traffic at the ALB to services running outside the Kubernetes cluster.</p> <p>The actions annotation allows the definition of a forward rule to a previously configured target group. Learn more about the actions annotation at <code>alb.ingress.kubernetes.io/actions.${action-name}</code></p>"},{"location":"guide/tasks/migrate_legacy_apps/#example-ingress-manifest","title":"Example Ingress Manifest","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  namespace: testcase\n  name: echoserver\n  annotations:\n    alb.ingress.kubernetes.io/actions.legacy-app: '{\"Type\": \"forward\", \"TargetGroupArn\": \"legacy-tg-arn\"}'\nspec:\n  ingressClassName: alb\n  rules:\n    - http:\n        paths:\n          - path: /v1/endpoints\n            pathType: Exact\n            backend:\n              service:\n                name: legacy-app\n                port:\n                  name: use-annotation\n          - path: /normal-path\n            pathType: Exact\n            backend:\n              service:\n                name: echoserver\n                port:\n                  number: 80\n</code></pre> <p>Note</p> <p>The <code>TargetGroupArn</code> must be set and the user is responsible for configuring the Target group in AWS before applying the forward rule.</p>"},{"location":"guide/tasks/ssl_redirect/","title":"Redirect Traffic from HTTP to HTTPS","text":"<p>You can use the <code>alb.ingress.kubernetes.io/ssl-redirect</code> annotation to setup an ingress to redirect http traffic to https</p>"},{"location":"guide/tasks/ssl_redirect/#example-ingress-manifest","title":"Example Ingress Manifest","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  namespace: default\n  name: ingress\n  annotations:\n    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-west-2:xxxx:certificate/xxxxxx\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}, {\"HTTPS\":443}]'\n    alb.ingress.kubernetes.io/ssl-redirect: '443'\nspec:\n  ingressClassName: alb\n  rules:\n    - http:\n        paths:\n         - path: /users/*\n           pathType: ImplementationSpecific\n           backend:\n             service:\n               name: user-service\n               port:\n                 number: 80\n         - path: /*\n           pathType: ImplementationSpecific\n           backend:\n             service:\n               name: default-service\n               port:\n                 number: 80\n</code></pre> <p>Note</p> <ul> <li><code>alb.ingress.kubernetes.io/listen-ports</code> annotation must at least include [{\"HTTP\": 80}, {\"HTTPS\":443}] to listen on 80 and 443.</li> <li><code>alb.ingress.kubernetes.io/certificate-arn</code> annotation must be set to allow listen for HTTPS traffic</li> <li>the ssl-redirect port must appear in the listen-port annotation, and must be an HTTPS port</li> </ul>"},{"location":"guide/tasks/ssl_redirect/#how-it-works","title":"How it works","text":"<p>If you enable SSL redirection, the controller configures each HTTP listener with a default action to redirect to HTTPS. The controller does not add any other rules to the HTTP listener.</p> <p>For the above example, the HTTP listener on port 80 will have a single default rule to redirect traffic to HTTPS on port 443.</p>"},{"location":"guide/use_cases/blue_green/","title":"Split Traffic","text":"<p>You can configure an Application Load Balancer (ALB) to split traffic from the same listener across multiple target groups using rules. This facilitates A/B testing, blue/green deployment, and traffic management without additional tools. The Load Balancer Controller (LBC) supports defining this behavior alongside the standard configuration of an Ingress resource. </p> <p>More specifically, the ALB supports weighted target groups and advanced request routing. </p> <p>Weighted target group Multiple target groups can be attached to the same forward action of a listener rule and specify a weight for each group. It allows developers to control how to distribute traffic to multiple versions of their application. For example, when you define a rule having two target groups with weights of 8 and 2, the load balancer will route 80 percent of the traffic to the first target group and 20 percent to the other.</p> <p>Advanced request routing In addition to the weighted target group, AWS announced the advanced request routing feature in 2019. Advanced request routing gives developers the ability to write rules (and route traffic) based on standard and custom HTTP headers and methods, the request path, the query string, and the source IP address. This new feature simplifies the application architecture by eliminating the need for a proxy fleet for routing, blocks unwanted traffic at the load balancer, and enables the implementation of A/B testing.</p>"},{"location":"guide/use_cases/blue_green/#overview","title":"Overview","text":"<p>The ALB is configured to split traffic using annotations on the ingress resources. More specifically, the ingress annotation <code>alb.ingress.kubernetes.io/actions.${service-name}</code> configures custom actions on the listener. </p> <p>The body of the annotation is a JSON document that identifies an action type, and configures it. The supported actions are <code>redirect</code>, <code>forward</code>, and <code>fixed-response</code>. </p> <p>With forward action, multiple target groups with different weights can be defined in the annotation. The LBC provisions the target groups and configures the listener rules as per the annotation to direct the traffic. </p> <p>Importantly:  * The <code>action-name</code> in the annotation must match the service name in the Ingress rules. For example, the annotation <code>alb.ingress.kubernetes.io/actions.blue-green</code> matches the service name <code>blue-green</code> referenced in the Ingress rules.  * The <code>servicePort</code> of the service in the Ingress rules must be <code>use-annotation</code>.</p>"},{"location":"guide/use_cases/blue_green/#example","title":"Example","text":"<p>The following ingress resource configures the ALB to forward all traffic to hello-kubernetes-v1 service (weight: 100 vs. 0).</p> <p>Note that the annotation name includes <code>blue-green</code>, which matches the service name referenced in the ingress rules. </p> <p>The annotation reference includes further examples of the JSON configuration for different actions.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: \"hello-kubernetes\"\n  namespace: \"hello-kubernetes\"\n  annotations:\n    kubernetes.io/ingress.class: alb\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/target-type: ip\n    alb.ingress.kubernetes.io/actions.blue-green: |\n      {\n        \"type\":\"forward\",\n        \"forwardConfig\":{\n          \"targetGroups\":[\n            {\n              \"serviceName\":\"hello-kubernetes-v1\",\n              \"servicePort\":\"80\",\n              \"weight\":100\n            },\n            {\n              \"serviceName\":\"hello-kubernetes-v2\",\n              \"servicePort\":\"80\",\n              \"weight\":0\n            }\n          ]\n        }\n      }\n  labels:\n    app: hello-kubernetes\nspec:\n  rules:\n    - http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: blue-green\n                port:\n                  name: use-annotation\n</code></pre>"},{"location":"guide/use_cases/frontend_sg/","title":"Restrict Access with Frontend Security Groups","text":"<p>Frontend security groups limit client/internet traffic with a load balancer. This improves security by preventing unauthorized access to cluster services, and blocking unexpected outbound connections. Both AWS Network Load Balancers (NLBs) and Application Load Balancers (ALBs) support frontend security groups. Learn more about how the Load Balancer Controller uses Frontend and Backend Security Groups. </p>"},{"location":"guide/use_cases/frontend_sg/#solution-overview","title":"Solution Overview","text":"<p>Load balancers expose cluster workloads to a wider network. Creating a frontend security group limits access to these workloads (service or ingress resources). More specifically, a security group acts as a virtual firewall to control incoming and outgoing traffic. Inbound rules control the incoming traffic to your load balancer, and outbound rules control the outgoing traffic from your load balancer.</p> <p>Security groups are particularly suited for defining what access other AWS resources (services, EC2 instances) have to your cluster. For example, if you have an existing security group including EC2 instances, you can permit only that security group to access a service.</p> <p>In this example, you will restrict access to a cluster service. You will create a new security group for the frontend of a load balancer, and add an inbound rule permitting traffic. The rule may limit traffic to a specific port, CIDR, or existing security group.</p>"},{"location":"guide/use_cases/frontend_sg/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes Cluster Version 1.22+</li> <li>AWS Load Balancer Controller v2.6.0+</li> <li>AWS CLI v2</li> </ul>"},{"location":"guide/use_cases/frontend_sg/#configure","title":"Configure","text":""},{"location":"guide/use_cases/frontend_sg/#1-find-the-vpc-id-of-your-cluster","title":"1. Find the VPC ID of your cluster","text":"<pre><code>$ aws eks describe-cluster --name &lt;cluster-name&gt; --query \"cluster.resourcesVpcConfig.vpcId\" --output text\n\nvpc-0101XXXXa356\n</code></pre> <p>Ensure you have the right cluster name, AWS region, and the AWS CLI is configured.</p>"},{"location":"guide/use_cases/frontend_sg/#2-create-a-security-group-using-the-vpc-id","title":"2. Create a security group using the VPC ID","text":"<pre><code>$ aws ec2 create-security-group --group-name &lt;sg-name&gt; --description &lt;description&gt; --vpc-id &lt;vpc-id&gt;\n\n{\n    \"GroupId\": \"sg-0406XXXX645c\"\n}\n</code></pre> <p>Note the security group ID. This will be the frontend security group for the load balancer.</p>"},{"location":"guide/use_cases/frontend_sg/#3-create-your-ingress-rules","title":"3. Create your ingress rules","text":"<p>Load balancers generally serve as an entrypoint for clients to access your cluster. This makes ingress rules especially important.</p> <p>For example, this rule permits all traffic on port 443:</p> <pre><code>aws ec2 authorize-security-group-ingress --group-id &lt;sg-id&gt; --protocol all --port 443 --cidr 0.0.0.0/0\n</code></pre> <p>Learn more about how to create an ingress rule with the AWS CLI.</p>"},{"location":"guide/use_cases/frontend_sg/#4-determine-your-egress-rules-optional","title":"4. Determine your egress rules (optional)","text":"<p>By default, all outbound traffic is allowed. Further, security groups are stateful, and responses to an allowed connection will also be permitted.</p> <p>Learn how to create an egress rule with the AWS CLI.</p>"},{"location":"guide/use_cases/frontend_sg/#5-add-the-security-group-annotation-to-your-ingress-or-service","title":"5. Add the security group annotation to your Ingress or Service","text":"<p>For Ingress resources, add the following annotation:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: frontend\n  annotations:\n    alb.ingress.kubernetes.io/security-groups: &lt;sg-id&gt;\n</code></pre> <p>For Service resources, add the following annotation:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: frontend\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-security-groups: &lt;sg-id&gt;\nspec:\n    type: LoadBalancer\n    loadBalancerClass: service.k8s.aws/nlb\n</code></pre> <p>For Ingress resources, the associated Application Load Balancer will be updated. For Service resources, the associated Network Load Balancer will be updated.</p>"},{"location":"guide/use_cases/frontend_sg/#6-list-your-load-balancers-and-verify-the-security-groups-are-attached","title":"6. List your load balancers and verify the security groups are attached","text":"<pre><code>$ aws elbv2 describe-load-balancers\n\n{\n    \"LoadBalancers\": [\n        {\n            \"LoadBalancerArn\": \"arn:aws:elasticloadbalancing:us-east-1:1853XXXX5115:loadbalancer/net/k8s-default-frontend-ae3743b818/3ad6d16fb75ff688\",\n            &lt;...&gt;\n            \"SecurityGroups\": [\n                \"sg-0406XXXX645c\",\n                \"sg-0873XXXX2bef\"\n            ],\n            \"IpAddressType\": \"ipv4\"\n        }\n    ]\n}\n</code></pre> <p>If you don't see the security groups, verify:</p> <ul> <li>The Load Balancer Controller is properly installed.</li> <li>The controller has proper IAM permissions to modify load balancers. Look at the logs of the controller pods for IAM errors.</li> </ul>"},{"location":"guide/use_cases/frontend_sg/#7-clean-up-optional","title":"7. Clean up (Optional)","text":"<p>Removing the annotations from Service/Ingress resources will revert to the default frontend ecurity groups.</p> <p>Load balancers may be costly. Delete Ingress and Service resources to deprovision the load balancers. If the load balancers are deleted from the console, they may be recreated by the controller.</p>"},{"location":"guide/use_cases/multi_cluster/","title":"MultiCluster Target Groups","text":"<p>The load balancer controller assumes full control over the configured target groups. When a target group is registered with the controller it de registers any targets not currently in the cluster. Target groups that have MultiCluster support enabled can be associated to multiple Kubernetes clusters or support arbitrary targets from other sources.</p>"},{"location":"guide/use_cases/multi_cluster/#overview","title":"Overview","text":"<p>When enabled, MultiCluster mode supports multiple methods, and every cluster associated with a target group has one of these methods. It's recommended to use new resources when configuring MutliCluster mode. There is a period of time when MultiCluster must take a snapshot of the cluster state in order to support the selected mode. This data is stored into ConfigMap, which resides in the same namespace as your load balancer resources. ConfigMap stores snapshots of managed targets at <code>aws-lbc-targets-$TARGET_GROUP_BINDING_NAME</code></p> <p>When using an ALB, you must specify this annotation in the ingress or service:</p> <p><code>alb.ingress.kubernetes.io/multi-cluster-target-group: \"true\"</code></p> <p>When using an NLB, you specify this annotation in your service:</p> <p><code>service.beta.kubernetes.io/aws-load-balancer-multi-cluster-target-group: \"true\"</code></p> <p>When using any out-of-band TargetGroupBindings, you must specify this field in the spec:</p> <p><code>multiClusterTargetGroup: true</code></p>"},{"location":"guide/use_cases/multi_cluster/#example","title":"Example","text":"<p>We will be setting up an echoserver in two clusters in order to demonstrate MultiCluster mode. See the full echoserver example in the 'Examples' tab.</p> <p>The following ingress configures the Target Group Binding as MultiCluster. We will take the created Target Group and share it in a second cluster.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: echoserver\n  namespace: echoserver\n  annotations:\n    alb.ingress.kubernetes.io/multi-cluster-target-group: \"true\"    \n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/tags: Environment=dev,Team=test\nspec:\n  ingressClassName: alb\n  rules:\n    - http:\n        paths:\n          - path: /\n            pathType: Exact\n            backend:\n              service:\n                name: echoserver\n                port:\n                  number: 80\n</code></pre> <p>Verify that MultiCluster is enabled by verifying that the created Target Group Binding is marked as MultiCluster.</p> <pre><code>kubectl -n echoserver get targetgroupbinding k8s-echoserv-echoserv-cc0122e143 -o yaml\napiVersion: elbv2.k8s.aws/v1beta1\nkind: TargetGroupBinding\nmetadata:\n  annotations:\n    elbv2.k8s.aws/checkpoint: cKay81gadoTtBSg6uVVginqtmCVG-1ApTvYN4YLD37U/_4kBy3Yg64qrXzjvIb2LlC3O__ex1qjozynsqHXmPgo\n    elbv2.k8s.aws/checkpoint-timestamp: \"1729021572\"\n  creationTimestamp: \"2024-10-15T19:46:06Z\"\n  finalizers:\n  - elbv2.k8s.aws/resources\n  generation: 1\n  labels:\n    ingress.k8s.aws/stack-name: echoserver\n    ingress.k8s.aws/stack-namespace: echoserver\n  name: k8s-echoserv-echoserv-cc0122e143\n  namespace: echoserver\n  resourceVersion: \"79121011\"\n  uid: 9ceaa2ea-14bb-44a5-abb0-69c7d2aac52c\nspec:\n  ipAddressType: ipv4\n  multiClusterTargetGroup: true &lt;&lt;&lt; HERE\n  networking:\n    ingress:\n    - from:\n      - securityGroup:\n          groupID: sg-06a2bd7d790ac1d2e\n      ports:\n      - port: 32197\n        protocol: TCP\n  serviceRef:\n    name: echoserver\n    port: 80\n  targetGroupARN: arn:aws:elasticloadbalancing:us-east-1:565768096483:targetgroup/k8s-echoserv-echoserv-cc0122e143/6816b87346280ee7\n  targetType: instance\n  vpcID: vpc-0a7ef5bd8943067a8\n</code></pre> <p>In another cluster, you can now register that Target Group ARN in a Target Group Binding.</p> <pre><code>apiVersion: elbv2.k8s.aws/v1beta1\nkind: TargetGroupBinding\nmetadata:\n  name: MyTargetGroupBinding\n  namespace: echoserver\nspec:\n  serviceRef:\n    name: echoserver\n    port: 80\n  multiClusterTargetGroup: true\n  targetType: instance\n  ipAddressType: ipv4\n  networking:\n    ingress:\n    - from:\n      - securityGroup:\n          groupID: $SG_FROM_ABOVE\n      ports:\n      - port: 32197\n        protocol: TCP\n  targetGroupARN: $TG_FROM_ABOVE\n</code></pre> <p>The configured TargetGroup should have targets from both clusters available to service traffic.</p>"},{"location":"guide/use_cases/nlb_tls_termination/","title":"TLS Termination with Network Load Balancer","text":""},{"location":"guide/use_cases/nlb_tls_termination/#motivation","title":"Motivation","text":"<p>Managing TLS certificates (and related configuration) for production cluster workloads is both time consuming, and high risk. For example, storing multiple copies of a certificate secret key in the cluster may increases the chances of it being compromised. Additionally, TLS can be complicated to configure and implement properly. </p> <p>Traditionally, TLS termination at the load balancer step required using more expensive application load balancers (ALBs). AWS introduced TLS termination for network load balancers (NLBs) for enhanced security and cost effectiveness. </p> <p>The TLS implementation used by the AWS NLB is formally verified and maintained. Additionally, AWS Certificate Manager (ACM) is used, fully isolating your cluster from access to the private key. </p>"},{"location":"guide/use_cases/nlb_tls_termination/#solution-overview","title":"Solution Overview","text":"<p>An external client transmits a request to the NLB. The request is encrypted with TLS using the production (e.g., client facing) certificate, and on port 443. </p> <p>The NLB decrypts the request, and transmits it on to your cluster on port 80. It follows the standard request routing configured within the cluster. Notably, the request received within the cluster includes the actual origin IP address of the external client. Alternate ports may be configured.  </p> <p>Note</p> <p>The NLB may be configured to maintain the source (i.e., client) IP address. However, there are some limitations. Review Client IP Preservation in the AWS docs. </p>"},{"location":"guide/use_cases/nlb_tls_termination/#prerequisites","title":"Prerequisites","text":"<p>\u2705 Access to DNS records for domain name.</p> <p>Review the docs on registering domains with AWS's Route 53. Alternate DNS providers may be used, such as Google Domains or Namecheap.</p> <p>Later, a subdomain (e.g., demo-service.gcline.us) will be created, pointing to the NLB. Access to the DNS records is required to generate a TLS certificate for use by the NLB. </p> <p>\u2705  AWS Load Balancer Controller Installed </p> <p>Generally, setting up the Load Balancer Controller has two steps: enabling IAM roles for service accounts, and adding the controller to the cluster. The IAM role allows the controller in the Kubernetes cluster to manage AWS resources. Learn more about IAM roles for service accounts.</p>"},{"location":"guide/use_cases/nlb_tls_termination/#configure","title":"Configure","text":""},{"location":"guide/use_cases/nlb_tls_termination/#generate-tls-certificate","title":"Generate TLS Certificate","text":"<p>Create a public TLS certificate for the domain using AWS Certificate Manager (ACM). This is streamlined when the domain is managed by Route 53. Review the AWS Certificate Manager Docs.</p> <p>The domain name on the TLS certificate must correspond to the planned domain name for the kubernetes service. The domain name may be specified explicitly (e.g., tls-demo.gcline.us), or a wildcard certificate can be used (e.g., *.gcline.us).</p> <p>If the domain is registered with Route53, the TLS certificate request will automatically be approved. Otherwise, follow ACM console the instructions to create a DNS record to validate the domain. </p> <p>After validation, the certificate will be available for use in your AWS account. </p> <p>Note the ARN of the certificate, which uniquely identifies it in kubernetes config files. </p> <p></p>"},{"location":"guide/use_cases/nlb_tls_termination/#create-service-with-new-nlb","title":"Create Service with new NLB","text":"<p>Add annotations to a load balancer service to enable NLB TLS termination, before the traffic reaches Envoy. The annotations are actioned by the load balancer controller. Review all the NLB annotations on GitHub.</p> annotation name value meaning service.beta.kubernetes.io/aws-load-balancer-type external explicitly requires an NLB, instead of an ALB service.beta.kubernetes.io/aws-load-balancer-nlb-target-type ip route traffic directly to the pod IP service.beta.kubernetes.io/aws-load-balancer-scheme internet-facing An internet-facing load balancer has a publicly resolvable DNS name service.beta.kubernetes.io/aws-load-balancer-ssl-cert \"arn:aws:acm:...\" identifies the TLS certificate used by the NLB service.beta.kubernetes.io/aws-load-balancer-ssl-ports 443 determines the port the NLB should listen for TLS traffic on <p>Example: </p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: MyAppSvc\n  namespace: dev\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-type: external\n    service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip\n    service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing\n    service.beta.kubernetes.io/aws-load-balancer-ssl-cert: \"arn:aws:acm:us-east-2:185309785115:certificate/7610ed7d-5a81-4ea2-a18a-7ba1606cca3e\"\n    service.beta.kubernetes.io/aws-load-balancer-ssl-ports: \"443\"\nspec:\n  externalTrafficPolicy: Local\n  ports:\n  - port: 443\n    targetPort: 80\n    name: http\n    protocol: TCP\n  selector:\n    app: MyApp\n  type: LoadBalancer\n</code></pre>"},{"location":"guide/use_cases/nlb_tls_termination/#configure-dns","title":"Configure DNS","text":"<p>Get domain name using kubectl. </p> <p>The service name and namespace were defined above.</p> <pre><code>kubectl get svc MyAppSvc --namespace dev\n</code></pre> <pre><code>NAME    TYPE           CLUSTER-IP      EXTERNAL-IP                                                                     PORT(S)        AGE\nenvoy   LoadBalancer   10.100.24.154   k8s-&lt;namespace&gt;-&lt;service_name&gt;-xxxxxxxxxx-xxxxxxxxxxxxxxxx.elb.&lt;region_code&gt;.amazonaws.com   443:31606/TCP   40d\n</code></pre> <p>Note the last 4 digits of the domain name for the NLB. For example, \"bb1f\". </p> <p>Setup DNS alias for NLB</p> <p>Create a DNS record pointing from a friendly name (e.g., tls-demo.gcline.us) to the NLB domain (e.g., bb1f.elb.us-east-2.amazonaws.com). </p> <p>For AWS's Route 53, follow the instructions below. If you use a different DNS provider, follow their instructions for creating a CNAME record. </p> <p>First, create a new record in Route 53. </p> <p>Use the \"A\" record type, and enable the \"alias\" option. This option attaches the DNS record to the AWS resource, without requiring an extra lookup step for clients. </p> <p>Select the NLB resource. Double check the region, and use the last 4 digits (noted earlier) to select the proper resource. </p> <p></p>"},{"location":"guide/use_cases/nlb_tls_termination/#verify","title":"Verify","text":"<p>Attempt to access the NLB domain at port 443 with HTTPS/TLS. Is the connection successful? What certificate is used? Does it reach the expected endpoint within the cluster? </p>"},{"location":"guide/use_cases/self_managed_lb/","title":"Externally Managed Load Balancers","text":""},{"location":"guide/use_cases/self_managed_lb/#motivation","title":"Motivation","text":"<p>The load balancer controller (LBC) generally creates and destroys AWS Load Balancers in response to Kubernetes resources. </p> <p>However, some cluster operators may prefer to manually manage AWS Load Balancers. This supports use cases like:</p> <ul> <li>Preventing accidental release of key IP addresses.</li> <li>Supporting load balancers where the Kubernetes cluster is one of multiple targets.</li> <li>Complying with organizational requirements on provisioning load balancers, for security or cost reasons. </li> </ul>"},{"location":"guide/use_cases/self_managed_lb/#solution-overview","title":"Solution Overview","text":"<p>Use the TargetGroupBinding CRD to sync a Kubernetes service with the targets of a load balancer.</p> <p>First, a load balancer is manually created directly with AWS. This guide uses a network load balancer, but an application load balancer may be similarly configured. </p> <p>Second, A listener and a target group are then added to the load balancer. </p> <p>Third, a TargetGroupBinding CRD is created in a cluster. The CRD includes references to a Kubernetes service and the ARN of the Load Balancer Target Group. The CRD configures the LBC to watch the service and automatically update the target group with the appropriate pod VPC IP addresses. </p>"},{"location":"guide/use_cases/self_managed_lb/#prerequisites","title":"Prerequisites","text":"<p>Install:</p> <ul> <li>Load Balancer Controller Installed on Cluster</li> <li>AWS CLI</li> <li>Kubectl</li> </ul> <p>Have this information available:</p> <ul> <li>Cluster VPC Information</li> <li>ID of EKS Cluster</li> <li>Subnet IDs</li> <li>This information is available in the \"Networking\" section of the EKS Cluster Console. </li> <li>Port and Protocol of Target Kubernetes Service</li> </ul>"},{"location":"guide/use_cases/self_managed_lb/#configure-load-balancer","title":"Configure Load Balancer","text":"<p>Create Load Balancer: (optional)</p> <ol> <li> <p>Use the create-load-balancer command to create an IPv4 load balancer, specifying a public subnet for each Availability Zone in which you have instances. </p> <p>You can specify only one subnet per Availability Zone. </p> <pre><code>aws elbv2 create-load-balancer --name my-load-balancer --type network --subnets subnet-0e3f5cac72EXAMPLE\n</code></pre> <p>Important: The output includes the ARN of the load balancer. This value is needed to configure the LBC. </p> <p>Example:</p> <pre><code>arn:aws:elasticloadbalancing:us-east-2:123456789012:loadbalancer/net/my-load-balancer/1234567890123456\n</code></pre> </li> <li> <p>Use the create-target-group command to create an IPv4 target group, specifying the same VPC of your EKS cluster. </p> </li> </ol> <pre><code>aws elbv2 create-target-group --name my-targets --protocol TCP --port 80 --vpc-id vpc-0598c7d356EXAMPLE\n</code></pre> <p>The output includes the ARN of the target group, with this format:</p> <pre><code>arn:aws:elasticloadbalancing:us-east-2:123456789012:targetgroup/my-targets/1234567890123456\n</code></pre> <ol> <li>Use the create-listener command to create a listener for your load balancer with a default rule that forwards requests to your target group. The listener port and protocol must match the Kubernetes service. However, TLS termination is permitted. [[double check it works in this configuration?]]</li> </ol> <pre><code>aws elbv2 create-listener --load-balancer-arn loadbalancer-arn --protocol TCP --port 80  \\\n--default-actions Type=forward,TargetGroupArn=targetgroup-arn\n</code></pre>"},{"location":"guide/use_cases/self_managed_lb/#create-targetgroupbinding-crd","title":"Create TargetGroupBinding CRD","text":"<ol> <li>Create the TargetGroupBinding CRD</li> </ol> <p>Insert the ARN of the Target Group, as created above.</p> <p>Insert the name and port of the target Kubernetes service.</p> <p><pre><code>apiVersion: elbv2.k8s.aws/v1beta1\nkind: TargetGroupBinding\nmetadata:\n  name: my-tgb\nspec:\n  serviceRef:\n    name: awesome-service # route traffic to the awesome-service\n    port: 80\n  targetGroupARN: arn:aws:elasticloadbalancing:us-east-2:123456789012:targetgroup/my-targets/1234567890123456\n</code></pre> 2. Apply the CRD</p> <p>Apply the TargetGroupBinding CRD CRD file to your Cluster.</p> <p><code>kubectl apply -f my-tgb.yaml</code></p>"},{"location":"guide/use_cases/self_managed_lb/#verify","title":"Verify","text":"<p>Wait approximately 30 seconds for the LBC to update the load balancer.</p> <p>View all target groups in the AWS console.</p> <p>Find the target group by the ARN noted above, and verify the appropriate instances from the cluster have been added.</p>"}]}